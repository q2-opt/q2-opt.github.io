<!doctype html>
<head>
  <title>Quantile QT-Opt for Risk-Aware Vision-Based Robotic Grasping</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1.0" />
  <meta http-equiv="X-UA-Compatible" content="ie=edge" />
  <link href='https://fonts.googleapis.com/css?family=Roboto:300' rel='stylesheet' type='text/css'>
  <meta name="theme-color" content="#1a4067" />
  <!-- SEO -->
  <meta property="og:title" content="Quantile QT-Opt for Risk-Aware Vision-Based Robotic Grasping" />
  <meta property="og:type" content="article" />
  <meta property="og:description" content="A distributional distributed Q-Learning algorithm for robotics." />
  <meta property="og:image" content="https://q2-opt.github.io/assets/img/logo_rect.png" />
  <meta property="og:url" content="https://q2-opt.github.io/" />
  <!-- Twitter Card data -->
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Quantile QT-Opt for Risk-Aware Vision-Based Robotic Grasping" />
  <meta name="twitter:description" content="" />
  <meta property="og:site_name" content="A distributional distributed Q-Learning algorithm for robotics." />
  <meta name="twitter:image" content="https://q2-opt.github.io/assets/img/logo_square.png" />

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css">
  <link rel="stylesheet" href="./style.css">
</head>
<body>

<script src="lib/jquery-1.12.4.min.js"></script>
<!--<script src="lib/mobile-detect.min.js"></script>-->
<script src="lib/template.v1.js"></script>

<div class="cover">
  <h1 class="unselectable">Quantile QT-Opt for Risk-Aware<br/> Vision-Based Robotic Grasping</h1>
  <video src="assets/mp4/intro_edited.mp4" autoplay loop playsinline muted></video>
  <div class="hint unselectable">scroll down</div>
</div>

<dt-article id="dtbody">
<dt-byline class="l-page transparent"></dt-byline>
<h1>Quantile QT-Opt for Risk-Aware<br/> Vision-Based Robotic Grasping</h1>
<dt-byline class="l-page" id="authors_section">
<div class="byline">
  <div class="authors">
    <div class="author">
        <a class="name" href="https://crisbodnar.github.io">Cristian Bodnar</a>
        <a class="affiliation" href="https://www.cst.cam.ac.uk/">University of Cambridge</a>
    </div>
    <div class="author">
        <a class="name" href="https://scholar.google.com/citations?user=ncJWfs0AAAAJ&hl=en">Adrian Li</a>
        <a class="affiliation" href="https://x.company/">Google X</a>
    </div>
    <div class="author">
        <a class="name" href="https://karolhausman.github.io/">Karol Hausman</a>
        <a class="affiliation" href="https://g.co/brain">Google Brain</a>
    </div>
    <div class="author">
        <a class="name" href="https://scholar.google.com/citations?user=_ws9LLgAAAAJ&hl=en">Peter Pastor</a>
        <a class="affiliation" href="https://x.company/">Google X</a>
    </div>
    <div class="author">
        <a class="name" href="https://scholar.google.com/citations?user=DMTuJzAAAAAJ&hl=en">Mrinal Kalakrishnan</a>
        <a class="affiliation" href="https://x.company/">Google X</a>
    </div>
  </div>
  <div class="date">
    <div class="month">RSS</div>
    <div class="year">2020</div>
  </div>
  <div class="date">
    <div class="month">Download</div>
    <div class="year" style="color: #0000FF;"><a href="https://arxiv.org/pdf/1910.02787.pdf" target="_blank">PDF</a></div> </div>
  </div>
</div>
</dt-byline>
</dt-byline>
<h2>Abstract</h2>
<p>The distributional perspective on reinforcement learning (RL) has given
rise to a series of successful Q-learning algorithms, resulting in
state-of-the-art performance in arcade game environments. However, it
has not yet been analyzed how these findings from a discrete setting
translate to complex practical applications characterized by noisy, high
dimensional and continuous state-action spaces. In this work, we propose
<strong>Quantile QT-Opt (Q2-Opt)</strong>, a <em>distributional</em> variant of the recently
introduced distributed Q-learning algorithm
for continuous domains, and examine its behaviour in a series of
simulated and real vision-based robotic grasping tasks. The absence of
an actor in Q2-Opt allows us to directly draw a parallel to the previous
discrete experiments in the literature without the additional
complexities induced by an actor-critic architecture. We demonstrate
that Q2-Opt achieves a superior vision-based object grasping success
rate, while also being more sample efficient. The distributional
formulation also allows us to experiment with various risk distortion
metrics that give us an indication of how robots can concretely manage
risk in practice using a Deep RL control policy. As an additional
contribution, we perform batch RL experiments in our virtual environment
and compare them with the latest findings from discrete settings.
Surprisingly, we find that the previous batch RL findings from the
literature obtained on arcade game environments do not generalise to our
setup.</p>
<hr>
<div class="figure">
<img src="assets/fig/q2_opt_system.png" style="margin: 0; width: 100%;"/>
<figcaption>
Figure 1. Distributed system architecture of Q2-Opt. The interactions between the robot and the environment are either stored in a database of episodes for later offline learning, or they are directly sent to the replay buffer when online learning is performed. The samples from the Simulation Buffer are pulled by the Bellman Updater, which appends the distributional targets. These labelled transitions are pushed to the train buffer and consumed by the TPU Training workers to compute the gradients. The parameter server uses the gradients to update the weights, which are asynchronously pulled by the agents.
</figcaption>
</div>
<h2>Introduction</h2>
<p>The new distributional perspective on RL has produced a novel class of
Deep Q-learning methods that learn a distribution over the state-action
returns, instead of using the expectation given by the traditional value
function. These methods, which obtained state-of-the-art results in the
arcade game environments  <dt-cite key="bellemare2017distributional,dabney2018distributional,dabney2018implicit"></dt-cite>,
present several attractive properties.</p>
<p>First, their ability to preserve the multi-modality of the action values
naturally accounts for learning from a non-stationary policy, most often
deployed in a highly stochastic environment. This ultimately results in
a more stable training process and improved performance and sample
efficiency. Second, they enable the use of risk-sensitive policies that
no longer select actions based on the expected value, but take entire
distributions into account. These policies can represent a continuum of
risk management strategies ranging from risk-averse to risk-seeking by
optimizing for a broader class of risk metrics.</p>
<p>Despite the improvements distributional Q-learning algorithms
demonstrated in the discrete arcade environments, it is yet to be
examined how these findings translate to practical, real-world
applications. Intuitively, the advantageous properties of distributional
Q-learning approaches should be particularly beneficial in a robotic
setting. The value distributions can have a significant qualitative
impact in robotic tasks, usually characterized by highly-stochastic and
continuous state-action spaces. Additionally, performing safe control in
the face of uncertainty is one of the biggest impediments to deploying
robots in the real world, an impediment that RL methods have not yet
tackled. In contrast, a distributional approach can allow robots to
learn an RL policy that appropriately quantifies risks for the task of
interest.</p>
<p>However, given the brittle nature of deep RL algorithms and their often
counter-intuitive behaviour <dt-cite key="Henderson2017DeepRL"></dt-cite>,
it is not entirely clear if these intuitions would hold in practice. Therefore, we believe
that an empirical analysis of distributional Q-learning algorithms in
real robotic applications would shed light on their benefits and
scalability, and provide essential insight for the robot learning
community.</p>
<p>In this paper, we aim to address this need and perform a thorough
analysis of distributional Q-learning algorithms in simulated and real
vision-based robotic manipulation tasks. To this end, we propose a
distributional enhancement of QT-Opt <dt-cite key="kalashnikov2018scalable"></dt-cite>
subbed
Quantile QT-Opt (Q2-Opt). The choice of QT-Opt, a recently introduced
distributed Q-learning algorithm that operates on continuous action
spaces, is dictated by its demonstrated applicability to large-scale
vision-based robotic experiments. In addition, by being an actor-free
generalization of Q-learning in continuous action spaces, QT-Opt enables
a direct comparison to the previous results on the arcade environments
without the additional complexities and compounding effects of an
actor-critic-type architecture.</p>
<p>In particular, we introduce two versions of Q2-Opt, based on Quantile Regression DQN (QR-DQN) <dt-cite key="dabney2018distributional"></dt-cite> and Implicit Quantile Networks (IQN) <dt-cite key="dabney2018implicit"></dt-cite>. The two methods are
evaluated on a vision-based grasping task in simulation and the real
world. We show that these distributional algorithms achieve
state-of-the-art grasping success rate in both settings, while also
being more sample efficient. Furthermore, we experiment with a multitude
of risk metrics, ranging from risk-seeking to risk-averse, and show that
risk-averse policies can bring significant performance improvements. We
also report on the interesting qualitative changes that different risk
metrics induce in the robots' grasping behaviour. As an additional
contribution, we analyze our distributional methods in a batch RL
scenario and compare our findings with an equivalent experiment from the
arcade environments <dt-cite key="Agarwal2019StrivingFS"></dt-cite>.</p>
<h2>Related Work</h2>
<p>Deep learning has shown to be a useful tool for learning visuomotor
policies that operate directly on raw images. Examples include various
manipulation tasks, where related approaches use either supervised
learning to predict the probability of a successful
grasp <dt-cite key="mahler2017dex,levine2016end,Pinto2015SupersizingSL"></dt-cite>
or learn a reinforcement-learning control policy <dt-cite key="levine2018learning,quillen2018deep,kalashnikov2018scalable"></dt-cite>.</p>
<p>Distributional Q-learning algorithms have been so far a separate line of
research, mainly evaluated on game environments. These algorithms
replace the expected return of an action with a distribution over the
returns and mainly vary by the way they parametrize this distribution.
Bellemare et al. <dt-cite key="bellemare2017distributional"></dt-cite>
express it as a
categorical distribution over a fixed set of equidistant points. Their
algorithm, C51, minimizes the KL-divergence to the projected
distributional Bellman target. A follow-up algorithm, QR-DQN <dt-cite key="dabney2018distributional"></dt-cite>
, approximates the distribution by learning the outputs of the quantile function at a fixed set of points, the quantile midpoints. This latter approach has been extended by IQN <dt-cite key="dabney2018implicit"></dt-cite>,
which reparametrized the critic network to take
as input any probability <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>τ</mi></mrow><annotation encoding="application/x-tex">\tau</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.1132em;">τ</span></span></span></span> and learn the quantile function itself.
Besides this extension, their paper also analyses various risk-sensitive
policies that the distributional formulation enables. In this work, we
apply these advancements to a challenging vision-based real-world
grasping task with a continuous action-space.</p>
<p>Closest to our work is D4PG <dt-cite key="barth2018distributed"></dt-cite>, a distributed and distributional version of DDPG <dt-cite key="Lillicrap2015ContinuousCW"></dt-cite> that
achieves superior performance to the non-distributional version in a
series of simulated continuous-control environments. In contrast to this
work, we analyze a different variant of Q-learning with continuous
action spaces, which allows us to focus on actor-free settings that are
similar to the previous distributional Q-learning algorithms. Besides,
we demonstrate our results on real robots on a challenging vision-based
grasping task.</p>
<h2>Background</h2>
<p>As previously stated, we build our method on top of
QT-Opt <dt-cite key="kalashnikov2018scalable"></dt-cite>, a distributed Q-learning algorithm
suitable for continuous action spaces. QT-Opt is one of the few scalable
deep RL algorithms with demonstrated generalisation performance in a
challenging real-world task. As the original paper shows, it achieves an
impressive 96% vision-based grasp success rate on unseen objects.
Therefore, building on top of QT-Opt is a natural choice for evaluating
value distributions on a challenging real-world task, beyond simulated
environments. Additionally, by directly generalising Q-Learning to
continuous domains, QT-Opt allows us to compare our results with the
existing distributional literature on discrete environments.</p>
<p>In this paper, we consider a standard Markov Decision Process
<dt-cite key="Puterman1994MarkovDP"></dt-cite> formulation
<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mrow><mi mathvariant="script">S</mi></mrow><mo separator="true">,</mo><mrow><mi mathvariant="script">A</mi></mrow><mo separator="true">,</mo><mi>r</mi><mo separator="true">,</mo><mi>p</mi><mo separator="true">,</mo><mi>γ</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">(\mathcal{S}, \mathcal{A}, r, p, \gamma)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mopen">(</span><span class="mord textstyle uncramped"><span class="mord mathcal" style="margin-right:0.075em;">S</span></span><span class="mpunct">,</span><span class="mord textstyle uncramped"><span class="mord mathcal">A</span></span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mpunct">,</span><span class="mord mathit">p</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.05556em;">γ</span><span class="mclose">)</span></span></span></span>, where <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>s</mi><mo>∈</mo><mrow><mi mathvariant="script">S</mi></mrow></mrow><annotation encoding="application/x-tex">s \in \mathcal{S}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.72243em;vertical-align:-0.0391em;"></span><span class="base textstyle uncramped"><span class="mord mathit">s</span><span class="mrel">∈</span><span class="mord textstyle uncramped"><span class="mord mathcal" style="margin-right:0.075em;">S</span></span></span></span></span>
and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi><mo>∈</mo><mrow><mi mathvariant="script">A</mi></mrow></mrow><annotation encoding="application/x-tex">a \in \mathcal{A}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.72243em;vertical-align:-0.0391em;"></span><span class="base textstyle uncramped"><span class="mord mathit">a</span><span class="mrel">∈</span><span class="mord textstyle uncramped"><span class="mord mathcal">A</span></span></span></span></span> denote the state and action spaces, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>r</mi><mo>(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">r(s, a)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mopen">(</span><span class="mord mathit">s</span><span class="mpunct">,</span><span class="mord mathit">a</span><span class="mclose">)</span></span></span></span> is
a deterministic reward function, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>(</mo><mo>⋅</mo><mi mathvariant="normal">∣</mi><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">p(\cdot|s, a)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord">⋅</span><span class="mord mathrm">∣</span><span class="mord mathit">s</span><span class="mpunct">,</span><span class="mord mathit">a</span><span class="mclose">)</span></span></span></span> is the transition
function and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>γ</mi><mo>∈</mo><mo>(</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo>)</mo></mrow><annotation encoding="application/x-tex">\gamma \in (0, 1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.05556em;">γ</span><span class="mrel">∈</span><span class="mopen">(</span><span class="mord mathrm">0</span><span class="mpunct">,</span><span class="mord mathrm">1</span><span class="mclose">)</span></span></span></span> is the discount factor. QT-Opt trains a
parameterized state-action value function <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>Q</mi><mi>θ</mi></msub><mo>(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">Q_\theta(s, a)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">Q</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit">s</span><span class="mpunct">,</span><span class="mord mathit">a</span><span class="mclose">)</span></span></span></span> which is
represented by a neural network with parameters <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span></span>. The
cross-entropy method (CEM) <dt-cite key="Rubinstein2004TheCM"></dt-cite>  is used to iteratively
optimize and select the best action for a given Q-function:</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo>(</mo><mi>s</mi><mo>)</mo><mo>=</mo><msub><mrow><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">g</mi><mspace width="0.16667em"></mspace><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">x</mi></mrow><mrow><mi>a</mi></mrow></msub><msub><mi>Q</mi><mi>θ</mi></msub><mo>(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">\pi_\theta(s)=\mathrm{arg\,max}_{a}Q_\theta(s,a)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">π</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.03588em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit">s</span><span class="mclose">)</span><span class="mrel">=</span><span class=""><span class="mord displaystyle textstyle uncramped"><span class="mord mathrm">a</span><span class="mord mathrm">r</span><span class="mord mathrm" style="margin-right:0.01389em;">g</span><span class="mord mspace thinspace"></span><span class="mord mathrm">m</span><span class="mord mathrm">a</span><span class="mord mathrm">x</span></span><span class="vlist"><span style="top:0.24444em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">a</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit">Q</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit">s</span><span class="mpunct">,</span><span class="mord mathit">a</span><span class="mclose">)</span></span></span></span></span></p>
<p>In order to train the Q-function, a separate process called the &quot;Bellman
Updater&quot; samples transition tuples <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo separator="true">,</mo><mi>r</mi><mo separator="true">,</mo><msup><mi>s</mi><mrow><mi mathvariant="normal">′</mi></mrow></msup><mo>)</mo></mrow><annotation encoding="application/x-tex">(s,a,r,s&#x27;)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.751892em;"></span><span class="strut bottom" style="height:1.001892em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mopen">(</span><span class="mord mathit">s</span><span class="mpunct">,</span><span class="mord mathit">a</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mpunct">,</span><span class="mord"><span class="mord mathit">s</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span> containing the state
<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>s</mi></mrow><annotation encoding="application/x-tex">s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">s</span></span></span></span>, action <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">a</span></span></span></span>, reward <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>r</mi></mrow><annotation encoding="application/x-tex">r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02778em;">r</span></span></span></span>, and next state <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>s</mi><mrow><mi mathvariant="normal">′</mi></mrow></msup></mrow><annotation encoding="application/x-tex">s&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.751892em;"></span><span class="strut bottom" style="height:0.751892em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">s</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> from a replay buffer
and generates Bellman target values according to a clipped Double
Q-learning rule <dt-cite key="Hasselt2010DoubleQ,Sutton:1998:IRL:551283"></dt-cite>:</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mover accent="true"><mrow><mi>Q</mi></mrow><mo>^</mo></mover><mo>(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo separator="true">,</mo><mi>r</mi><mo separator="true">,</mo><msup><mi>s</mi><mrow><mi mathvariant="normal">′</mi></mrow></msup><mo>)</mo><mo>=</mo><mi>r</mi><mo>+</mo><mi>γ</mi><mi>V</mi><mo>(</mo><msup><mi>s</mi><mrow><mi mathvariant="normal">′</mi></mrow></msup><mo>)</mo></mrow><annotation encoding="application/x-tex">\hat{Q}(s,a,r,s&#x27;)=r+\gamma V(s&#x27;)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.9467699999999999em;"></span><span class="strut bottom" style="height:1.19677em;vertical-align:-0.25em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord displaystyle textstyle cramped"><span class="mord mathit">Q</span></span></span><span style="top:-0.25233em;margin-left:0.16668em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>^</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit">s</span><span class="mpunct">,</span><span class="mord mathit">a</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mpunct">,</span><span class="mord"><span class="mord mathit">s</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mrel">=</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mbin">+</span><span class="mord mathit" style="margin-right:0.05556em;">γ</span><span class="mord mathit" style="margin-right:0.22222em;">V</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">s</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span></span></p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mover accent="true"><mrow><mi>Q</mi></mrow><mo>^</mo></mover><mo>(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo separator="true">,</mo><msup><mi>s</mi><mrow><mi mathvariant="normal">′</mi></mrow></msup><mo>)</mo><mo>=</mo><mi>r</mi><mo>(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo>)</mo><mo>+</mo><mi>γ</mi><mi>V</mi><mo>(</mo><msup><mi>s</mi><mrow><mi mathvariant="normal">′</mi></mrow></msup><mo>)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\hat{Q}(s, a, s&#x27;) = r(s,a)+\gamma V(s&#x27;),
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.9467699999999999em;"></span><span class="strut bottom" style="height:1.19677em;vertical-align:-0.25em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord displaystyle textstyle cramped"><span class="mord mathit">Q</span></span></span><span style="top:-0.25233em;margin-left:0.16668em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>^</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit">s</span><span class="mpunct">,</span><span class="mord mathit">a</span><span class="mpunct">,</span><span class="mord"><span class="mord mathit">s</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mrel">=</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mopen">(</span><span class="mord mathit">s</span><span class="mpunct">,</span><span class="mord mathit">a</span><span class="mclose">)</span><span class="mbin">+</span><span class="mord mathit" style="margin-right:0.05556em;">γ</span><span class="mord mathit" style="margin-right:0.22222em;">V</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">s</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mpunct">,</span></span></span></span></span></p>
<p>where <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>V</mi><mo>(</mo><msup><mi>s</mi><mrow><mi mathvariant="normal">′</mi></mrow></msup><mo>)</mo><mo>=</mo><msub><mi>Q</mi><mrow><mover accent="true"><mrow><msub><mi>θ</mi><mn>1</mn></msub></mrow><mo>¯</mo></mover></mrow></msub><mo>(</mo><msup><mi>s</mi><mrow><mi mathvariant="normal">′</mi></mrow></msup><mo separator="true">,</mo><msub><mi>π</mi><mrow><mover accent="true"><mrow><msub><mi>θ</mi><mn>2</mn></msub></mrow><mo>¯</mo></mover></mrow></msub><mo>(</mo><msup><mi>s</mi><mrow><mi mathvariant="normal">′</mi></mrow></msup><mo>)</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">V(s&#x27;)=Q_{\bar{\theta_1}}(s&#x27;,\pi_{\bar{\theta_2}}(s&#x27;))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.751892em;"></span><span class="strut bottom" style="height:1.0939459999999999em;vertical-align:-0.34205399999999986em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.22222em;">V</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">s</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mrel">=</span><span class="mord"><span class="mord mathit">Q</span><span class="vlist"><span style="top:0.23705399999999988em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord scriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:0.02778em;">θ</span><span class="vlist"><span style="top:0.15em;margin-right:0.07142857142857144em;margin-left:-0.02778em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathrm">1</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span><span style="top:-0.26343999999999995em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>¯</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathit">s</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">π</span><span class="vlist"><span style="top:0.23705399999999988em;margin-right:0.05em;margin-left:-0.03588em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord scriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:0.02778em;">θ</span><span class="vlist"><span style="top:0.15em;margin-right:0.07142857142857144em;margin-left:-0.02778em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span><span style="top:-0.26343999999999995em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>¯</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathit">s</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mclose">)</span></span></span></span>, and
<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mover accent="true"><mrow><msub><mi>θ</mi><mn>1</mn></msub></mrow><mo>¯</mo></mover></mrow><annotation encoding="application/x-tex">\bar{\theta_1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8312199999999998em;"></span><span class="strut bottom" style="height:0.9812199999999999em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord textstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:0.02778em;">θ</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.02778em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathrm">1</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span><span style="top:-0.26343999999999995em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>¯</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mover accent="true"><mrow><msub><mi>θ</mi><mn>2</mn></msub></mrow><mo>¯</mo></mover></mrow><annotation encoding="application/x-tex">\bar{\theta_2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8312199999999998em;"></span><span class="strut bottom" style="height:0.9812199999999999em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord textstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:0.02778em;">θ</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.02778em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span><span style="top:-0.26343999999999995em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>¯</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> are the parameters of two delayed
target networks. These target values are pushed to another replay buffer
<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="script">D</mi></mrow></mrow><annotation encoding="application/x-tex">\mathcal{D}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathcal" style="margin-right:0.02778em;">D</span></span></span></span></span>, and a separate training process optimizes the Q-function
against a training objective:</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="script">E</mi></mrow><mo>(</mo><mi>θ</mi><mo>)</mo><mo>=</mo><msub><mrow><mi mathvariant="double-struck">E</mi></mrow><mrow><mo>(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo separator="true">,</mo><msup><mi>s</mi><mrow><mi mathvariant="normal">′</mi></mrow></msup><mo>)</mo><mo>∼</mo><mrow><mi mathvariant="script">D</mi></mrow></mrow></msub><mrow><mo fence="true">[</mo><mi>D</mi><mo>(</mo><msub><mi>Q</mi><mi>θ</mi></msub><mo>(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo>)</mo><mo separator="true">,</mo><mover accent="true"><mrow><mi>Q</mi></mrow><mo>^</mo></mover><mo>(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo separator="true">,</mo><msup><mi>s</mi><mrow><mi mathvariant="normal">′</mi></mrow></msup><mo>)</mo><mo>)</mo><mo fence="true">]</mo></mrow><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\mathcal{E}(\theta)=\mathbb{E}_{(s,a,s&#x27;)\sim\mathcal{D}}\left[D(Q_\theta(s,a),\hat{Q}(s, a, s&#x27;))\right],
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.15em;"></span><span class="strut bottom" style="height:1.80002em;vertical-align:-0.65002em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord displaystyle textstyle uncramped"><span class="mord mathcal" style="margin-right:0.08944em;">E</span></span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span><span class="mrel">=</span><span class=""><span class="mord displaystyle textstyle uncramped"><span class="mord mathbb">E</span></span><span class="vlist"><span style="top:0.18019999999999992em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mopen">(</span><span class="mord mathit">s</span><span class="mpunct">,</span><span class="mord mathit">a</span><span class="mpunct">,</span><span class="mord"><span class="mord mathit">s</span><span class="vlist"><span style="top:-0.289em;margin-right:0.07142857142857144em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord scriptscriptstyle cramped"><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mrel">∼</span><span class="mord scriptstyle cramped"><span class="mord mathcal" style="margin-right:0.02778em;">D</span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="minner displaystyle textstyle uncramped"><span class="style-wrap reset-textstyle textstyle uncramped" style="top:0em;"><span class="delimsizing size2">[</span></span><span class="mord mathit" style="margin-right:0.02778em;">D</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">Q</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit">s</span><span class="mpunct">,</span><span class="mord mathit">a</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord displaystyle textstyle cramped"><span class="mord mathit">Q</span></span></span><span style="top:-0.25233em;margin-left:0.16668em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>^</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit">s</span><span class="mpunct">,</span><span class="mord mathit">a</span><span class="mpunct">,</span><span class="mord"><span class="mord mathit">s</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mclose">)</span><span class="style-wrap reset-textstyle textstyle uncramped" style="top:0em;"><span class="delimsizing size2">]</span></span></span><span class="mpunct">,</span></span></span></span></span></p>
<p>where <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02778em;">D</span></span></span></span> is a divergence metric.</p>
<p>In particular, the cross-entropy loss is chosen for <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02778em;">D</span></span></span></span>, and the output
of the network is passed through a sigmoid activation to ensure that the
predicted Q-values are inside the unit interval.</p>
<h2>Quantile QT-Opt (Q2-Opt)</h2>
<p>In Q2-Opt (Figure 1) the value function no longer
predicts a scalar value, but rather a vector <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mrow><mi mathvariant="bold">q</mi></mrow><mi>θ</mi></msub><mo>(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo separator="true">,</mo><mrow><mi>τ</mi></mrow><mo>)</mo></mrow><annotation encoding="application/x-tex">\mathbf{q}_\theta(s,a, \mathbf{\tau})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class=""><span class="mord textstyle uncramped"><span class="mord mathbf">q</span></span><span class="vlist"><span style="top:0.24444em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit">s</span><span class="mpunct">,</span><span class="mord mathit">a</span><span class="mpunct">,</span><span class="mord textstyle uncramped"><span class="mord mathit" style="margin-right:0.1132em;">τ</span></span><span class="mclose">)</span></span></span></span>
that predicts the quantile function output for a vector of input
probabilities <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi>τ</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{\tau}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathit" style="margin-right:0.1132em;">τ</span></span></span></span></span>, with <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mrow><mi>τ</mi></mrow><mi>i</mi></msub><mo>∈</mo><mo>[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo>]</mo></mrow><annotation encoding="application/x-tex">\mathbf{\tau}_i \in [0, 1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class=""><span class="mord textstyle uncramped"><span class="mord mathit" style="margin-right:0.1132em;">τ</span></span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">∈</span><span class="mopen">[</span><span class="mord mathrm">0</span><span class="mpunct">,</span><span class="mord mathrm">1</span><span class="mclose">]</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>i</mi><mo>=</mo><mn>1</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">i=1,\ldots,N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit">i</span><span class="mrel">=</span><span class="mord mathrm">1</span><span class="mpunct">,</span><span class="minner">…</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.10903em;">N</span></span></span></span>.
Thus the <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.65952em;"></span><span class="strut bottom" style="height:0.65952em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">i</span></span></span></span>-th element of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mrow><mi mathvariant="bold">q</mi></mrow><mi>i</mi></msub><mo>(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo separator="true">,</mo><mrow><mi>τ</mi></mrow><mo>)</mo></mrow><annotation encoding="application/x-tex">\mathbf{q}_i(s,a, \mathbf{\tau})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class=""><span class="mord textstyle uncramped"><span class="mord mathbf">q</span></span><span class="vlist"><span style="top:0.24444em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit">s</span><span class="mpunct">,</span><span class="mord mathit">a</span><span class="mpunct">,</span><span class="mord textstyle uncramped"><span class="mord mathit" style="margin-right:0.1132em;">τ</span></span><span class="mclose">)</span></span></span></span> approximates <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>F</mi><mrow><mi>s</mi><mo separator="true">,</mo><mi>a</mi></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mo>(</mo><msub><mrow><mi>τ</mi></mrow><mi>i</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">F^{-1}_{s, a}(\mathbf{\tau}_i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8141079999999999em;"></span><span class="strut bottom" style="height:1.197216em;vertical-align:-0.383108em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">F</span><span class="vlist"><span style="top:0.24700000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">s</span><span class="mpunct">,</span><span class="mord mathit">a</span></span></span></span><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord">−</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class=""><span class="mord textstyle uncramped"><span class="mord mathit" style="margin-right:0.1132em;">τ</span></span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span>, where
<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>F</mi><mrow><mi>s</mi><mo separator="true">,</mo><mi>a</mi></mrow></msub></mrow><annotation encoding="application/x-tex">F_{s, a}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">F</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.13889em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">s</span><span class="mpunct">,</span><span class="mord mathit">a</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> is the CDF of the value distribution belonging to the state
action pair <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">(s, a)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mopen">(</span><span class="mord mathit">s</span><span class="mpunct">,</span><span class="mord mathit">a</span><span class="mclose">)</span></span></span></span>. In practice, this means that the neural network
has a multi-headed output. However, unlike QT-Opt where CEM optimizes
directly over the Q-values, in Quantile QT-Opt, CEM maximizes a scoring
function <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ψ</mi><mo>:</mo><msup><mrow><mrow><mi mathvariant="double-struck">R</mi></mrow></mrow><mi>N</mi></msup><mo>→</mo><mrow><mrow><mi mathvariant="double-struck">R</mi></mrow></mrow></mrow><annotation encoding="application/x-tex">\psi: {\mathbb{R}}^N \to {\mathbb{R}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8413309999999999em;"></span><span class="strut bottom" style="height:1.035771em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">ψ</span><span class="mrel">:</span><span class="mord"><span class="mord textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathbb">R</span></span></span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:0.10903em;">N</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">→</span><span class="mord textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathbb">R</span></span></span></span></span></span> that maps the vector
<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="bold">q</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{q}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.44444em;"></span><span class="strut bottom" style="height:0.63888em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathbf">q</span></span></span></span></span> to a score <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ψ</mi><mo>(</mo><mrow><mi mathvariant="bold">q</mi></mrow><mo>)</mo></mrow><annotation encoding="application/x-tex">\psi(\mathbf{q})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">ψ</span><span class="mopen">(</span><span class="mord textstyle uncramped"><span class="mord mathbf">q</span></span><span class="mclose">)</span></span></span></span>:</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo>(</mo><mi>s</mi><mo separator="true">,</mo><mrow><mi>τ</mi></mrow><mo>)</mo><mo>=</mo><msub><mrow><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">g</mi><mspace width="0.16667em"></mspace><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">x</mi></mrow><mrow><mi>a</mi></mrow></msub><mi>ψ</mi><mo>(</mo><msub><mrow><mi mathvariant="bold">q</mi></mrow><mi>θ</mi></msub><mo>(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo separator="true">,</mo><mrow><mi>τ</mi></mrow><mo>)</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">\pi_\theta(s, \mathbf{\tau})=\mathrm{arg\,max}_{a}\psi(\mathbf{q}_\theta(s,a, \mathbf{\tau}))
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">π</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.03588em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit">s</span><span class="mpunct">,</span><span class="mord displaystyle textstyle uncramped"><span class="mord mathit" style="margin-right:0.1132em;">τ</span></span><span class="mclose">)</span><span class="mrel">=</span><span class=""><span class="mord displaystyle textstyle uncramped"><span class="mord mathrm">a</span><span class="mord mathrm">r</span><span class="mord mathrm" style="margin-right:0.01389em;">g</span><span class="mord mspace thinspace"></span><span class="mord mathrm">m</span><span class="mord mathrm">a</span><span class="mord mathrm">x</span></span><span class="vlist"><span style="top:0.24444em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">a</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathit" style="margin-right:0.03588em;">ψ</span><span class="mopen">(</span><span class=""><span class="mord displaystyle textstyle uncramped"><span class="mord mathbf">q</span></span><span class="vlist"><span style="top:0.24444em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit">s</span><span class="mpunct">,</span><span class="mord mathit">a</span><span class="mpunct">,</span><span class="mord displaystyle textstyle uncramped"><span class="mord mathit" style="margin-right:0.1132em;">τ</span></span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></span></p>
<p>Similarly, the target values produced by the “Bellman updater” are
vectorized using a generalization of the clipped Double Q-learning rule
from QT-Opt:</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mover accent="true"><mrow><mrow><mi mathvariant="bold">q</mi></mrow></mrow><mo>^</mo></mover><mo>(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo separator="true">,</mo><msup><mi>s</mi><mrow><mi mathvariant="normal">′</mi></mrow></msup><mo separator="true">,</mo><msup><mi>τ</mi><mrow><mi mathvariant="normal">′</mi></mrow></msup><mo separator="true">,</mo><msup><mi>τ</mi><mrow><mi mathvariant="normal">′</mi><mi mathvariant="normal">′</mi></mrow></msup><mo>)</mo><mo>=</mo><mi>r</mi><mo>(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo>)</mo><mrow><mn mathvariant="bold">1</mn></mrow><mo>+</mo><mi>γ</mi><mrow><mi mathvariant="bold">v</mi></mrow><mo>(</mo><msup><mi>s</mi><mrow><mi mathvariant="normal">′</mi></mrow></msup><mo separator="true">,</mo><mi>τ</mi><mo separator="true">,</mo><msup><mi>τ</mi><mrow><mi mathvariant="normal">′</mi></mrow></msup><mo>)</mo></mrow><annotation encoding="application/x-tex">\hat{\mathbf{q}}(s, a, s&#x27;, \tau&#x27;, \tau&#x27;&#x27;) = r(s,a)\mathbf{1}+\gamma\mathbf{v}(s&#x27;, \tau, \tau&#x27;)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.801892em;"></span><span class="strut bottom" style="height:1.051892em;vertical-align:-0.25em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord displaystyle textstyle cramped"><span class="mord displaystyle textstyle cramped"><span class="mord mathbf">q</span></span></span></span><span style="top:-0.013440000000000007em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>^</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit">s</span><span class="mpunct">,</span><span class="mord mathit">a</span><span class="mpunct">,</span><span class="mord"><span class="mord mathit">s</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.1132em;">τ</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.1132em;">τ</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mrel">=</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mopen">(</span><span class="mord mathit">s</span><span class="mpunct">,</span><span class="mord mathit">a</span><span class="mclose">)</span><span class="mord displaystyle textstyle uncramped"><span class="mord mathbf">1</span></span><span class="mbin">+</span><span class="mord mathit" style="margin-right:0.05556em;">γ</span><span class="mord displaystyle textstyle uncramped"><span class="mord mathbf" style="margin-right:0.01597em;">v</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathit">s</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.1132em;">τ</span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.1132em;">τ</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span></span></p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="bold">v</mi></mrow><mo>(</mo><msup><mi>s</mi><mrow><mi mathvariant="normal">′</mi></mrow></msup><mo separator="true">,</mo><mi>τ</mi><mo separator="true">,</mo><msup><mi>τ</mi><mrow><mi mathvariant="normal">′</mi></mrow></msup><mo>)</mo><mo>=</mo><msub><mrow><mi mathvariant="bold">q</mi></mrow><mrow><msub><mover accent="true"><mrow><mi>θ</mi></mrow><mo>¯</mo></mover><mn>1</mn></msub></mrow></msub><mo>(</mo><msup><mi>s</mi><mrow><mi mathvariant="normal">′</mi></mrow></msup><mo separator="true">,</mo><msub><mi>π</mi><mrow><msub><mover accent="true"><mrow><mi>θ</mi></mrow><mo>¯</mo></mover><mn>2</mn></msub></mrow></msub><mo>(</mo><msup><mi>s</mi><mrow><mi mathvariant="normal">′</mi></mrow></msup><mo separator="true">,</mo><msup><mi>τ</mi><mrow><mi mathvariant="normal">′</mi><mi mathvariant="normal">′</mi></mrow></msup><mo>)</mo><mo separator="true">,</mo><msup><mi>τ</mi><mrow><mi mathvariant="normal">′</mi></mrow></msup><mo>)</mo></mrow><annotation encoding="application/x-tex">\mathbf{v}(s&#x27;, \tau, \tau&#x27;) = \mathbf{q}_{\bar{\theta}_1}(s&#x27;,\pi_{\bar{\theta}_2}(s&#x27;, \tau&#x27;&#x27;), \tau&#x27;)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.801892em;"></span><span class="strut bottom" style="height:1.151332em;vertical-align:-0.34944em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord displaystyle textstyle uncramped"><span class="mord mathbf" style="margin-right:0.01597em;">v</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathit">s</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.1132em;">τ</span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.1132em;">τ</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mrel">=</span><span class=""><span class="mord displaystyle textstyle uncramped"><span class="mord mathbf">q</span></span><span class="vlist"><span style="top:0.24444em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord"><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span><span style="top:-0.26343999999999995em;margin-left:0.16668em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>¯</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="vlist"><span style="top:0.15em;margin-right:0.07142857142857144em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathrm">1</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathit">s</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">π</span><span class="vlist"><span style="top:0.23705399999999988em;margin-right:0.05em;margin-left:-0.03588em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord"><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span><span style="top:-0.26343999999999995em;margin-left:0.16668em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>¯</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="vlist"><span style="top:0.15em;margin-right:0.07142857142857144em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathit">s</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.1132em;">τ</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.1132em;">τ</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span></span></p>
<p>where \vone is a vector of ones, and, as before, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mover accent="true"><mrow><mi>θ</mi></mrow><mo>¯</mo></mover><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">\bar{\theta}_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8312199999999998em;"></span><span class="strut bottom" style="height:0.9812199999999999em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord textstyle cramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span><span style="top:-0.26343999999999995em;margin-left:0.16668em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>¯</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathrm">1</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mover accent="true"><mrow><mi>θ</mi></mrow><mo>¯</mo></mover><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\bar{\theta}_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8312199999999998em;"></span><span class="strut bottom" style="height:0.9812199999999999em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord textstyle cramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span><span style="top:-0.26343999999999995em;margin-left:0.16668em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>¯</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> are the parameters of two delayed target networks. Even though this update rule has not been considered so far in the distributional RL literature, we find it  effective in reducing the overestimation in the predictions.</p>
<p>In the following sections, we present two versions of Q2-Opt based on two recently introduced distributional algorithms: QR-DQN and IQN. The main differences between them arise from the inputs <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>τ</mi><mo separator="true">,</mo><msup><mi>τ</mi><mrow><mi mathvariant="normal">′</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\tau, \tau&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.751892em;"></span><span class="strut bottom" style="height:0.946332em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.1132em;">τ</span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.1132em;">τ</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>, and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>τ</mi><mrow><mi mathvariant="normal">′</mi><mi mathvariant="normal">′</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\tau&#x27;&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.751892em;"></span><span class="strut bottom" style="height:0.751892em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.1132em;">τ</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> that are used. To avoid overloading our notation, from now on we omit the parameter subscript in <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mrow><mi mathvariant="bold">q</mi></mrow><mi>θ</mi></msub><mo separator="true">,</mo><msub><mover accent="true"><mrow><mrow><mi mathvariant="bold">q</mi></mrow></mrow><mo>^</mo></mover><mrow><mover accent="true"><mrow><mi>θ</mi></mrow><mo>^</mo></mover></mrow></msub></mrow><annotation encoding="application/x-tex">\mathbf{q}_\theta, \hat{\mathbf{q}}_{\hat{\theta}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.70788em;"></span><span class="strut bottom" style="height:1.0335959999999997em;vertical-align:-0.3257159999999999em;"></span><span class="base textstyle uncramped"><span class=""><span class="mord textstyle uncramped"><span class="mord mathbf">q</span></span><span class="vlist"><span style="top:0.24444em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord textstyle cramped"><span class="mord textstyle cramped"><span class="mord mathbf">q</span></span></span></span><span style="top:-0.013440000000000007em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>^</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="vlist"><span style="top:0.3257159999999999em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span><span style="top:-0.26343999999999995em;margin-left:0.16668em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>^</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> and replace it with an index into these vectors <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mrow><mi mathvariant="bold">q</mi></mrow><mi>i</mi></msub><mo separator="true">,</mo><msub><mover accent="true"><mrow><mrow><mi mathvariant="bold">q</mi></mrow></mrow><mo>^</mo></mover><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{q}_i, \hat{\mathbf{q}}_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.70788em;"></span><span class="strut bottom" style="height:1.088428em;vertical-align:-0.380548em;"></span><span class="base textstyle uncramped"><span class=""><span class="mord textstyle uncramped"><span class="mord mathbf">q</span></span><span class="vlist"><span style="top:0.24444em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord textstyle cramped"><span class="mord textstyle cramped"><span class="mord mathbf">q</span></span></span></span><span style="top:-0.013440000000000007em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>^</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="vlist"><span style="top:0.24444000000000002em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>.</p>
<p><strong>Quantile Regression QT-Opt (Q2R-Opt)</strong></p>
<p>In Quantile Regression QT-Opt (Q2R-Opt), the vectors <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>τ</mi><mo separator="true">,</mo><msup><mi>τ</mi><mrow><mi mathvariant="normal">′</mi></mrow></msup><mo separator="true">,</mo><msup><mi>τ</mi><mrow><mi mathvariant="normal">′</mi><mi mathvariant="normal">′</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\tau, \tau&#x27;, \tau&#x27;&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.751892em;"></span><span class="strut bottom" style="height:0.946332em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.1132em;">τ</span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.1132em;">τ</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.1132em;">τ</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> in <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="bold">q</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{q}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.44444em;"></span><span class="strut bottom" style="height:0.63888em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathbf">q</span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mover accent="true"><mrow><mrow><mi mathvariant="bold">q</mi></mrow></mrow><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{\mathbf{q}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.70788em;"></span><span class="strut bottom" style="height:0.90232em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord textstyle cramped"><span class="mord textstyle cramped"><span class="mord mathbf">q</span></span></span></span><span style="top:-0.013440000000000007em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>^</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> are fixed. They all contain <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10903em;">N</span></span></span></span> quantile midpoints of the value distribution. Concretely, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mrow><mi mathvariant="bold">q</mi></mrow><mi>i</mi></msub><mo>(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo separator="true">,</mo><mi>τ</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">\mathbf{q}_i(s,a,\tau)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class=""><span class="mord textstyle uncramped"><span class="mord mathbf">q</span></span><span class="vlist"><span style="top:0.24444em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit">s</span><span class="mpunct">,</span><span class="mord mathit">a</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.1132em;">τ</span><span class="mclose">)</span></span></span></span> is assigned the fixed quantile target <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>τ</mi><mi>i</mi></msub><mo>=</mo><mfrac><mrow><msub><mover accent="true"><mrow><mi>τ</mi></mrow><mo>¯</mo></mover><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mover accent="true"><mrow><mi>τ</mi></mrow><mo>¯</mo></mover><mrow><mi>i</mi></mrow></msub></mrow><mrow><mn>2</mn></mrow></mfrac></mrow><annotation encoding="application/x-tex">\tau_i = \frac{\bar{\tau}_{i-1} + \bar{\tau}_{i}}{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8649960000000001em;"></span><span class="strut bottom" style="height:1.209996em;vertical-align:-0.345em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.1132em;">τ</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.1132em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">=</span><span class="mord reset-textstyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:0.345em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathrm">2</span></span></span></span><span style="top:-0.22999999999999998em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-0.456665em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord"><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.1132em;">τ</span></span></span><span style="top:0em;margin-left:0.05556em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>¯</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="vlist"><span style="top:0.15000000000000002em;margin-right:0.07142857142857144em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord scriptscriptstyle cramped"><span class="mord mathit">i</span><span class="mbin">−</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mbin">+</span><span class="mord"><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.1132em;">τ</span></span></span><span style="top:0em;margin-left:0.05556em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>¯</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="vlist"><span style="top:0.15em;margin-right:0.07142857142857144em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord scriptscriptstyle cramped"><span class="mord mathit">i</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span> with <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mover accent="true"><mrow><mi>τ</mi></mrow><mo>¯</mo></mover><mi>i</mi></msub><mo>=</mo><mfrac><mrow><mi>i</mi></mrow><mrow><mi>N</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\bar{\tau}_i = \frac{i}{N}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.855664em;"></span><span class="strut bottom" style="height:1.200664em;vertical-align:-0.345em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord textstyle cramped"><span class="mord mathit" style="margin-right:0.1132em;">τ</span></span></span><span style="top:0em;margin-left:0.05556em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>¯</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">=</span><span class="mord reset-textstyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:0.345em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.10903em;">N</span></span></span></span><span style="top:-0.22999999999999998em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-0.394em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">i</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span>. The scoring function <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ψ</mi><mo>(</mo><mo>⋅</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">\psi(\cdot)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">ψ</span><span class="mopen">(</span><span class="mord">⋅</span><span class="mclose">)</span></span></span></span> takes the mean of this vector, reducing the <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10903em;">N</span></span></span></span> quantile midpoints to the expected value of the distribution. Because <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>τ</mi><mo separator="true">,</mo><msup><mi>τ</mi><mrow><mi mathvariant="normal">′</mi></mrow></msup><mo separator="true">,</mo><msup><mi>τ</mi><mrow><mi mathvariant="normal">′</mi><mi mathvariant="normal">′</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\tau, \tau&#x27;, \tau&#x27;&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.751892em;"></span><span class="strut bottom" style="height:0.946332em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.1132em;">τ</span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.1132em;">τ</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.1132em;">τ</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> are always fixed we consider them implicit and omit adding them as an argument to <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="bold">q</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{q}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.44444em;"></span><span class="strut bottom" style="height:0.63888em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathbf">q</span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mover accent="true"><mrow><mrow><mi mathvariant="bold">q</mi></mrow></mrow><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{\mathbf{q}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.70788em;"></span><span class="strut bottom" style="height:0.90232em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord textstyle cramped"><span class="mord textstyle cramped"><span class="mord mathbf">q</span></span></span></span><span style="top:-0.013440000000000007em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>^</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> for Q2R-Opt.</p>
<p>The quantile heads are optimized by minimizing the Huber <dt-cite key="huber1964"></dt-cite>
quantile regression loss:</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>ρ</mi><mi>τ</mi><mi>κ</mi></msubsup><mo>(</mo><msub><mi>δ</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>)</mo><mo>=</mo><mi mathvariant="normal">∣</mi><mi>τ</mi><mo>−</mo><mrow><mi mathvariant="double-struck">I</mi></mrow><mo>{</mo><msub><mi>δ</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>&lt;</mo><mn>0</mn><mo>}</mo><mi mathvariant="normal">∣</mi><mfrac><mrow><msub><mrow><mi mathvariant="script">L</mi></mrow><mi>κ</mi></msub><mo>(</mo><msub><mi>δ</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>)</mo></mrow><mrow><mi>κ</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\rho_\tau^\kappa(\delta_{ij}) = |\tau - \mathbb{I}\{\delta_{ij} &lt; 0\}| \frac{\mathcal{L}_\kappa(\delta_{ij})}{\kappa}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.427em;"></span><span class="strut bottom" style="height:2.113em;vertical-align:-0.686em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord"><span class="mord mathit">ρ</span><span class="vlist"><span style="top:0.247em;margin-left:0em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.1132em;">τ</span></span></span><span style="top:-0.4129999999999999em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">κ</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.03785em;">δ</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.03785em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mrel">=</span><span class="mord mathrm">∣</span><span class="mord mathit" style="margin-right:0.1132em;">τ</span><span class="mbin">−</span><span class="mord displaystyle textstyle uncramped"><span class="mord mathbb">I</span></span><span class="mopen">{</span><span class="mord"><span class="mord mathit" style="margin-right:0.03785em;">δ</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.03785em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">&lt;</span><span class="mord mathrm">0</span><span class="mclose">}</span><span class="mord mathrm">∣</span><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:0.686em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord textstyle cramped"><span class="mord mathit">κ</span></span></span></span><span style="top:-0.22999999999999998em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-0.677em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord textstyle uncramped"><span class=""><span class="mord textstyle uncramped"><span class="mord mathcal">L</span></span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">κ</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.03785em;">δ</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.03785em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span></span></p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mrow><mi mathvariant="script">L</mi></mrow><mi>κ</mi></msub><mo>(</mo><msub><mi>δ</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>)</mo><mo>=</mo><mrow><mo fence="true">{</mo><mtable><mtr><mtd><mrow><mfrac><mrow><mn>1</mn></mrow><mrow><mn>2</mn></mrow></mfrac><msubsup><mi>δ</mi><mrow><mi>i</mi><mi>j</mi></mrow><mn>2</mn></msubsup><mo separator="true">,</mo></mrow></mtd><mtd><mrow><mtext><mi mathvariant="normal">i</mi><mi mathvariant="normal">f</mi><mtext> </mtext></mtext><mi mathvariant="normal">∣</mi><msub><mi>δ</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mi mathvariant="normal">∣</mi><mo>≤</mo><mi>κ</mi></mrow></mtd></mtr><mtr><mtd><mrow><mi>κ</mi><mo>(</mo><mi mathvariant="normal">∣</mi><msub><mi>δ</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mi mathvariant="normal">∣</mi><mo>−</mo><mfrac><mrow><mn>1</mn></mrow><mrow><mn>2</mn></mrow></mfrac><mi>κ</mi><mo>)</mo><mo separator="true">,</mo></mrow></mtd><mtd><mrow><mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">h</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">w</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">s</mi><mi mathvariant="normal">e</mi></mtext></mrow></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}_\kappa(\delta_{ij}) = 
\begin{cases}
    \frac{1}{2} \delta_{ij}^2, &amp; \text{if } |\delta_{ij}| \leq \kappa \\ 
    \kappa (|\delta_{ij}| - \frac{1}{2}\kappa),   &amp; \text{otherwise} 
\end{cases}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:2.35002em;"></span><span class="strut bottom" style="height:4.2000399999999996em;vertical-align:-1.8500199999999998em;"></span><span class="base displaystyle textstyle uncramped"><span class=""><span class="mord displaystyle textstyle uncramped"><span class="mord mathcal">L</span></span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">κ</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.03785em;">δ</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.03785em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mrel">=</span><span class="minner displaystyle textstyle uncramped"><span class="style-wrap reset-textstyle textstyle uncramped"><span class="delimsizing mult"><span class="vlist"><span style="top:0.9500099999999998em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="delimsizinginner delim-size4"><span>⎩</span></span></span><span style="top:0.9500099999999998em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="delimsizinginner delim-size4"><span>⎪</span></span></span><span style="top:-0.000010000000000287557em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="delimsizinginner delim-size4"><span>⎨</span></span></span><span style="top:-1.1500100000000002em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="delimsizinginner delim-size4"><span>⎪</span></span></span><span style="top:-1.4500200000000003em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="delimsizinginner delim-size4"><span>⎧</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist"><span style="top:-0.9359999999999999em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord displaystyle textstyle uncramped"><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:0.686em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord textstyle cramped"><span class="mord mathrm">2</span></span></span></span><span style="top:-0.22999999999999998em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-0.677em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mord"><span class="mord mathit" style="margin-right:0.03785em;">δ</span><span class="vlist"><span style="top:0.24700000000000003em;margin-left:-0.03785em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span></span><span style="top:-0.41300000000000003em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span></span></span><span style="top:1.07144em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord displaystyle textstyle uncramped"><span class="mord mathit">κ</span><span class="mopen">(</span><span class="mord mathrm">∣</span><span class="mord"><span class="mord mathit" style="margin-right:0.03785em;">δ</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.03785em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathrm">∣</span><span class="mbin">−</span><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:0.686em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord textstyle cramped"><span class="mord mathrm">2</span></span></span></span><span style="top:-0.22999999999999998em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-0.677em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mord mathit">κ</span><span class="mclose">)</span><span class="mpunct">,</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="arraycolsep" style="width:1em;"></span><span class="col-align-l"><span class="vlist"><span style="top:-0.9359999999999999em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord displaystyle textstyle uncramped"><span class="text mord displaystyle textstyle uncramped"><span class="mord mathrm">i</span><span class="mord mathrm" style="margin-right:0.07778em;">f</span><span class="mord mspace"> </span></span><span class="mord mathrm">∣</span><span class="mord"><span class="mord mathit" style="margin-right:0.03785em;">δ</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.03785em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathrm">∣</span><span class="mrel">≤</span><span class="mord mathit">κ</span></span></span><span style="top:1.07144em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord displaystyle textstyle uncramped"><span class="text mord displaystyle textstyle uncramped"><span class="mord mathrm">o</span><span class="mord mathrm">t</span><span class="mord mathrm">h</span><span class="mord mathrm">e</span><span class="mord mathrm">r</span><span class="mord mathrm" style="margin-right:0.01389em;">w</span><span class="mord mathrm">i</span><span class="mord mathrm">s</span><span class="mord mathrm">e</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span></span></p>
<p>for all the pairwise TD-errors:</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>δ</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><msub><mover accent="true"><mrow><mrow><mi mathvariant="bold">q</mi></mrow></mrow><mo>^</mo></mover><mi>j</mi></msub><mo>(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo separator="true">,</mo><msup><mi>s</mi><mrow><mi mathvariant="normal">′</mi></mrow></msup><mo>)</mo><mo>−</mo><msub><mrow><mi mathvariant="bold">q</mi></mrow><mi>i</mi></msub><mo>(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">\delta_{ij} = \hat{\mathbf{q}}_j(s,a,s&#x27;) - \mathbf{q}_i(s, a)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.801892em;"></span><span class="strut bottom" style="height:1.1824400000000002em;vertical-align:-0.380548em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.03785em;">δ</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.03785em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">=</span><span class="mord"><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord displaystyle textstyle cramped"><span class="mord displaystyle textstyle cramped"><span class="mord mathbf">q</span></span></span></span><span style="top:-0.013440000000000007em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>^</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="vlist"><span style="top:0.24444000000000002em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit">s</span><span class="mpunct">,</span><span class="mord mathit">a</span><span class="mpunct">,</span><span class="mord"><span class="mord mathit">s</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mbin">−</span><span class=""><span class="mord displaystyle textstyle uncramped"><span class="mord mathbf">q</span></span><span class="vlist"><span style="top:0.24444em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit">s</span><span class="mpunct">,</span><span class="mord mathit">a</span><span class="mclose">)</span></span></span></span></span></p>
<p>Thus, the network is trained to minimize the loss function:</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="script">E</mi></mrow><mo>(</mo><mi>θ</mi><mo>)</mo><mo>=</mo><msub><mrow><mrow><mi mathvariant="double-struck">E</mi></mrow></mrow><mrow><mo>(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo separator="true">,</mo><msup><mi>s</mi><mrow><mi mathvariant="normal">′</mi></mrow></msup><mo>)</mo><mo>∼</mo><mrow><mi mathvariant="script">D</mi></mrow></mrow></msub><mrow><mo fence="true">[</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><msub><mrow><mrow><mi mathvariant="double-struck">E</mi></mrow></mrow><mi>j</mi></msub><mo>[</mo><msubsup><mi>ρ</mi><mrow><msub><mover accent="true"><mrow><mi>τ</mi></mrow><mo>^</mo></mover><mi>i</mi></msub></mrow><mi>κ</mi></msubsup><mo>(</mo><msub><mi>δ</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>)</mo><mo>]</mo><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{E}(\theta)={\mathbb{E}}_{(s,a,s&#x27;)\sim\mathcal{D}}\left[\sum_{i=1}^N {\mathbb{E}}_j[\rho_{\hat{\tau}_i}^\kappa(\delta_{ij})]\right]
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.8283360000000002em;"></span><span class="strut bottom" style="height:3.106005em;vertical-align:-1.277669em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord displaystyle textstyle uncramped"><span class="mord mathcal" style="margin-right:0.08944em;">E</span></span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord"><span class="mord displaystyle textstyle uncramped"><span class="mord displaystyle textstyle uncramped"><span class="mord mathbb">E</span></span></span><span class="vlist"><span style="top:0.18019999999999992em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mopen">(</span><span class="mord mathit">s</span><span class="mpunct">,</span><span class="mord mathit">a</span><span class="mpunct">,</span><span class="mord"><span class="mord mathit">s</span><span class="vlist"><span style="top:-0.289em;margin-right:0.07142857142857144em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord scriptscriptstyle cramped"><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mrel">∼</span><span class="mord scriptstyle cramped"><span class="mord mathcal" style="margin-right:0.02778em;">D</span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="minner displaystyle textstyle uncramped"><span class="style-wrap reset-textstyle textstyle uncramped" style="top:0em;"><span class="delimsizing size4">[</span></span><span class="mop op-limits"><span class="vlist"><span style="top:1.1776689999999999em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mrel">=</span><span class="mord mathrm">1</span></span></span></span><span style="top:-0.000005000000000143778em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span><span class="op-symbol large-op mop">∑</span></span></span><span style="top:-1.2500050000000003em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:0.10903em;">N</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord"><span class="mord displaystyle textstyle uncramped"><span class="mord displaystyle textstyle uncramped"><span class="mord mathbb">E</span></span></span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">[</span><span class="mord"><span class="mord mathit">ρ</span><span class="vlist"><span style="top:0.247em;margin-left:0em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord"><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.1132em;">τ</span></span></span><span style="top:0em;margin-left:0.05556em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>^</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="vlist"><span style="top:0.15em;margin-right:0.07142857142857144em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span><span style="top:-0.4129999999999999em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">κ</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.03785em;">δ</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.03785em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mclose">]</span><span class="style-wrap reset-textstyle textstyle uncramped" style="top:0em;"><span class="delimsizing size4">]</span></span></span></span></span></span></span></p>
<p>We set <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>κ</mi></mrow><annotation encoding="application/x-tex">\kappa</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">κ</span></span></span></span>, the threshold between the quadratic and linear regime
of the loss, to <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>0</mn><mi mathvariant="normal">.</mi><mn>0</mn><mn>0</mn><mn>2</mn></mrow><annotation encoding="application/x-tex">0.002</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">0</span><span class="mord mathrm">.</span><span class="mord mathrm">0</span><span class="mord mathrm">0</span><span class="mord mathrm">2</span></span></span></span> across all of our experiments.</p>
<p><strong>Quantile Function QT-Opt (Q2F-Opt)</strong></p>
<p>In Q2F-Opt, the neural network itself approximates the quantile function of the value distribution, and therefore it can predict the inverse CDF for any <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi><mi>a</mi><mi>u</mi></mrow><annotation encoding="application/x-tex">tau</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.61508em;"></span><span class="strut bottom" style="height:0.61508em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">t</span><span class="mord mathit">a</span><span class="mord mathit">u</span></span></span></span>. Since <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>τ</mi><mo separator="true">,</mo><msup><mi>τ</mi><mrow><mi mathvariant="normal">′</mi></mrow></msup><mo separator="true">,</mo><msup><mi>τ</mi><mrow><mi mathvariant="normal">′</mi><mi mathvariant="normal">′</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\tau, \tau&#x27;, \tau&#x27;&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.751892em;"></span><span class="strut bottom" style="height:0.946332em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.1132em;">τ</span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.1132em;">τ</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.1132em;">τ</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> are no longer fixed, we explicitly include them in the arguments of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="bold">q</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{q}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.44444em;"></span><span class="strut bottom" style="height:0.63888em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathbf">q</span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mover accent="true"><mrow><mrow><mi mathvariant="bold">q</mi></mrow></mrow><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{\mathbf{q}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.70788em;"></span><span class="strut bottom" style="height:0.90232em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord textstyle cramped"><span class="mord textstyle cramped"><span class="mord mathbf">q</span></span></span></span><span style="top:-0.013440000000000007em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>^</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>. Thus, the TD-errors <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>δ</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\delta_{ij}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.980548em;vertical-align:-0.286108em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.03785em;">δ</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.03785em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> take the form:</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>δ</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><msub><mover accent="true"><mrow><mrow><mi mathvariant="bold">q</mi></mrow></mrow><mo>^</mo></mover><mi>j</mi></msub><mo>(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo separator="true">,</mo><msup><mi>s</mi><mrow><mi mathvariant="normal">′</mi></mrow></msup><mo separator="true">,</mo><msup><mrow><mi>τ</mi></mrow><mrow><mi mathvariant="normal">′</mi></mrow></msup><mo separator="true">,</mo><msup><mi>τ</mi><mrow><mi mathvariant="normal">′</mi><mi mathvariant="normal">′</mi></mrow></msup><mo>)</mo><mo>−</mo><msub><mrow><mi mathvariant="bold">q</mi></mrow><mi>i</mi></msub><mo>(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo separator="true">,</mo><mrow><mi>τ</mi></mrow><mo>)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\delta_{ij} = \hat{\mathbf{q}}_j(s, a, s&#x27;, \mathbf{\tau}&#x27;, \tau&#x27;&#x27;) - \mathbf{q}_i(s, a, \mathbf{\tau}),
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.801892em;"></span><span class="strut bottom" style="height:1.1824400000000002em;vertical-align:-0.380548em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.03785em;">δ</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.03785em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">=</span><span class="mord"><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord displaystyle textstyle cramped"><span class="mord displaystyle textstyle cramped"><span class="mord mathbf">q</span></span></span></span><span style="top:-0.013440000000000007em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>^</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="vlist"><span style="top:0.24444000000000002em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit">s</span><span class="mpunct">,</span><span class="mord mathit">a</span><span class="mpunct">,</span><span class="mord"><span class="mord mathit">s</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class=""><span class="mord displaystyle textstyle uncramped"><span class="mord mathit" style="margin-right:0.1132em;">τ</span></span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.1132em;">τ</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mbin">−</span><span class=""><span class="mord displaystyle textstyle uncramped"><span class="mord mathbf">q</span></span><span class="vlist"><span style="top:0.24444em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit">s</span><span class="mpunct">,</span><span class="mord mathit">a</span><span class="mpunct">,</span><span class="mord displaystyle textstyle uncramped"><span class="mord mathit" style="margin-right:0.1132em;">τ</span></span><span class="mclose">)</span><span class="mpunct">,</span></span></span></span></span></p>
<p>where <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>τ</mi><mi>i</mi></msub><mo>∼</mo><mi>U</mi><mo>[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo>]</mo></mrow><annotation encoding="application/x-tex">\tau_i \sim U[0, 1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.1132em;">τ</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.1132em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">∼</span><span class="mord mathit" style="margin-right:0.10903em;">U</span><span class="mopen">[</span><span class="mord mathrm">0</span><span class="mpunct">,</span><span class="mord mathrm">1</span><span class="mclose">]</span></span></span></span>, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>τ</mi><mi>j</mi><mrow><mi mathvariant="normal">′</mi></mrow></msubsup><mo>∼</mo><mi>U</mi><mo>[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo>]</mo></mrow><annotation encoding="application/x-tex">\tau&#x27;_j \sim U[0, 1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.751892em;"></span><span class="strut bottom" style="height:1.146664em;vertical-align:-0.394772em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.1132em;">τ</span><span class="vlist"><span style="top:0.258664em;margin-left:-0.1132em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">∼</span><span class="mord mathit" style="margin-right:0.10903em;">U</span><span class="mopen">[</span><span class="mord mathrm">0</span><span class="mpunct">,</span><span class="mord mathrm">1</span><span class="mclose">]</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>τ</mi><mi>j</mi><mrow><mi mathvariant="normal">′</mi><mi mathvariant="normal">′</mi></mrow></msubsup><mo>∼</mo><mi>U</mi><mo>[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo>]</mo></mrow><annotation encoding="application/x-tex">\tau&#x27;&#x27;_j \sim U[0, 1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.751892em;"></span><span class="strut bottom" style="height:1.146664em;vertical-align:-0.394772em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.1132em;">τ</span><span class="vlist"><span style="top:0.258664em;margin-left:-0.1132em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">∼</span><span class="mord mathit" style="margin-right:0.10903em;">U</span><span class="mopen">[</span><span class="mord mathrm">0</span><span class="mpunct">,</span><span class="mord mathrm">1</span><span class="mclose">]</span></span></span></span> are sampled from independent uniform distributions. Using different input probability vectors also decreases the correlation between the networks. Note that now the length of the prediction and target vectors are determined by the lengths of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>τ</mi></mrow><annotation encoding="application/x-tex">\tau</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.1132em;">τ</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>τ</mi><mrow><mi mathvariant="normal">′</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\tau&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.751892em;"></span><span class="strut bottom" style="height:0.751892em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.1132em;">τ</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>. The model is optimized using the same loss function as Q2R-Opt.</p>
<p><strong>Risk-Sensitive Policies</strong></p>
<p>The additional information provided by a value distribution compared to the (scalar) expected return gives birth to a broader class of policies that go beyond optimizing for the expected value of the actions. Concretely, the expectation can be replaced with any risk metric, that is any function that maps the random return to a scalar quantifying the risk. In Q2-Opt, this role is played by the function <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ψ</mi></mrow><annotation encoding="application/x-tex">\psi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">ψ</span></span></span></span> that acts as a risk-metric. Thus the agent can handle the intrinsic uncertainty of the task in different ways depending on the specific form of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ψ</mi></mrow><annotation encoding="application/x-tex">\psi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">ψ</span></span></span></span>. It is important to specify that this uncertainty is generated by the environment dynamics (<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit">p</span></span></span></span>) and the (non-stationary) policy collecting the real robot rollouts and that it is not a parametric uncertainty.</p>
<p>We distinguish two methods to construct risk-sensitive policies for Q2R-Opt and Q2F-Opt, each specific to one of the methods. In Q2R-Opt, risk-averse and risk-seeking policies can be obtained by changing the function <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ψ</mi><mo>(</mo><mo>⋅</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">\psi(\cdot)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">ψ</span><span class="mopen">(</span><span class="mord">⋅</span><span class="mclose">)</span></span></span></span> when selecting actions. Rather than computing the mean of the target quantiles, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ψ</mi><mo>(</mo><mo>⋅</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">\psi(\cdot)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">ψ</span><span class="mopen">(</span><span class="mord">⋅</span><span class="mclose">)</span></span></span></span> can be defined as a weighted average over the quantiles <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ψ</mi><mo>(</mo><mrow><mi mathvariant="bold">q</mi></mrow><mo>(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo>)</mo><mo>)</mo><mo>=</mo><mfrac><mrow><mn>1</mn></mrow><mrow><mi>N</mi></mrow></mfrac><mo>∑</mo><msub><mi>w</mi><mi>i</mi></msub><msub><mrow><mi mathvariant="bold">q</mi></mrow><mi>i</mi></msub><mo>(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">\psi(\mathbf{q}(s,a)) = \frac{1}{N} \sum w_i \mathbf{q}_i(s,a)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.845108em;"></span><span class="strut bottom" style="height:1.190108em;vertical-align:-0.345em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">ψ</span><span class="mopen">(</span><span class="mord textstyle uncramped"><span class="mord mathbf">q</span></span><span class="mopen">(</span><span class="mord mathit">s</span><span class="mpunct">,</span><span class="mord mathit">a</span><span class="mclose">)</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord reset-textstyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:0.345em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.10903em;">N</span></span></span></span><span style="top:-0.22999999999999998em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-0.394em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="op-symbol small-op mop" style="top:-0.0000050000000000050004em;">∑</span><span class="mord"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.02691em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class=""><span class="mord textstyle uncramped"><span class="mord mathbf">q</span></span><span class="vlist"><span style="top:0.24444em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit">s</span><span class="mpunct">,</span><span class="mord mathit">a</span><span class="mclose">)</span></span></span></span>. This sum produces a policy that is in between a worst-case and best-case action selector and, for most purposes, it would be preferable in practice over the two extremes. For instance, a robot that would consider only the worst-case scenario would most likely terminate immediately since this strategy, even though it is not useful, does not incur any penalty. Behaviours like this have been encountered in our evaluation of very conservative policies.</p>
<p>In contrast, Q2F-Opt provides a more elegant way of learning risk-sensitive control policies by using risk-distortion metrics <dt-cite key="wang_1996"></dt-cite>. Recently, Majmidar and Pavone  <dt-cite key="Majumdar2017HowSA"></dt-cite> have argued for the use of risk distortion metrics in robotics. They proposed a set of six axioms that any risk measure should meet to produce reasonable behaviour and showed that risk distortion metrics satisfy all of them. However, to the best of our knowledge, they have not been tried
on real robotic applications.</p>
<p>The key idea is to use a policy:</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo>(</mo><mi>s</mi><mo separator="true">,</mo><mi>β</mi><mo>(</mo><mi>τ</mi><mo>)</mo><mo>)</mo><mo>=</mo><msub><mrow><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">g</mi><mspace width="0.16667em"></mspace><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">x</mi></mrow><mi>a</mi></msub><mi>ψ</mi><mo>(</mo><mrow><mi mathvariant="bold">q</mi></mrow><mo>(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo separator="true">,</mo><mi>β</mi><mo>(</mo><mrow><mi>τ</mi></mrow><mo>)</mo><mo>)</mo><mo>)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\pi_\theta(s, \beta(\tau)) = \mathrm{arg\,max}_a \psi(\mathbf{q}(s,a, \beta(\mathbf{\tau}))),
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">π</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.03588em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit">s</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.05278em;">β</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.1132em;">τ</span><span class="mclose">)</span><span class="mclose">)</span><span class="mrel">=</span><span class=""><span class="mord displaystyle textstyle uncramped"><span class="mord mathrm">a</span><span class="mord mathrm">r</span><span class="mord mathrm" style="margin-right:0.01389em;">g</span><span class="mord mspace thinspace"></span><span class="mord mathrm">m</span><span class="mord mathrm">a</span><span class="mord mathrm">x</span></span><span class="vlist"><span style="top:0.24444em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">a</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathit" style="margin-right:0.03588em;">ψ</span><span class="mopen">(</span><span class="mord displaystyle textstyle uncramped"><span class="mord mathbf">q</span></span><span class="mopen">(</span><span class="mord mathit">s</span><span class="mpunct">,</span><span class="mord mathit">a</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.05278em;">β</span><span class="mopen">(</span><span class="mord displaystyle textstyle uncramped"><span class="mord mathit" style="margin-right:0.1132em;">τ</span></span><span class="mclose">)</span><span class="mclose">)</span><span class="mclose">)</span><span class="mpunct">,</span></span></span></span></span></p>
<p>where <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>β</mi><mo>:</mo><mo>[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo>]</mo><mo>→</mo><mo>[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo>]</mo></mrow><annotation encoding="application/x-tex">\beta: [0, 1] \to [0, 1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.05278em;">β</span><span class="mrel">:</span><span class="mopen">[</span><span class="mord mathrm">0</span><span class="mpunct">,</span><span class="mord mathrm">1</span><span class="mclose">]</span><span class="mrel">→</span><span class="mopen">[</span><span class="mord mathrm">0</span><span class="mpunct">,</span><span class="mord mathrm">1</span><span class="mclose">]</span></span></span></span> is an element-wise function that
distorts the uniform distribution that <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi>τ</mi></mrow></mrow><annotation encoding="application/x-tex">{\tau}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathit" style="margin-right:0.1132em;">τ</span></span></span></span></span> is effectively
sampled from, and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ψ</mi><mo>(</mo><mo>⋅</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">\psi(\cdot)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">ψ</span><span class="mopen">(</span><span class="mord">⋅</span><span class="mclose">)</span></span></span></span> computes the mean of the vector as
usual. Functions <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.05278em;">β</span></span></span></span> that are concave induce risk-averse policies,
while convex function induce risk-seeking policies.</p>
<div class="figure">
<img src="assets/fig/risk_metrics_tab.png" style="margin: 0; width: 60%;"/>
<figcaption>
<center>
Table 1. The considered risk distortion metrics.
</center>
</figcaption>
</div>
<p>In our experiments, we consider the same risk distortion metrics used by
Dabney et al. <dt-cite key="dabney2018implicit"></dt-cite>: the cumulative probability
weighting (<strong>CPW</strong>) <dt-cite key="Gonzlez1999OnTS"></dt-cite>, the standard normal CDF-based
metric proposed by <strong>Wang</strong> <dt-cite key="Wang00aclass"></dt-cite>, the conditional value at
risk (<strong>CVaR</strong>) <dt-cite key="Rockafellar00optimizationof"></dt-cite>,
<strong>Norm</strong> <dt-cite key="dabney2018implicit"></dt-cite>, and a power law formula
(<strong>Pow</strong>) <dt-cite key="dabney2018implicit"></dt-cite>. Concretely, we use these metrics with a
parameter choice similar to that of Dabney et al. <dt-cite key="dabney2018implicit"></dt-cite>. CPW(0.71) is known to be a good model
human behaviour <dt-cite key="Wu1996"></dt-cite>, Wang<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mo>−</mo><mi mathvariant="normal">.</mi><mn>7</mn><mn>5</mn><mo>)</mo></mrow><annotation encoding="application/x-tex">(-.75)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mopen">(</span><span class="mord">−</span><span class="mord mathrm">.</span><span class="mord mathrm">7</span><span class="mord mathrm">5</span><span class="mclose">)</span></span></span></span>, Pow<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mo>−</mo><mn>2</mn><mo>)</mo></mrow><annotation encoding="application/x-tex">(-2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mopen">(</span><span class="mord">−</span><span class="mord mathrm">2</span><span class="mclose">)</span></span></span></span>, CVaR<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mn>0</mn><mi mathvariant="normal">.</mi><mn>2</mn><mn>5</mn><mo>)</mo></mrow><annotation encoding="application/x-tex">(0.25)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mopen">(</span><span class="mord mathrm">0</span><span class="mord mathrm">.</span><span class="mord mathrm">2</span><span class="mord mathrm">5</span><span class="mclose">)</span></span></span></span> and CVaR<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mn>0</mn><mi mathvariant="normal">.</mi><mn>4</mn><mo>)</mo></mrow><annotation encoding="application/x-tex">(0.4)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mopen">(</span><span class="mord mathrm">0</span><span class="mord mathrm">.</span><span class="mord mathrm">4</span><span class="mclose">)</span></span></span></span> are risk-averse. Norm(<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>3</mn></mrow><annotation encoding="application/x-tex">3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">3</span></span></span></span>) decreases the weight of the distribution's tails by averaging 3 uniformly sampled <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>τ</mi></mrow><annotation encoding="application/x-tex">\tau</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.1132em;">τ</span></span></span></span>. Ultimately, Wang<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mi mathvariant="normal">.</mi><mn>7</mn><mn>5</mn><mo>)</mo></mrow><annotation encoding="application/x-tex">(.75)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mopen">(</span><span class="mord mathrm">.</span><span class="mord mathrm">7</span><span class="mord mathrm">5</span><span class="mclose">)</span></span></span></span> produces
risk-seeking behaviour. We include all these metrics in Table 1.</p>
<p>Due to the relationships between Q2F-Opt and the literature of risk
distortion measures, we focus our risk-sensitivity experiments on the
metrics mentioned above and leave the possibility of trying different
functions <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ψ</mi><mo>(</mo><mo>⋅</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">\psi(\cdot)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">ψ</span><span class="mopen">(</span><span class="mord">⋅</span><span class="mclose">)</span></span></span></span> in Q2R-Opt for future work.</p>
<p><strong>Model Architecture</strong></p>
<div class="figure">
<img src="assets/fig/icra_net.png" style="margin: 0; width: 100%;"/>
<figcaption>
Figure 2. The neural network architectures for QT-Opt (yellow), Q2R-Opt (green) and Q2F-Opt (red). The common components of all the three models are represented by black arrows. The top-left image shows a view from the robot camera inside our simulation environment.
</figcaption>
</div>
<p>To maintain our comparisons with QT-Opt, we use very similar
architectures for Q2R-Opt and Q2F-Opt. For Q2R-Opt, we modify the output
layer of the standard QT-Opt architecture to be a vector of size
<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>N</mi><mo>=</mo><mn>1</mn><mn>0</mn><mn>0</mn></mrow><annotation encoding="application/x-tex">N = 100</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10903em;">N</span><span class="mrel">=</span><span class="mord mathrm">1</span><span class="mord mathrm">0</span><span class="mord mathrm">0</span></span></span></span>, rather than a scalar. For Q2F-Opt, we take a similar approach
to Dabney et al. <dt-cite key="dabney2018implicit"></dt-cite>, and embed every <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mrow><mi>τ</mi></mrow><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">{\tau}_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord textstyle uncramped"><span class="mord mathit" style="margin-right:0.1132em;">τ</span></span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> with
<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi><mo>∈</mo><mo>{</mo><mn>1</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mi>N</mi><mo>=</mo><mn>3</mn><mn>2</mn><mo>}</mo></mrow><annotation encoding="application/x-tex">k \in \{1, \ldots, N = 32\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span><span class="mrel">∈</span><span class="mopen">{</span><span class="mord mathrm">1</span><span class="mpunct">,</span><span class="minner">…</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.10903em;">N</span><span class="mrel">=</span><span class="mord mathrm">3</span><span class="mord mathrm">2</span><span class="mclose">}</span></span></span></span> using a series of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi><mo>=</mo><mn>6</mn><mn>4</mn></mrow><annotation encoding="application/x-tex">n = 64</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">n</span><span class="mrel">=</span><span class="mord mathrm">6</span><span class="mord mathrm">4</span></span></span></span> cosine basis
functions:</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>ϕ</mi><mi>j</mi></msub><mo>(</mo><msub><mi>τ</mi><mi>k</mi></msub><mo>)</mo><mo>:</mo><mo>=</mo><mtext><mi mathvariant="normal">R</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">L</mi><mi mathvariant="normal">U</mi></mtext><mrow><mo fence="true">(</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></msubsup><mi>cos</mi><mo>(</mo><mi>π</mi><mi>i</mi><msub><mi>τ</mi><mi>k</mi></msub><mo>)</mo><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>+</mo><msub><mi>b</mi><mi>j</mi></msub><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\phi_j(\tau_k) := \text{ReLU}\left(\sum_{i=0}^{n-1} \cos(\pi i \tau_k)w_{ij} + b_j\right)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.8011130000000004em;"></span><span class="strut bottom" style="height:3.0787820000000004em;vertical-align:-1.277669em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord"><span class="mord mathit">ϕ</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.1132em;">τ</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.1132em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mrel">:</span><span class="mrel">=</span><span class="text mord displaystyle textstyle uncramped"><span class="mord mathrm">R</span><span class="mord mathrm">e</span><span class="mord mathrm">L</span><span class="mord mathrm">U</span></span><span class="minner displaystyle textstyle uncramped"><span class="style-wrap reset-textstyle textstyle uncramped" style="top:0em;"><span class="delimsizing size4">(</span></span><span class="mop op-limits"><span class="vlist"><span style="top:1.1776689999999999em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mrel">=</span><span class="mord mathrm">0</span></span></span></span><span style="top:-0.000005000000000143778em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span><span class="op-symbol large-op mop">∑</span></span></span><span style="top:-1.2500050000000003em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">n</span><span class="mbin">−</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mop">cos</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.03588em;">π</span><span class="mord mathit">i</span><span class="mord"><span class="mord mathit" style="margin-right:0.1132em;">τ</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.1132em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mord"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.02691em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mbin">+</span><span class="mord"><span class="mord mathit">b</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="style-wrap reset-textstyle textstyle uncramped" style="top:0em;"><span class="delimsizing size4">)</span></span></span></span></span></span></span></p>
<p>We then perform the Hadamard product between this embedding and the convolutional features.
Another difference in Q2F-Opt is that we replace batch normalization
<dt-cite key="Ioffe:2015:BNA:3045118.3045167"></dt-cite>
in the final fully-connected layers with layer normalization
<dt-cite key="Ba2016LayerN"></dt-cite>.
We notice that this better keeps the sampled values in the range allowed by our
MDP formulation. The three architectures are all included in a single diagram in Figure 2.</p>
<h2>Results</h2>
<p>In this section, we present our results on simulated and real
environments. In simulation, we perform both online and offline
experiments, while for the real world, the training is exclusively
offline. We begin by describing our evaluation method.</p>
<p><strong>Experimental Setup</strong></p>
<p>We consider the problem of vision-based robotic grasping for our
evaluations. In our grasping setup, the robot arm is placed at a fixed
distance from a bin containing a variety of objects and tasked with
grasping any object. The MDP specifying our robotic manipulation task
provides a simple binary reward to the agent at the end of the episode:
<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">0</span></span></span></span> for a failed grasp, and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">1</span></span></span></span> for a successful grasp. To encourage the
robot to grasp objects as fast as possible, we use a time step penalty
of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>−</mo><mn>0</mn><mi mathvariant="normal">.</mi><mn>0</mn><mn>1</mn></mrow><annotation encoding="application/x-tex">-0.01</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">−</span><span class="mord mathrm">0</span><span class="mord mathrm">.</span><span class="mord mathrm">0</span><span class="mord mathrm">1</span></span></span></span> and a discount factor <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>γ</mi><mo>=</mo><mn>0</mn><mi mathvariant="normal">.</mi><mn>9</mn></mrow><annotation encoding="application/x-tex">\gamma=0.9</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.8388800000000001em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.05556em;">γ</span><span class="mrel">=</span><span class="mord mathrm">0</span><span class="mord mathrm">.</span><span class="mord mathrm">9</span></span></span></span>. The state is represented
by a <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>4</mn><mn>7</mn><mn>2</mn><mo>×</mo><mn>4</mn><mn>7</mn><mn>2</mn></mrow><annotation encoding="application/x-tex">472\times472</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">4</span><span class="mord mathrm">7</span><span class="mord mathrm">2</span><span class="mbin">×</span><span class="mord mathrm">4</span><span class="mord mathrm">7</span><span class="mord mathrm">2</span></span></span></span> RGB image; the actions are a mixture of continuous
4-DOF tool displacements in <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">x</span></span></span></span>, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">y</span></span></span></span>, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.04398em;">z</span></span></span></span> with azimuthal rotation
<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϕ</mi></mrow><annotation encoding="application/x-tex">\phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit">ϕ</span></span></span></span>, and discrete actions to open and close the gripper, as well as
to terminate the episode.</p>
<p>In simulation, we grasp from a bin containing 8 to 12 randomly generated
procedural objects (Figure 2). For the first <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>5</mn><mo separator="true">,</mo><mn>0</mn><mn>0</mn><mn>0</mn></mrow><annotation encoding="application/x-tex">5,000</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.8388800000000001em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">5</span><span class="mpunct">,</span><span class="mord mathrm">0</span><span class="mord mathrm">0</span><span class="mord mathrm">0</span></span></span></span> global training
steps, we use a procedural exploration policy. The scripted policy is
lowering the end effector at a random position at the level of the bin
and attempts to grasp. After <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>5</mn><mo separator="true">,</mo><mn>0</mn><mn>0</mn><mn>0</mn></mrow><annotation encoding="application/x-tex">5,000</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.8388800000000001em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">5</span><span class="mpunct">,</span><span class="mord mathrm">0</span><span class="mord mathrm">0</span><span class="mord mathrm">0</span></span></span></span> steps, we switch to an
<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">ϵ</span></span></span></span>-greedy policy with <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi><mo>=</mo><mn>0</mn><mi mathvariant="normal">.</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">\epsilon = 0.2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">ϵ</span><span class="mrel">=</span><span class="mord mathrm">0</span><span class="mord mathrm">.</span><span class="mord mathrm">2</span></span></span></span>. We train the network
from scratch (no pretraining) using Adam <dt-cite key="kingma2014adam"></dt-cite> with a
learning rate <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>4</mn></mrow></msup></mrow><annotation encoding="application/x-tex">10^{-4}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8141079999999999em;"></span><span class="strut bottom" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">1</span><span class="mord"><span class="mord mathrm">0</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord">−</span><span class="mord mathrm">4</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> and batch size <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>4</mn><mn>0</mn><mn>9</mn><mn>6</mn></mrow><annotation encoding="application/x-tex">4096</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">4</span><span class="mord mathrm">0</span><span class="mord mathrm">9</span><span class="mord mathrm">6</span></span></span></span> (<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>2</mn><mn>5</mn><mn>6</mn></mrow><annotation encoding="application/x-tex">256</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">2</span><span class="mord mathrm">5</span><span class="mord mathrm">6</span></span></span></span> per chip on a
<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>4</mn><mo>×</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">4\times4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">4</span><span class="mbin">×</span><span class="mord mathrm">4</span></span></span></span> TPU). Additionally, we use two iterations of CEM with 64
samples for each.</p>
<p>In the real world, we train our model offline from a <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>7</mn><mn>2</mn></mrow><annotation encoding="application/x-tex">72</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">7</span><span class="mord mathrm">2</span></span></span></span> TiB dataset of
real-world experiences collected over five months, containing <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>5</mn><mn>5</mn><mn>9</mn><mo separator="true">,</mo><mn>6</mn><mn>4</mn><mn>2</mn></mrow><annotation encoding="application/x-tex">559,642</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.8388800000000001em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">5</span><span class="mord mathrm">5</span><span class="mord mathrm">9</span><span class="mpunct">,</span><span class="mord mathrm">6</span><span class="mord mathrm">4</span><span class="mord mathrm">2</span></span></span></span>
episodes of up to <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>2</mn><mn>0</mn></mrow><annotation encoding="application/x-tex">20</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">2</span><span class="mord mathrm">0</span></span></span></span> time steps each. Out of these, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>3</mn><mn>9</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">39\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:0.80556em;vertical-align:-0.05556em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">3</span><span class="mord mathrm">9</span><span class="mord mathrm">%</span></span></span></span> were
generated by noise-free trained QT-Opt policies, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>2</mn><mn>2</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">22\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:0.80556em;vertical-align:-0.05556em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">2</span><span class="mord mathrm">2</span><span class="mord mathrm">%</span></span></span></span> by an
<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi>ϵ</mi></mrow></mrow><annotation encoding="application/x-tex">{\epsilon}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathit">ϵ</span></span></span></span></span>-greedy strategy using trained QT-Opt policies and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>3</mn><mn>9</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">39\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:0.80556em;vertical-align:-0.05556em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">3</span><span class="mord mathrm">9</span><span class="mord mathrm">%</span></span></span></span> by
an <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi>ϵ</mi></mrow></mrow><annotation encoding="application/x-tex">{\epsilon}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathit">ϵ</span></span></span></span></span>-greedy strategy based on a scripted policy. For
evaluation, we attempt 6 consecutive grasps from a bin containing 6
objects without replacement, repeated across 5 rounds. Figures 2 and 3
include our workspace setup. We perform
this experiment in parallel on 7 robots, resulting in a total of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>2</mn><mn>1</mn><mn>0</mn></mrow><annotation encoding="application/x-tex">210</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">2</span><span class="mord mathrm">1</span><span class="mord mathrm">0</span></span></span></span>
grasp attempts. All the robots use a similar object setup consisting of
two plastic bottles, one metal can, one paper bowl, one paper cup, and
one paper cup sleeve. In the results section, we report the success rate
over the <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>2</mn><mn>1</mn><mn>0</mn></mrow><annotation encoding="application/x-tex">210</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">2</span><span class="mord mathrm">1</span><span class="mord mathrm">0</span></span></span></span> attempts.</p>
<p>Our evaluation methodology is different from that of Kalashnikov et al.
<dt-cite key="kalashnikov2018scalable"></dt-cite>. The original QT-Opt paper reports an average
success rate of 76% for grasping 28 objects over 30 attempts without
replacement, trained on a mixture of off-policy and on-policy data.
While not directly comparable, we reproduce QT-Opt on a completely
different robot with different objects and report an average success
rate of 70% for grasping 6 objects over 6 attempts without replacement,
trained on off-policy data only.</p>
<p><strong>Simulation Experiments</strong></p>
<div class="figure">
<img src="assets/fig/sim_success.png" style="margin: 0; width: 60%;"/>
<figcaption>
Figure 3. Sim success rate as a function of the global step. The distributional
methods achieve higher grasp success rates in a lower number of global
steps.
</figcaption>
</div>
<p>We begin by evaluating Q2-Opt against QT-Opt in simulation.
Figure. Figure 3 shows the mean success rate as a function
of the global training step together with the standard deviation across
five runs for QT-Opt, Q2R-Opt and Q2F-Opt. Because Q2-Opt and QT-Opt are
distributed systems, the global training step does not directly match
the number of environment episodes used by the models during training.
Therefore, to understand the sample efficiency of the algorithm, we also
include in Figure 4 the success rate as a function of
the total number of environment episodes added to the buffer.</p>
<div class="figure">
<img src="assets/fig/sim_sample.png" style="margin: 0; width: 60%;"/>
<figcaption>
<center>
Figure 4. Sim success rate as a function of the number of generated environment
episodes. The distributional methods are significantly more sample
efficient than QT-Opt.
</center>
</figcaption>
</div>
<p>The distributional methods achieve higher success rates while also being
more sample efficient than QT-Opt. While Q2F-Opt performs best, Q2R-Opt
exhibits an intermediary performance and, despite being less sample
efficient than Q2F-Opt, it still learns significantly faster than our
baseline.</p>
<div class="figure">
<img src="assets/fig/sim_success_tab.png" style="margin: 0; width: 60%;"/>
<figcaption>
<center>
Table 2. Final sim success rate statistics. Distributional 
risk-averse policies have the best performance.
</center>
</figcaption>
</div>
<p>We extend these simulation experiments with a series of risk distortion
measures equipped with different parameters. Figure 5 shows the success rate for various
measures used in Q2F-Opt. We notice that risk-averse policies
(Wang<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mo>−</mo><mn>0</mn><mi mathvariant="normal">.</mi><mn>7</mn><mn>5</mn><mo>)</mo></mrow><annotation encoding="application/x-tex">(-0.75)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mopen">(</span><span class="mord">−</span><span class="mord mathrm">0</span><span class="mord mathrm">.</span><span class="mord mathrm">7</span><span class="mord mathrm">5</span><span class="mclose">)</span></span></span></span>, Pow<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mo>−</mo><mn>2</mn><mo>)</mo></mrow><annotation encoding="application/x-tex">(-2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mopen">(</span><span class="mord">−</span><span class="mord mathrm">2</span><span class="mclose">)</span></span></span></span>, CVaR) are generally more stable in the late
stages of training and achieve a higher success rate. Pow<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mo>−</mo><mn>2</mn><mo>)</mo></mrow><annotation encoding="application/x-tex">(-2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mopen">(</span><span class="mord">−</span><span class="mord mathrm">2</span><span class="mclose">)</span></span></span></span>
remarkably achieves 95% grasp success rate. However, being too
conservative can also be problematic. Particularly, the CVaR<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mn>0</mn><mi mathvariant="normal">.</mi><mn>2</mn><mn>5</mn><mo>)</mo></mrow><annotation encoding="application/x-tex">(0.25)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mopen">(</span><span class="mord mathrm">0</span><span class="mord mathrm">.</span><span class="mord mathrm">2</span><span class="mord mathrm">5</span><span class="mclose">)</span></span></span></span>
policy becomes more vulnerable to the locally optimal behaviour of
stopping immediately (which does not induce any reward penalty). This
makes its performance fluctuate throughout training, even though it
ultimately obtains a good final success rate.
Table 2 gives the complete final success rate statistics.</p>
<div class="figure">
<img src="assets/fig/sim_risk_success.png" style="margin: 0; width: 60%;"/>
<figcaption>
<center>
Figure 5. Average success rate over five runs for different risk-sensitive
policies in sim. Most risk averse policies perform better, but extremely
conservative ones like CVaR(.25) can become unstable. The risk-seeking
policy Wang(.75) performs worse than the others. 
</center>
</figcaption>
</div>
<p><strong>Real-World Experiments</strong></p>
<div class="figure">
<img src="assets/fig/real_success_tab.png" style="margin: 0; width: 60%;"/>
<figcaption>
<center>
Table 3. Real world grasp success rate out of 210 total grasps. 
Our methods significantly outperform QT-Opt, 
while risk-averse polices are better by a significant margin.
</center>
</figcaption> 
</div>
<p>The chaotic physical interactions specific to real-world environments
and the diversity of policies used to gather the experiences make the
real environment an ideal setting for distributional RL. Furthermore,
this experiment is also of practical importance for robotics since any
increase in grasp success rate from offline data reduces the amount of
costly online training that has to be performed to obtain a good policy.</p>
<div class="figure">
<video class="b-lazy" data-src="assets/mp4/qr_pull_center.mp4" type="video/mp4" autoplay muted playsinline loop style="display: block; width: 60%;"></video>
<figcaption>
Q2R-Opt learns to pull the can from the corner of the bin to a more central position from where it is easier to grasp.
</figcaption>
</div>
<p>We report in Table 3 the grasp success rate statistics for all
the considered models. We find that the best risk-averse version of
Q2-Opt achieves an impressive 17.6% higher success rate than QT-Opt.
While the real evaluation closely matches the model hierarchy observed
in sim, the success rate differences between the models are much more
significant.</p>
<div class="figure">
<video class="b-lazy" data-src="assets/mp4/wang_n_paper_certain.mp4" type="video/mp4" autoplay muted playsinline loop style="display: block; width: 60%;"></video>
<figcaption>
Risk-averse Q2F-Opt Wang(-0.75) performing active perception. The agent needs to move the end effector arround to realise that it grasped the object.
</figcaption>
</div>
<p>Besides the improvement in performance, we notice that the distortion
measures of Q2F-Opt have a significant qualitative impact, even though
the training is performed from the same data. Risk-averse policies tend
to readjust the gripper in positions that are more favourable or move
objects around to make grasping easier. CVaR(0.4), the most conservative
metric we tested in the real world, presented a particularly interesting
behaviour of intentionally dropping poorly grasped objects to attempt a
better re-grasp.</p>
<div class="figure">
<video class="b-lazy" data-src="assets/mp4/cvar_can_bin_edge.mp4" type="video/mp4" autoplay muted playsinline loop style="display: block; width: 60%;"></video>
<figcaption>
CVaR (risk-averse) policy learns to use the edges of the bin to get a better grip of the object and reduce the uncertainty.
</figcaption>
</div>
<p>The CVaR policy mainly used this technique when
attempting to grasp objects from the corners of the bin to move them in
a central position.  However, a downside of risk-averse policies that we noticed is that, for
the difficult-to-grasp paper cup sleeves, the agent often kept searching
for an ideal position without actually attempting to grasp. We believe
this is an interesting example of the trade-offs between being
conservative and risk-seeking.</p>
<div class="figure">
<video class="b-lazy" data-src="assets/mp4/wang_n_uncertain.mp4" type="video/mp4" autoplay muted playsinline loop style="display: block; width: 60%;"></video>
<figcaption>
Risk-averse policies can sometimes be too conservative and avoid attempting a grasp even in promising positions.
</figcaption>
</div>
<p>The only tested risk-seeking policy,
using Wang(0.75), made many high-force contacts with the bin and
objects, which often resulted in broken gripper fingers and objects
being thrown out of the bin.</p>
<div class="figure">
<video class="b-lazy" data-src="assets/mp4/wang_p_bottle.mp4" type="video/mp4" autoplay muted playsinline loop style="display: block; width: 60%;"></video>
<figcaption>
Risk-seeking Wang(0.75) is more sloppy. After multiple attempts it succeeds to grasp the bottle but it does not get a good grip of it. 
</figcaption>
</div>
<p>These qualitative differences in behaviour that value distributions
cause can provide a way to achieve safe robot control. An interesting
metric we considered to quantify these behaviours is the number of
broken gripper fingers throughout the entire evaluation process
presented above. Occasionally, the gripper fingers break in high-force
contacts with the objects or the bin. Figure 6 plots these numbers for each policy.
Even though we do not have a statistically significant number of
samples, we believe this figure is a good indicator that risk-averse
policies implicitly achieve safer control.</p>
<div class="figure">
<img src="assets/fig/broken_fingers.png" style="margin: 0; width: 60%;"/>
<figcaption>
<center>
Figure 6. The number of detached gripper fingers during evaluation. Among the risk-averse policies (first three), only one gripper broke. The agents controlled by risk-neutral policies (Q2R-Opt, Q2F-Opt, QT-Opt) lost eight gripper fingers in total, with half of those belonging to QT-Opt. The last policy, which is risk-seeking lost four, similar to QT-Opt. The other policies (Norm, CPW) behaved similarly to risk-neutral policies.
</center>
</figcaption>
</div>
<p><strong>Batch RL and Exploitation</strong></p>
<p>Recently, Agarwal et al. <dt-cite key="Agarwal2019StrivingFS"></dt-cite> have argued that most
of the advantages of distributional algorithms come from better
exploitation. Their results demonstrated that QR-DQN could achieve in
offline training a performance superior to online C51. Since environment
interactions are particularly costly in robotics, we aim to reproduce
these results in a robotic setting. Therefore, we perform an equivalent
experiment in simulation and train the considered models on all the
transitions collected during training by a QT-Opt agent with a final
success rate of 90% (Figure 7a).</p>
<div class="figure">
<img src="assets/fig/sim_offp_train.png" style="margin: 0; width: 60%;"/>
<figcaption>
<center>
Figure 7a. Sim off-policy success rate for data collected during a full training run. 
None of the methods can achieve the final performance of the policy trained online (90%).
</center>
</figcaption>
</div>
<p>We note that despite the minor success rate improvements brought by
Q2R-Opt and Q2F-Opt, the two models are not even capable of achieving
the final success rate of the policy trained from the same data. We
hypothesize this is due to the out-of-distribution action
problem <dt-cite key="Kumar2019StabilizingOQ"></dt-cite>, which becomes more prevalent in
continuous action spaces.</p>
<div class="figure">
<img src="assets/fig/sim_offp_scripted.png" style="margin: 0; width: 60%;"/>
<figcaption>
<center>
Figure 7b. Sim success rate from an offline dataset produced by a 
scripted exploration policy. The models achieve a higher 
success rate than on the replay buffer dataset.
</center>
</figcaption>
</div>
<p>We investigated this further on two other datasets: one collected by a
scripted stochastic exploration policy with <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>4</mn><mn>6</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">46\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:0.80556em;vertical-align:-0.05556em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">4</span><span class="mord mathrm">6</span><span class="mord mathrm">%</span></span></span></span> success rate and
another produced by an almost optimal policy with <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>8</mn><mn>9</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">89\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:0.80556em;vertical-align:-0.05556em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">8</span><span class="mord mathrm">9</span><span class="mord mathrm">%</span></span></span></span> grasp success
rate. Figures 7b and 7c plot the results for these two datasets.
Surprisingly, the models achieve a higher success rate on the scripted
exploration dataset than on the dataset collected during training. On
the dataset generated by the almost optimal policy, none of the methods
manages to obtain a reasonable success rate. These results, taken
together with our real-world experiments, suggest that offline datasets
must contain a diverse set of experiences to learn effectively in a
batch RL setting.</p>
<div class="figure">
<img src="assets/fig/sim_offp_optimal.png" style="margin: 0; width: 60%;"/>
<figcaption>
<center>
Figure 7c. Sim success rate from an offline dataset produced by a policy 
close to optimality. None of the models is able to learn from a 
dataset of successful grasps.
</center>
</figcaption>
</div>
<h2>Conclusion</h2>
<p>In this work, we have examined the impact that value distributions have
on practical robotic tasks. Our proposed methods, collectively called
Q2-Opt, achieved state-of-the-art success rates on simulated and real
vision-based robotic grasping tasks, while also being significantly more
sample efficient than the non-distributional equivalent, QT-Opt.
Additionally, we have shown how safe reinforcement learning control can
be achieved through risk-sensitive policies and reported the rich set of
behaviours these policies produce in practice despite being trained from
the same data. As a final contribution, we evaluated the proposed
distributional methods in a batch RL setting similar to that of Agarwal
et al. <dt-cite key="Agarwal2019StrivingFS"></dt-cite> and showed that,
unfortunately, their findings do not translate to the continuous grasping
environment presented in this work.</p>
<h2>Acknowledgments</h2>
<p>We would like to give special thanks to Ivonne Fajardo and Noah Brown
for overseeing the robot operations. We would also like to extend our
gratitude to Julian Ibarz for helpful comments.</p>
</dt-article>
<dt-appendix>
<p>This article was prepared using the <a href="https://distill.pub">Distill</a> <a href="https://github.com/distillpub/template">template</a>.</p>
<h3 id="citation">Citation</h3>
<p>For attribution in academic contexts, please cite this work as</p>
<pre class="citation short">Bodnar et al., "Quantile QT-Opt for Risk-Aware Vision-Based Robotic Grasping", 2019.</pre>
<p>BibTeX citation</p>
<pre class="citation long">@article{bodnar2019quantile,
  title={Quantile QT-Opt for Risk-Aware Vision-Based Robotic Grasping},
  author={Cristian Bodnar and Adrian Li and Karol Hausman and Peter Pastor and Mrinal Kalakrishnan},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.02787}
}</pre>
</dt-appendix>
</dt-appendix>
</body>
<script type="text/bibliography">

% distributional %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{bellemare2017distributional,
  title={A Distributional Perspective on Reinforcement Learning},
  author={Bellemare, Marc G and Dabney, Will and Munos, R{\'e}mi},
  booktitle={International Conference on Machine Learning},
  pages={449--458},
  year={2017}
}

@inproceedings{dabney2018distributional,
  title={Distributional reinforcement learning with quantile regression},
  author={Dabney, Will and Rowland, Mark and Bellemare, Marc G and Munos, R{\'e}mi},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

@inproceedings{dabney2018implicit,
  title={Implicit Quantile Networks for Distributional Reinforcement Learning},
  author={Dabney, Will and Ostrovski, Georg and Silver, David and Munos, Remi},
  booktitle={International Conference on Machine Learning},
  pages={1104--1113},
  year={2018}
}

@article{Henderson2017DeepRL,
  title={Deep Reinforcement Learning That Matters},
  author={Peter Henderson and Riashat Islam and Philip Bachman and Joelle Pineau and Doina Precup and David Meger},
  journal={ArXiv},
  year={2017},
  volume={abs/1709.06560}
}

@inproceedings{kalashnikov2018scalable,
  title={Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation},
  author={Kalashnikov, Dmitry and Irpan, Alex and Pastor, Peter and Ibarz, Julian and Herzog, Alexander and Jang, Eric and Quillen, Deirdre and Holly, Ethan and Kalakrishnan, Mrinal and Vanhoucke, Vincent and others},
  booktitle={Conference on Robot Learning},
  pages={651--673},
  year={2018}
}

@article{Agarwal2019StrivingFS,
  title={Striving for Simplicity in Off-policy Deep Reinforcement Learning},
  author={Rishabh Agarwal and Dale Schuurmans and Mohammad Norouzi},
  journal={ArXiv},
  year={2019},
  volume={abs/1907.04543}
}

@article{kober2013reinforcement,
  title={Reinforcement learning in robotics: A survey},
  author={Kober, Jens and Bagnell, J Andrew and Peters, Jan},
  journal={The International Journal of Robotics Research},
  volume={32},
  number={11},
  pages={1238--1274},
  year={2013},
  publisher={SAGE Publications Sage UK: London, England}
}

@inproceedings{quillen2018deep,
  title={Deep reinforcement learning for vision-based robotic grasping: A simulated comparative evaluation of off-policy methods},
  author={Quillen, Deirdre and Jang, Eric and Nachum, Ofir and Finn, Chelsea and Ibarz, Julian and Levine, Sergey},
  booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={6284--6291},
  year={2018},
  organization={IEEE}
}

@article{levine2018learning,
  title={Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection},
  author={Levine, Sergey and Pastor, Peter and Krizhevsky, Alex and Ibarz, Julian and Quillen, Deirdre},
  journal={The International Journal of Robotics Research},
  volume={37},
  number={4-5},
  pages={421--436},
  year={2018},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{levine2016end,
  title={End-to-end training of deep visuomotor policies},
  author={Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
  journal={The Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={1334--1373},
  year={2016},
  publisher={JMLR. org}
}

@inproceedings{gu2017deep,
  title={Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates},
  author={Gu, Shixiang and Holly, Ethan and Lillicrap, Timothy and Levine, Sergey},
  booktitle={2017 IEEE international conference on robotics and automation (ICRA)},
  pages={3389--3396},
  year={2017},
  organization={IEEE}
}

@inproceedings{mahler2016dex,
  title={Dex-net 1.0: A cloud-based network of 3d objects for robust grasp planning using a multi-armed bandit model with correlated rewards},
  author={Mahler, Jeffrey and Pokorny, Florian T and Hou, Brian and Roderick, Melrose and Laskey, Michael and Aubry, Mathieu and Kohlhoff, Kai and Kr{\"o}ger, Torsten and Kuffner, James and Goldberg, Ken},
  booktitle={IEEE International Conference on Robotics and Automation (ICRA)},
  pages={1957--1964},
  year={2016},
  organization={IEEE}
}

@article{mahler2017dex,
  title={Dex-Net 2.0: Deep Learning to Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp Metrics},
  author={Mahler, Jeffrey and Liang, Jacky and Niyaz, Sherdil and Laskey, Michael and Doan, Richard and Liu, Xinyu and Ojea, Juan Aparicio and Goldberg, Ken},
  booktitle={Robotics: Science and Systems (RSS)},
  year={2017}
}

@article{Pinto2015SupersizingSL,
  title={Supersizing self-supervision: Learning to grasp from 50K tries and 700 robot hours},
  author={Leontina Pinto and Abhinav Gupta},
  journal={2016 IEEE International Conference on Robotics and Automation (ICRA)},
  year={2015},
  pages={3406-3413}
}

@article{barth2018distributed,
  title={Distributed distributional deterministic policy gradients},
  author={Barth-Maron, Gabriel and Hoffman, Matthew W and Budden, David and Dabney, Will and Horgan, Dan and Muldal, Alistair and Heess, Nicolas and Lillicrap, Timothy},
  journal={arXiv preprint arXiv:1804.08617},
  year={2018}
}

@article{Lillicrap2015ContinuousCW,
  title={Continuous control with deep reinforcement learning},
  author={Timothy P. Lillicrap and Jonathan J. Hunt and Alexander Pritzel and Nicolas Manfred Otto Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},
  journal={CoRR},
  year={2015},
  volume={abs/1509.02971}
}

@article{wang_1996, 
  title={Premium Calculation by Transforming the Layer Premium Density}, 
  volume={26}, DOI={10.2143/AST.26.1.563234}, 
  number={1}, 
  journal={ASTIN Bulletin}, 
  publisher={Cambridge U
  niversity Press}, 
  author={Wang, Shaun}, 
  year={1996}, 
  pages={71–92}
}

@article{Majumdar2017HowSA,
  title={How Should a Robot Assess Risk? Towards an Axiomatic Theory of Risk in Robotics},
  author={Anirudha Majumdar and Marco Pavone},
  journal={ArXiv},
  year={2017},
  volume={abs/1710.11040}
}

@article{Wu1996,
  title = {Curvature of the Probability Weighting Function},
  author = {Wu, George and Gonzalez, Richard},
  year = {1996},
  journal = {Management Science},
  volume = {42},
  number = {12},
  pages = {1676-1690},
  abstract = {When individuals choose among risky alternatives, the psychological weight attached to an outcome may not correspond to the probability of that outcome. In rank-dependent utility theories, including prospect theory, the probability weighting function permits probabilities to be weighted nonlinearly. Previous empirical studies of the weighting function have suggested an inverse S-shaped function, first concave and then convex. However, these studies suffer from a methodological shortcoming: estimation procedures have required assumptions about the functional form of the value and/or weighting functions. We propose two preference conditions that are necessary and sufficient for concavity and convexity of the weighting function. Empirical tests of these conditions are independent of the form of the value function. We test these conditions using preference "ladders" (a series of questions that differ only by a common consequence). The concavity-convexity ladders validate previous findings of an S-shaped weighting function, concave up to p},
  keywords = {decision making; expected utility; nonexpected utility theory; prospect theory; risk; risk aversion},
  url = {https://EconPapers.repec.org/RePEc:inm:ormnsc:v:42:y:1996:i:12:p:1676-1690}
}

@article{Gonzlez1999OnTS,
  title={On the Shape of the Probability Weighting Function},
  author={Rodrigo Gonz{\'a}lez and George Wu},
  journal={Cognitive Psychology},
  year={1999},
  volume={38},
  pages={129-166}
}

@article{Rockafellar00optimizationof,
  author = {R. Tyrrell Rockafellar and Stanislav Uryasev},
  title = {Optimization of Conditional Value-at-Risk},
  journal = {Journal of Risk},
  year = {2000},
  volume = {2},
  pages = {21--41}
}

@article{Wang00aclass,
  author = {Shaun S. Wang},
  title = {A class of distortion operators for pricing financial and insurance risks},
  journal = {Journal of Risk and Insurance},
  year = {2000},
  pages = {15--36}
}

@article{huber1964,
  author = "Huber, Peter J.",
  doi = "10.1214/aoms/1177703732",
  fjournal = "The Annals of Mathematical Statistics",
  journal = "Ann. Math. Statist.",
  month = "03",
  number = "1",
  pages = "73--101",
  publisher = "The Institute of Mathematical Statistics",
  title = "Robust Estimation of a Location Parameter",
  url = "https://doi.org/10.1214/aoms/1177703732",
  volume = "35",
  year = "1964"
}

@inproceedings{Puterman1994MarkovDP,
  title={Markov Decision Processes: Discrete Stochastic Dynamic Programming},
  author={Martin L. Puterman},
  booktitle={Wiley Series in Probability and Statistics},
  year={1994}
}

@article{Kumar2019StabilizingOQ,
  title={Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction},
  author={Aviral Kumar and Justin Fu and George Tucker and Sergey Levine},
  journal={ArXiv},
  year={2019},
  volume={abs/1906.00949}
}

@inproceedings{Hasselt2010DoubleQ,
  title={Double Q-learning},
  author={Hado van Hasselt},
  booktitle={NIPS},
  year={2010}
}

@book{Sutton:1998:IRL:551283,
  author = {Sutton, Richard S. and Barto, Andrew G.},
  title = {Introduction to Reinforcement Learning},
  year = {1998},
  isbn = {0262193981},
  edition = {1st},
  publisher = {MIT Press},
  address = {Cambridge, MA, USA},
 } 

@inproceedings{Rubinstein2004TheCM,
  title={The Cross-Entropy Method},
  author={Reuven Y. Rubinstein and Dirk P. Kroese},
  booktitle={Information Science and Statistics},
  year={2004}
}

@inproceedings{Ioffe:2015:BNA:3045118.3045167,
  author = {Ioffe, Sergey and Szegedy, Christian},
  title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  booktitle = {Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37},
  series = {ICML'15},
  year = {2015},
  location = {Lille, France},
  pages = {448--456},
  numpages = {9},
  url = {http://dl.acm.org/citation.cfm?id=3045118.3045167},
  acmid = {3045167},
  publisher = {JMLR.org},
 }
 
 @article{Ba2016LayerN,
   title={Layer Normalization},
   author={Jimmy Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
   journal={ArXiv},
   year={2016},
   volume={abs/1607.06450}
 }

% imagenet %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{imagenet_cvpr09,
 AUTHOR = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and Fei-Fei, L.},
TITLE = {{ImageNet: A Large-Scale Hierarchical Image Database}},
BOOKTITLE = {CVPR09},
year = {2009},
BIBSOURCE = "http://www.image-net.org/papers/imagenet_cvpr09.bib"}

@inproceedings{Szegedy2015Inception,
title = {Going Deeper with Convolutions},
author  = {Christian Szegedy and Wei Liu and Yangqing Jia and Pierre Sermanet and Scott Reed and Dragomir Anguelov and Dumitru Erhan and Vincent Vanhoucke and Andrew Rabinovich},
year  = {2015},
url = {http://arxiv.org/abs/1409.4842},
biburl = {https://github.com/sermanet/home/blob/master/docs/bib/Szegedy2015Inception.bib},
booktitle = {Computer Vision and Pattern Recognition (CVPR)},
}

@article{He2016DeepRL,
  title={Deep Residual Learning for Image Recognition},
  author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  journal={CVPR},
  year={2016},
  pages={770-778}
}

% sermanet %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{Sermanet2017TCN,
  author    = {Pierre Sermanet and
               Corey Lynch and
               Yevgen Chebotar and
               Jasmine Hsu and
               Eric Jang and
               Stefan Schaal and
               Sergey Levine},
  title     = {Time-Contrastive Networks: Self-Supervised Learning from Video},
  journal   = {International Conference in Robotics and Automation (ICRA)},
  year      = {2018},
  url       = {http://arxiv.org/abs/1704.06888},
  biburl    = {https://github.com/sermanet/home/blob/master/docs/bib/Sermanet2017TCN.bib},
}

@article{Sermanet2017Rewards,
  author    = {Pierre Sermanet and
               Kelvin Xu and
               Sergey Levine},
  title     = {Unsupervised Perceptual Rewards for Imitation Learning},
  journal   = {Proceedings of Robotics: Science and Systems (RSS)},
  year      = {2017},
  url       = {http://arxiv.org/abs/1612.06699},
  biburl    = {https://github.com/sermanet/home/blob/master/docs/bib/Sermanet2017Rewards.bib},
}

@article{Dwibedi2018Actionable,
  author    = {Debidatta Dwibedi and
               Jonathan Tompson and
               Corey Lynch and
               Pierre Sermanet},
  title     = {Learning Actionable Representations from Visual Observations},
  journal   = {International Conference on Intelligent Robots (IROS)},
  year      = {2018},
  url       = {https://arxiv.org/abs/1808.00928},
  biburl    = {https://github.com/sermanet/home/blob/master/docs/bib/Dwibedi2018Actionable.bib},
}

@article{Sermanet2009Multirange,
  title={A Multirange Architecture for Collision-Free Off-Road Robot Navigation},
  author={Sermanet, Pierre and Hadsell, Raia and Scoffier, Marco and Grimes, Matt and Ben, Jan and Erkan, Ayse and Crudele, Chris and Miller, Urs and LeCun, Yann},
  journal={Journal of Field Robotics (JFR)},
  volume={26},
  number={1},
  pages={52--87},
  year={2009},
  publisher={Wiley Online Library},
  url={http://yann.lecun.com/exdb/publis/pdf/sermanet-jfr-09.pdf},
  biburl={https://github.com/sermanet/home/blob/master/docs/bib/Sermanet2009Multirange.bib},
}

@inproceedings{kumar2015mujoco,
  title={MuJoCo HAPTIX: A virtual reality system for hand manipulation},
  author={Kumar, Vikash and Todorov, Emanuel},
  booktitle={Humanoid Robots (Humanoids), 2015 IEEE-RAS 15th International Conference on},
  pages={657--663},
  year={2015},
  organization={IEEE}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@phdthesis{li2006optimal,
  title={Optimal control for biological movement systems},
  author={Li, Weiwei},
  year={2006},
  school={UC San Diego}
}

article{DBLP:journals/corr/BowmanVVDJB15,
  author    = {Samuel R. Bowman and
               Luke Vilnis and
               Oriol Vinyals and
               Andrew M. Dai and
               Rafal J{\'{o}}zefowicz and
               Samy Bengio},
  title     = {Generating Sentences from a Continuous Space},
  journal   = {CoRR},
  volume    = {abs/1511.06349},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.06349},
  archivePrefix = {arXiv},
  eprint    = {1511.06349},
  timestamp = {Mon, 13 Aug 2018 16:47:06 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/BowmanVVDJB15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
<!-- @inproceedings{bowman45404,
title	= {Generating Sentences from a Continuous Space},
author	= {Samuel R. Bowman and Luke Vilnis and Oriol Vinyals and Andrew M. Dai and Rafal Jozefowicz and Samy Bengio},
year	= {2016}
}
 -->
@article{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@article{kober2013reinforcement,
  title={Reinforcement learning in robotics: A survey},
  author={Kober, Jens and Bagnell, J Andrew and Peters, Jan},
  journal={The International Journal of Robotics Research},
  volume={32},
  number={11},
  pages={1238--1274},
  year={2013},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{deisenroth2013survey,
  title={A survey on policy search for robotics},
  author={Deisenroth, Marc Peter and Neumann, Gerhard and Peters, Jan and others},
  journal={Foundations and Trends{\textregistered} in Robotics},
  volume={2},
  number={1--2},
  pages={1--142},
  year={2013},
  publisher={Now Publishers, Inc.}
}

@inproceedings{pastor2009learning,
  title={Learning and generalization of motor skills by learning from demonstration},
  author={Pastor, Peter and Hoffmann, Heiko and Asfour, Tamim and Schaal, Stefan},
  booktitle={Robotics and Automation, 2009. ICRA'09. IEEE International Conference on},
  pages={763--768},
  year={2009},
  organization={IEEE}
}

@incollection{NIPS2015_5775,
title = {Learning Structured Output Representation using Deep Conditional Generative Models},
author = {Sohn, Kihyuk and Lee, Honglak and Yan, Xinchen},
booktitle = {Advances in Neural Information Processing Systems 28},
editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
pages = {3483--3491},
year = {2015},
publisher = {Curran Associates, Inc.},
}

@article{DBLP:journals/corr/abs-1710-04615,
  author    = {Tianhao Zhang and
               Zoe McCarthy and
               Owen Jow and
               Dennis Lee and
               Ken Goldberg and
               Pieter Abbeel},
  title     = {Deep Imitation Learning for Complex Manipulation Tasks from Virtual
               Reality Teleoperation},
  journal   = {CoRR},
  volume    = {abs/1710.04615},
  year      = {2017},
  url       = {http://arxiv.org/abs/1710.04615},
  archivePrefix = {arXiv},
  eprint    = {1710.04615},
  timestamp = {Mon, 13 Aug 2018 16:47:13 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1710-04615},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/RahmatizadehABL17,
  author    = {Rouhollah Rahmatizadeh and
               Pooya Abolghasemi and
               Ladislau B{\"{o}}l{\"{o}}ni and
               Sergey Levine},
  title     = {Vision-Based Multi-Task Manipulation for Inexpensive Robots Using
               End-To-End Learning from Demonstration},
  journal   = {CoRR},
  volume    = {abs/1707.02920},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.02920},
  archivePrefix = {arXiv},
  eprint    = {1707.02920},
  timestamp = {Mon, 13 Aug 2018 16:46:42 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/RahmatizadehABL17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{rajeswaran2017learning,
  title={Learning complex dexterous manipulation with deep reinforcement learning and demonstrations},
  author={Rajeswaran, Aravind and Kumar, Vikash and Gupta, Abhishek and Vezzani, Giulia and Schulman, John and Todorov, Emanuel and Levine, Sergey},
  journal={arXiv preprint arXiv:1709.10087},
  year={2017}
}

@article{DBLP:journals/corr/DuanASHSSAZ17,
  author    = {Yan Duan and
               Marcin Andrychowicz and
               Bradly C. Stadie and
               Jonathan Ho and
               Jonas Schneider and
               Ilya Sutskever and
               Pieter Abbeel and
               Wojciech Zaremba},
  title     = {One-Shot Imitation Learning},
  journal   = {CoRR},
  volume    = {abs/1703.07326},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.07326},
  archivePrefix = {arXiv},
  eprint    = {1703.07326},
  timestamp = {Mon, 13 Aug 2018 16:47:41 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/DuanASHSSAZ17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{kaelbling1993learning,
  title={Learning to achieve goals},
  author={Kaelbling, Leslie Pack},
  booktitle={IJCAI},
  pages={1094--1099},
  year={1993},
  organization={Citeseer}
}

@article{pong2018temporal,
  title={Temporal difference models: Model-free deep rl for model-based control},
  author={Pong, Vitchyr and Gu, Shixiang and Dalal, Murtaza and Levine, Sergey},
  journal={arXiv preprint arXiv:1802.09081},
  year={2018}
}

@inproceedings{nair2018visual,
  title={Visual reinforcement learning with imagined goals},
  author={Nair, Ashvin V and Pong, Vitchyr and Dalal, Murtaza and Bahl, Shikhar and Lin, Steven and Levine, Sergey},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9209--9220},
  year={2018}
}

@inproceedings{schaul2015universal,
  title={Universal value function approximators},
  author={Schaul, Tom and Horgan, Daniel and Gregor, Karol and Silver, David},
  booktitle={International Conference on Machine Learning},
  pages={1312--1320},
  year={2015}
}

@inproceedings{andrychowicz2017hindsight,
  title={Hindsight experience replay},
  author={Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, OpenAI Pieter and Zaremba, Wojciech},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5048--5058},
  year={2017}
}

@article{DBLP:journals/corr/abs-1712-00948,
  author    = {Andrew Levy and
               Robert Platt Jr. and
               Kate Saenko},
  title     = {Hierarchical Actor-Critic},
  journal   = {CoRR},
  volume    = {abs/1712.00948},
  year      = {2017},
  url       = {http://arxiv.org/abs/1712.00948},
  archivePrefix = {arXiv},
  eprint    = {1712.00948},
  timestamp = {Mon, 13 Aug 2018 16:46:09 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1712-00948},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1711-06006,
  author    = {Paulo Rauber and
               Filipe Mutz and
               J{\"{u}}rgen Schmidhuber},
  title     = {Hindsight policy gradients},
  journal   = {CoRR},
  volume    = {abs/1711.06006},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.06006},
  archivePrefix = {arXiv},
  eprint    = {1711.06006},
  timestamp = {Mon, 13 Aug 2018 16:47:50 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1711-06006},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/CabiCHDWF17,
  author    = {Serkan Cabi and
               Sergio Gomez Colmenarejo and
               Matthew W. Hoffman and
               Misha Denil and
               Ziyu Wang and
               Nando de Freitas},
  title     = {The Intentional Unintentional Agent: Learning to Solve Many Continuous
               Control Tasks Simultaneously},
  journal   = {CoRR},
  volume    = {abs/1707.03300},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.03300},
  archivePrefix = {arXiv},
  eprint    = {1707.03300},
  timestamp = {Mon, 13 Aug 2018 16:46:46 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/CabiCHDWF17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/SukhbaatarKSF17,
  author    = {Sainbayar Sukhbaatar and
               Ilya Kostrikov and
               Arthur Szlam and
               Rob Fergus},
  title     = {Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play},
  journal   = {CoRR},
  volume    = {abs/1703.05407},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.05407},
  archivePrefix = {arXiv},
  eprint    = {1703.05407},
  timestamp = {Mon, 13 Aug 2018 16:46:02 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/SukhbaatarKSF17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/AgrawalNAML16,
  author    = {Pulkit Agrawal and
               Ashvin Nair and
               Pieter Abbeel and
               Jitendra Malik and
               Sergey Levine},
  title     = {Learning to Poke by Poking: Experiential Learning of Intuitive Physics},
  journal   = {CoRR},
  volume    = {abs/1606.07419},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.07419},
  archivePrefix = {arXiv},
  eprint    = {1606.07419},
  timestamp = {Mon, 13 Aug 2018 16:46:07 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/AgrawalNAML16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/NairCAIAML17,
  author    = {Ashvin Nair and
               Dian Chen and
               Pulkit Agrawal and
               Phillip Isola and
               Pieter Abbeel and
               Jitendra Malik and
               Sergey Levine},
  title     = {Combining Self-Supervised Learning and Imitation for Vision-Based
               Rope Manipulation},
  journal   = {CoRR},
  volume    = {abs/1703.02018},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.02018},
  archivePrefix = {arXiv},
  eprint    = {1703.02018},
  timestamp = {Mon, 13 Aug 2018 16:47:21 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/NairCAIAML17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{christiano2016transfer,
  title={Transfer from simulation to real world through learning deep inverse dynamics model},
  author={Christiano, Paul and Shah, Zain and Mordatch, Igor and Schneider, Jonas and Blackwell, Trevor and Tobin, Joshua and Abbeel, Pieter and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1610.03518},
  year={2016}
}

@article{DBLP:journals/corr/abs-1805-01954,
  author    = {Faraz Torabi and
               Garrett Warnell and
               Peter Stone},
  title     = {Behavioral Cloning from Observation},
  journal   = {CoRR},
  volume    = {abs/1805.01954},
  year      = {2018},
  url       = {http://arxiv.org/abs/1805.01954},
  archivePrefix = {arXiv},
  eprint    = {1805.01954},
  timestamp = {Mon, 13 Aug 2018 16:47:35 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1805-01954},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{pathakICLR18zeroshot,
    Author = {Pathak, Deepak and
    Mahmoudieh, Parsa and Luo, Guanghao and
    Agrawal, Pulkit and Chen, Dian and
    Shentu, Yide and Shelhamer, Evan and
    Malik, Jitendra and Efros, Alexei A. and
    Darrell, Trevor},
    Title = {Zero-Shot Visual Imitation},
    Booktitle = {ICLR},
    Year = {2018}
}

@inproceedings{rahmatizadeh2018vision,
  title={Vision-based multi-task manipulation for inexpensive robots using end-to-end learning from demonstration},
  author={Rahmatizadeh, Rouhollah and Abolghasemi, Pooya and Boloni, Ladislau and Levine, Sergey},
  booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={3758--3765},
  year={2018},
  organization={IEEE}
}

%% sergey %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{agrawal2016poking,
abstract = {We investigate an experiential learning paradigm for acquiring an internal model of intuitive physics. Our model is evaluated on a real-world robotic manipulation task that requires displacing objects to target locations by poking. The robot gathered over 400 hours of experience by executing more than 100K pokes on different objects. We propose a novel approach based on deep neural networks for modeling the dynamics of robot's interactions directly from images, by jointly estimating forward and inverse models of dynamics. The inverse model objective provides supervision to construct informative visual features, which the forward model can then predict and in turn regularize the feature space for the inverse model. The interplay between these two objectives creates useful, accurate models that can then be used for multi-step decision making. This formulation has the additional benefit that it is possible to learn forward models in an abstract feature space and thus alleviate the need of predicting pixels. Our experiments show that this joint modeling approach outperforms alternative methods.},
archivePrefix = {arXiv},
arxivId = {1606.07419},
author = {Agrawal, Pulkit and Nair, Ashvin and Abbeel, Pieter and Malik, Jitendra and Levine, Sergey},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
eprint = {1606.07419},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Agrawal et al. - Unknown - Learning to Poke by Poking Experiential Learning of Intuitive Physics.pdf:pdf},
issn = {10495258},
title = {{Learning to Poke by Poking: Experiential Learning of Intuitive Physics}},
url = {http://arxiv.org/abs/1606.07419},
year = {2016}
}
@inproceedings{lange2012autonomous,
abstract = {We propose a learning architecture, that is able to do reinforcement learning based on raw visual input data. In contrast to previous approaches, not only the control policy is learned. In order to be successful, the system must also autonomously learn, how to extract relevant information out of a high-dimensional stream of input information, for which the semantics are not provided to the learning system. We give a first proof-of-concept of this novel learning architecture on a challenging benchmark, namely visual control of a racing slot car. The resulting policy, learned only by success or failure, is hardly beaten by an experienced human player.},
author = {Lange, Sascha and Riedmiller, Martin and Voigtlander, Arne and Voigtl{\"{a}}nder, Arne},
booktitle = {International Joint Conference on Neural Networks (IJCNN)},
doi = {10.1109/IJCNN.2012.6252823},
file = {:Users/ashvin/Downloads/Autonomous{\_}reinforcement{\_}learning{\_}on{\_}raw{\_}visual{\_}in.pdf:pdf},
isbn = {9781467314909},
issn = {2161-4393},
number = {June},
organization = {IEEE},
pages = {1--8},
title = {{Autonomous reinforcement learning on raw visual input data in a real world application}},
year = {2012}
}
@inproceedings{kingma2014vae,
abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differ-entiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using stan-dard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made espe-cially efficient by fitting an approximate inference model (also called a recogni-tion model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
author = {Kingma, Diederik P and Welling, Max},
booktitle = {International Conference on Learning Representations (ICLR)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Kingma, Welling - Unknown - Auto-Encoding Variational Bayes(2).pdf:pdf;:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Kingma, Welling - Unknown - Auto-Encoding Variational Bayes.pdf:pdf},
title = {{Auto-Encoding Variational Bayes}},
url = {https://arxiv.org/pdf/1312.6114.pdf},
year = {2014}
}
@article{levine2016gps,
abstract = {Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control. In this paper, we aim to answer the following question: does training the perception and control systems jointly end-to-end provide better performance than training each component separately? To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot's motors. The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method. We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods.},
archivePrefix = {arXiv},
arxivId = {1504.00702},
author = {Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1504.00702},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Levine et al. - 2016 - End-to-End Training of Deep Visuomotor Policies.pdf:pdf;:Users/ashvin/code/kindlize/pdfs/deeprl/end-to-end-deep-visuomotor-policies-Levine15.pdf:pdf},
isbn = {9781479969227},
issn = {15337928},
journal = {Journal of Machine Learning Research (JMLR)},
keywords = {Neural Networks,Optimal Control,Reinforcement Learning,Vision},
number = {1},
pages = {1334--1373},
pmid = {15003161},
publisher = {JMLR. org},
title = {{End-to-End Training of Deep Visuomotor Policies}},
url = {https://arxiv.org/pdf/1504.00702.pdf},
volume = {17},
year = {2016}
}
@article{bojarski2016nvidia,
abstract = {We trained a convolutional neural network (CNN) to map raw pixels from a single front-facing camera directly to steering commands. This end-to-end approach proved surprisingly powerful. With minimum training data from humans the system learns to drive in traffic on local roads with or without lane markings and on highways. It also operates in areas with unclear visual guidance such as in parking lots and on unpaved roads. The system automatically learns internal representations of the necessary processing steps such as detecting useful road features with only the human steering angle as the training signal. We never explicitly trained it to detect, for example, the outline of roads. Compared to explicit decomposition of the problem, such as lane marking detection, path planning, and control, our end-to-end system optimizes all processing steps simultaneously. We argue that this will eventually lead to better performance and smaller systems. Better performance will result because the internal components self-optimize to maximize overall system performance, instead of optimizing human-selected intermediate criteria, e.g., lane detection. Such criteria understandably are selected for ease of human interpretation which doesn't automatically guarantee maximum system performance. Smaller networks are possible because the system learns to solve the problem with the minimal number of processing steps. We used an NVIDIA DevBox and Torch 7 for training and an NVIDIA DRIVE(TM) PX self-driving car computer also running Torch 7 for determining where to drive. The system operates at 30 frames per second (FPS).},
archivePrefix = {arXiv},
arxivId = {1604.07316},
author = {Bojarski, Mariusz and {Del Testa}, Davide and Dworakowski, Daniel and Firner, Bernhard and Flepp, Beat and Goyal, Prasoon and Jackel, Lawrence D. and Monfort, Mathew and Muller, Urs and Zhang, Jiakai and Zhang, Xin and Zhao, Jake and Zieba, Karol and Testa, Davide Del and Dworakowski, Daniel and Firner, Bernhard and Flepp, Beat and Goyal, Prasoon and Jackel, Lawrence D. and Monfort, Mathew and Muller, Urs and Zhang, Jiakai and Zhang, Xin and Zhao, Jake and Zieba, Karol},
eprint = {1604.07316},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Bojarski et al. - 2016 - End to End Learning for Self-Driving Cars(2).pdf:pdf;:Users/ashvin/code/kindlize/pdfs/deeprl/end-to-end-self-driving-cars.pdf:pdf},
journal = {CoRR},
pages = {1--9},
title = {{End to End Learning for Self-Driving Cars}},
url = {https://arxiv.org/pdf/1604.07316.pdf http://arxiv.org/abs/1604.07316},
volume = {abs/1604.0},
year = {2016}
}
@inproceedings{mnih2016asynchronous,
abstract = {We propose a conceptually simple and lightweight framework for deep reinforce-ment learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
author = {Mnih, Volodymyr and {Puigdom{\`{e}}nech Badia}, Adri{\`{a}} and Mirza, Mehdi and Graves, Alex and Harley, Tim and Lillicrap, Timothy P and Silver, David and Kavukcuoglu, Koray and Com, Korayk@google and Deepmind, Google},
booktitle = {International Conference on Machine Learning (ICML)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Mnih et al. - Unknown - Asynchronous Methods for Deep Reinforcement Learning.pdf:pdf},
title = {{Asynchronous Methods for Deep Reinforcement Learning}},
url = {https://arxiv.org/pdf/1602.01783.pdf},
year = {2016}
}
@article{jordan1992forward,
abstract = {Internal models of the environment h a v e an important role to play in adap-tive systems in general and are of particular importance for the supervised learning paradigm. In this paper we demonstrate that certain classical prob-lems associated with the notion of the eteacher" in supervised learning can be solved by judicious use of learned internal models as components of the adap-tive system. In particular, we show h o w supervised learning algorithms can be utilized in cases in which an unknown dynamical system intervenes between actions and desired outcomes. Our approach applies to any supervised learning algorithm that is capable of learning in multi-layer networks.},
author = {Jordan, Michael I and Rumelhart, David E and Mozer, Michael and Barto, Andrew and Jacobs, Robert and Loeb, Eric and Mcclelland, James},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Jordan et al. - 1992 - Forward models Supervised learning with a distal teacher.pdf:pdf},
journal = {Cognitive Science},
number = {3},
pages = {307--354},
publisher = {Wiley Online Library},
title = {{Forward models: Supervised learning with a distal teacher}},
url = {https://pdfs.semanticscholar.org/bea6/713f0bdd8069d734846fc532660c3152d027.pdf},
volume = {16},
year = {1992}
}
@inproceedings{hester17dqfd,
archivePrefix = {arXiv},
arxivId = {1704.03732},
author = {Hester, Todd and Vecerik, Matej and Pietquin, Olivier and Lanctot, Marc and Schaul, Tom and Piot, Bilal and Horgan, Dan and Quan, John and Sendonaris, Andrew and Dulac-Arnold, Gabriel and Osband, Ian and Agapiou, John and Leibo, Joel Z and Gruslys, Audrunas},
booktitle = {AAAI Conference on Artificial Intelligence},
eprint = {1704.03732},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Hester et al. - 2017 - Learning from Demonstrations for Real World Reinforcement Learning.pdf:pdf},
title = {{Learning from Demonstrations for Real World Reinforcement Learning}},
url = {https://arxiv.org/pdf/1704.03732.pdf http://arxiv.org/abs/1704.03732},
year = {2018}
}
@inproceedings{torralba,
author = {Vondrick, Carl and Pirsiavash, Hamed and Torralba, Antonio},
booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {{Anticipating Visual Representations from Unlabeled Video}},
year = {2016}
}
@inproceedings{nair2017icra,
abstract = {{\textcopyright} 2017 IEEE. Manipulation of deformable objects, such as ropes and cloth, is an important but challenging problem in robotics. We present a learning-based system where a robot takes as input a sequence of images of a human manipulating a rope from an initial to goal configuration, and outputs a sequence of actions that can reproduce the human demonstration, using only monocular images as input. To perform this task, the robot learns a pixel-level inverse dynamics model of rope manipulation directly from images in a self-supervised manner, using about 60K interactions with the rope collected autonomously by the robot. The human demonstration provides a high-level plan of what to do and the low-level inverse model is used to execute the plan. We show that by combining the high and low-level plans, the robot can successfully manipulate a rope into a variety of target shapes using only a sequence of human-provided images for direction.},
archivePrefix = {arXiv},
arxivId = {1703.02018},
author = {Nair, Ashvin and Chen, Dian and Agrawal, Pulkit and Isola, Phillip and Abbeel, Pieter and Malik, Jitendra and Levine, Sergey and Chen, Dian and Isola, Phillip and Abbeel, Pieter and Malik, Jitendra and Levine, Sergey},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2017.7989247},
eprint = {1703.02018},
file = {:Users/ashvin/code/kindlize/pdfs/deeprl/rope-manipulation-Nair17.pdf:pdf},
isbn = {9781509046331},
issn = {10504729},
title = {{Combining Self-Supervised Learning and Imitation for Vision-Based Rope Manipulation}},
year = {2017}
}
@inproceedings{andrychowicz2017her,
archivePrefix = {arXiv},
arxivId = {1707.01495},
author = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and Mcgrew, Bob and Tobin, Josh and Abbeel, Pieter and Zaremba, Wojciech},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
eprint = {1707.01495},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Andrychowicz et al. - 2017 - Hindsight Experience Replay(2).pdf:pdf},
month = {jul},
title = {{Hindsight Experience Replay}},
url = {https://arxiv.org/pdf/1707.01495.pdf http://arxiv.org/abs/1707.01495},
year = {2017}
}
@inproceedings{ebert2017videoprediction,
abstract = {In order to autonomously learn wide repertoires of complex skills, robots must be able to learn from their own autonomously collected data, without human supervision. One learning signal that is always available for autonomously collected data is prediction. If a robot can learn to predict the future, it can use this predictive model to take actions to produce desired outcomes, such as mov-ing an object to a particular location. However, in complex open-world scenarios, designing a representation for prediction is difficult. In this work, we instead aim to enable self-supervised robot learning through direct video prediction: instead of attempting to design a good representation, we directly predict what the robot will see next, and then use this model to achieve desired goals. A key challenge in video prediction for robotic manipulation is handling complex spatial arrange-ments such as occlusions. To that end, we introduce a video prediction model that can keep track of objects through occlusion by incorporating temporal skip-connections. Together with a novel planning criterion and action space formu-lation, we demonstrate that this model substantially outperforms prior work on video prediction-based control. Our results show manipulation of objects not seen during training, handling multiple objects, and pushing objects around obstruc-tions. These results represent a significant advance in the range and complexity of skills that can be performed entirely with self-supervised robot learning.},
author = {Ebert, Frederik and Finn, Chelsea and Lee, Alex X and Levine, Sergey},
booktitle = {Conference on Robot Learning (CoRL)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Ebert et al. - Unknown - Self-Supervised Visual Planning with Temporal Skip Connections(2).pdf:pdf},
keywords = {,deep learning,manipulation,model-based rein-forcement learning,video prediction},
title = {{Self-Supervised Visual Planning with Temporal Skip Connections}},
url = {https://128.84.21.199/pdf/1710.05268.pdf https://arxiv.org/pdf/1710.05268.pdf},
year = {2017}
}
@inproceedings{watter2015embed,
abstract = {We introduce Embed to Control (E2C), a method for model learning and control of non-linear dynamical systems from raw pixel images. E2C consists of a deep generative model, belonging to the family of variational autoencoders, that learns to generate image trajectories from a latent space in which the dynamics is constrained to be locally linear. Our model is derived directly from an optimal control formulation in latent space, supports long-term prediction of image sequences and exhibits strong performance on a variety of complex control problems.},
archivePrefix = {arXiv},
arxivId = {1506.07365},
author = {Watter, Manuel and Springenberg, Jost Tobias and Boedecker, Joschka and Riedmiller, Martin},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
eprint = {1506.07365},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Watter et al. - Unknown - Embed to Control A Locally Linear Latent Dynamics Model for Control from Raw Images.pdf:pdf;:Users/ashvin/code/kindlize/pdfs/deeprl/embed2control-Watter15.pdf:pdf},
issn = {10495258},
pages = {2728--2736},
title = {{Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images}},
url = {https://arxiv.org/pdf/1506.07365.pdf http://arxiv.org/abs/1506.07365},
year = {2015}
}
@inproceedings{lange2010deep,
abstract = {This paper discusses the effectiveness of deep auto-encoding neural nets in visual reinforcement learning (RL) tasks. We describe a new algorithm and give results on succesfully learning policies directly on synthesized and real images without a predefined image processing. Fur-thermore, we present a thorough evaluation of the learned feature spaces.},
author = {Lange, Sascha and Riedmiller, Martin A},
booktitle = {European Symposium on Artificial Neural Networks (ESANN)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Lange, Riedmiller - 2010 - Deep Learning of Visual Control Policies.pdf:pdf},
organization = {Citeseer},
title = {{Deep learning of visual control policies.}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.226.6898{\&}rep=rep1{\&}type=pdf},
year = {2010}
}
@article{popov17stacking,
abstract = {Deep learning and reinforcement learning methods have recently been used to solve a variety of problems in continuous control domains. An obvious application of these techniques is dexterous manipulation tasks in robotics which are difficult to solve using traditional control theory or hand-engineered approaches. One example of such a task is to grasp an object and precisely stack it on another. Solving this difficult and practically relevant problem in the real world is an important long-term goal for the field of robotics. Here we take a step towards this goal by examining the problem in simulation and providing models and techniques aimed at solving it. We introduce two extensions to the Deep Deterministic Policy Gradient algorithm (DDPG), a model-free Q-learning based method, which make it significantly more data-efficient and scalable. Our results show that by making extensive use of off-policy data and replay, it is possible to find control policies that robustly grasp objects and stack them. Further, our results hint that it may soon be feasible to train successful stacking policies by collecting interactions on real robots.},
archivePrefix = {arXiv},
arxivId = {1704.03073},
author = {Popov, Ivaylo and Heess, Nicolas and Lillicrap, Timothy and Hafner, Roland and Barth-maron, Gabriel and Vecerik, Matej and Lampe, Thomas and Tassa, Yuval and Erez, Tom and Riedmiller, Martin and Deepmind, Martin Riedmiller},
eprint = {1704.03073},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Popov et al. - Unknown - Data-efficient Deep Reinforcement Learning for Dexterous Manipulation(2).pdf:pdf},
journal = {CoRR},
keywords = {popov2017stacking},
title = {{Data-efficient Deep Reinforcement Learning for Dexterous Manipulation}},
url = {https://arxiv.org/pdf/1704.03073.pdf},
volume = {abs/1704.0},
year = {2017}
}
@inproceedings{rauber2017hindsight,
abstract = {Goal-conditional policies allow reinforcement learning agents to pursue specific goals during different episodes. In addition to their potential to generalize desired behavior to unseen goals, such policies may also help in defining options for arbi-trary subgoals, enabling higher-level planning. While trying to achieve a specific goal, an agent may also be able to exploit information about the degree to which it has achieved alternative goals. Reinforcement learning agents have only recently been endowed with such capacity for hindsight, which is highly valuable in environ-ments with sparse rewards. In this paper, we show how hindsight can be introduced to likelihood-ratio policy gradient methods, generalizing this capacity to an entire class of highly successful algorithms. Our preliminary experiments suggest that hindsight may increase the sample efficiency of policy gradient methods.},
author = {Rauber, Paulo and Mutz, Filipe and Schmidhuber, Juergen J{\"{u}}rgen},
booktitle = {CoRR},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Rauber, Mutz, Schmidhuber - Unknown - Hindsight policy gradients.pdf:pdf},
title = {{Hindsight policy gradients}},
url = {https://arxiv.org/pdf/1711.06006.pdf},
volume = {abs/1711.0},
year = {2017}
}
@inproceedings{lillicrap2015continuous,
abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the de-terministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our al-gorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is com-petitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies " end-to-end " : directly from raw pixel in-puts.},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
booktitle = {International Conference on Learning Representations (ICLR)},
doi = {10.1613/jair.301},
eprint = {9605103},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Lillicrap et al. - Unknown - CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING(2).pdf:pdf;:Users/ashvin/code/kindlize/pdfs/deeprl/continuous-control-Lillicrap16.pdf:pdf},
isbn = {0-7803-3213-X},
issn = {10769757},
pmid = {17255001},
primaryClass = {cs},
title = {{Continuous control with deep reinforcement learning}},
url = {https://arxiv.org/pdf/1509.02971.pdf},
year = {2016}
}
@inproceedings{kober2008mp,
abstract = {Many motor skills in humanoid robotics can be learned using parametrized motor primitives as done in imitation learning. However, most interesting motor learn-ing problems are high-dimensional reinforcement learning problems often beyond the reach of current methods. In this paper, we extend previous work on policy learning from the immediate reward case to episodic reinforcement learning. We show that this results in a general, common framework also connected to pol-icy gradient methods and yielding a novel algorithm for policy learning that is particularly well-suited for dynamic motor primitives. The resulting algorithm is an EM-inspired algorithm applicable to complex motor learning tasks. We compare this algorithm to several well-known parametrized policy search methods and show that it outperforms them. We apply it in the context of motor learning and show that it can learn a complex Ball-in-a-Cup task using a real Barrett WAM TM robot arm.},
author = {Kober, Jens and Peter, J.},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
doi = {10.1007/978-3-319-03194-1_4},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Kober, Peter - 2014 - Policy search for motor primitives in robotics.pdf:pdf},
isbn = {1099401052236},
issn = {1610742X},
pages = {83--117},
title = {{Policy search for motor primitives in robotics}},
url = {http://papers.nips.cc/paper/3545-policy-search-for-motor-primitives-in-robotics.pdf},
volume = {97},
year = {2008}
}
@inproceedings{deisenroth2011pilco,
abstract = {In this paper, we introduce pilco, a practi-cal, data-efficient model-based policy search method. Pilco reduces model bias, one of the key problems of model-based reinforce-ment learning, in a principled way. By learn-ing a probabilistic dynamics model and ex-plicitly incorporating model uncertainty into long-term planning, pilco can cope with very little data and facilitates learning from scratch in only a few trials. Policy evaluation is performed in closed form using state-of-the-art approximate inference. Furthermore, policy gradients are computed analytically for policy improvement. We report unprece-dented learning efficiency on challenging and high-dimensional control tasks.},
author = {Deisenroth, Marc Peter and Rasmussen, Carl Edward},
booktitle = {International Conference on Machine Learning (ICML)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Deisenroth, Rasmussen - Unknown - PILCO A Model-Based and Data-Efficient Approach to Policy Search.pdf:pdf},
pages = {465--472},
title = {{PILCO: A model-based and data-efficient approach to policy search}},
url = {http://mlg.eng.cam.ac.uk/pub/pdf/DeiRas11.pdf},
year = {2011}
}
@inproceedings{gu2016naf,
abstract = {Model-free reinforcement learning has been successfully applied to a range of challenging problems, and has recently been extended to handle large neural network policies and value functions. However, the sample complexity of model-free algorithms, particularly when using high-dimensional function approximators, tends to limit their applicability to physical systems. In this paper, we explore algorithms and representations to reduce the sample complexity of deep reinforcement learning for continuous control tasks. We propose two complementary techniques for improving the efficiency of such algorithms. First, we derive a continuous variant of the Q-learning algorithm, which we call normalized adantage functions (NAF), as an alternative to the more commonly used policy gradient and actor-critic methods. NAF representation allows us to apply Q-learning with experience replay to continuous tasks, and substantially improves performance on a set of simulated robotic control tasks. To further improve the efficiency of our approach, we explore the use of learned models for accelerating model-free reinforcement learning. We show that iteratively refitted local linear models are especially effective for this, and demonstrate substantially faster learning on domains where such models are applicable.},
archivePrefix = {arXiv},
arxivId = {1603.00748},
author = {Gu, Shixiang and Lillicrap, Timothy and Sutskever, Ilya and Levine, Sergey},
booktitle = {International Conference on Machine Learning (ICML)},
doi = {10.3390/robotics2030122},
eprint = {1603.00748},
file = {:Users/ashvin/code/kindlize/pdfs/deeprl/continuous-deepq-qlearning-with-model-based-acceleration-Gu16.pdf:pdf;:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Gu et al. - Unknown - Continuous Deep Q-Learning with Model-based Acceleration.pdf:pdf},
isbn = {3405062780},
issn = {{\textless}null{\textgreater}},
pmid = {21487784},
title = {{Continuous Deep Q-Learning with Model-based Acceleration}},
url = {https://arxiv.org/pdf/1603.00748.pdf http://arxiv.org/abs/1603.00748},
year = {2016}
}
@inproceedings{oh2015action,
abstract = {Motivated by vision-based reinforcement learning (RL) problems, in particular Atari games from the recent benchmark Aracade Learning Environment (ALE), we consider spatio-temporal prediction problems where future (image-)frames are dependent on control variables or actions as well as previous frames. While not composed of natural scenes, frames in Atari games are high-dimensional in size, can involve tens of objects with one or more objects being controlled by the actions directly and many other objects being influenced indirectly, can involve entry and departure of objects, and can involve deep partial observability. We propose and evaluate two deep neural network architectures that consist of encoding, action-conditional transformation, and decoding layers based on convolutional neural networks and recurrent neural networks. Experimental results show that the pro-posed architectures are able to generate visually-realistic frames that are also use-ful for control over approximately 100-step action-conditional futures in some games. To the best of our knowledge, this paper is the first to make and evaluate long-term predictions on high-dimensional video conditioned by control inputs.},
author = {Oh, Junhyuk and Guo, Xiaoxiao and Lee, Honglak and Lewis, Richard and Singh, Satinder},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Oh et al. - 2015 - Action-Conditional Video Prediction using Deep Networks in Atari Games.pdf:pdf},
title = {{Action-Conditional Video Prediction using Deep Networks in Atari Games}},
url = {https://arxiv.org/pdf/1507.08750v1.pdf},
year = {2015}
}
@article{henderson2017deep,
author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
journal = {arXiv preprint arXiv:1709.06560},
title = {{Deep reinforcement learning that matters}},
year = {2017}
}
@inproceedings{inoue1985hand,
author = {Inoue, H and Inaba, M},
booktitle = {obotics Research: The First International Symposium},
title = {{Hand-eye coordination in rope handling}},
volume = {1},
year = {1985}
}
@book{winograd72shrdlr,
author = {Winograd, Terry},
publisher = {Academic Press},
title = {{Understanding Natural Language}},
year = {1972}
}
@article{billiards,
author = {Fragkiadaki, Katerina and Agrawal, Pulkit and Levine, Sergey and Malik, Jitendra},
journal = {International Conference on Learning Representations (ICLR)},
title = {{Learning Visual Predictive Models of Physics for Playing Billiards}},
year = {2016}
}
@article{burgess2018understanding,
author = {Burgess, Christopher P and Higgins, Irina and Pal, Arka and Matthey, Loic and Watters, Nick and Desjardins, Guillaume and Lerchner, Alexander},
journal = {arXiv preprint arXiv:1804.03599},
title = {{Understanding disentangling in {\$}\beta{\$}-VAE}},
year = {2018}
}
@article{machado2018eigenoption,
author = {Machado, Marlos C and Rosenbaum, Clemens and Guo, Xiaoxiao and Liu, Miao and Tesauro, Gerald and Campbell, Murray},
journal = {International Conference on Learning Representations (ICLR)},
title = {{Eigenoption Discovery through the Deep Successor Representation}},
year = {2018}
}
@inproceedings{kopicki2011learning,
author = {Kopicki, Marek and Zurek, Sebastian and Stolkin, Rustam and M{\"{o}}rwald, Thomas and Wyatt, Jeremy},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
organization = {IEEE},
pages = {5722--5729},
title = {{Learning to predict how rigid objects behave under simple manipulation}},
year = {2011}
}
@article{kuniyoshi1994learning,
author = {Kuniyoshi, Yasuo and Inaba, Masayuki and Inoue, Hirochika},
journal = {IEEE transactions on robotics and automation},
number = {6},
pages = {799--822},
publisher = {IEEE},
title = {{Learning by watching: Extracting reusable task knowledge from visual observation of human performance}},
volume = {10},
year = {1994}
}
@article{jaderberg2016auxiliary,
author = {Jaderberg, Max and Mnih, Volodymyr and Czarnecki, Wojciech Marian and Schaul, Tom and Leibo, Joel Z and Silver, David and Kavukcuoglu, Koray},
journal = {International Conference on Learning Representations (ICLR)},
title = {{Reinforcement learning with unsupervised auxiliary tasks}},
year = {2017}
}
@article{hansen2001completely,
author = {Hansen, Nikolaus and Ostermeier, Andreas},
journal = {Evolutionary computation},
number = {2},
pages = {159--195},
publisher = {MIT Press},
title = {{Completely derandomized self-adaptation in evolution strategies}},
volume = {9},
year = {2001}
}
@article{pinto2016curious,
author = {Pinto, Lerrel and Gandhi, Dhiraj and Han, Yuanfeng and Park, Yong-Lae and Gupta, Abhinav},
journal = {European Conference on Computer Vision (ECCV)},
title = {{The Curious Robot: Learning Visual Representations via Physical Interactions}},
year = {2016}
}
@inproceedings{lau2011automatic,
author = {Lau, Manfred and Mitani, Jun and Igarashi, Takeo},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
organization = {IEEE},
pages = {3733--3738},
title = {{Automatic learning of pushing strategy for delivery of irregular-shaped objects}},
year = {2011}
}
@article{kingma2014adam,
author = {Kingma, Diederik and Ba, Jimmy},
journal = {International Conference on Learning Representations (ICLR)},
title = {Adam: A method for stochastic optimization},
year = {2015}
}
@article{ponomarenko2015image,
author = {Ponomarenko, Nikolay and Jin, Lina and Ieremeiev, Oleg and Lukin, Vladimir and Egiazarian, Karen and Astola, Jaakko and Vozel, Benoit and Chehdi, Kacem and Carli, Marco and Battisti, Federica and Others},
journal = {Signal Processing: Image Communication},
pages = {57--77},
publisher = {Elsevier},
title = {{Image database TID2013: Peculiarities, results and perspectives}},
volume = {30},
year = {2015}
}
@article{schmidhuber1992learning,
author = {Schmidhuber, J{\"{u}}rgen},
journal = {Neural Computation},
number = {6},
pages = {863--879},
publisher = {MIT Press},
title = {{Learning factorial codes by predictability minimization}},
volume = {4},
year = {1992}
}
@inproceedings{abbeel2004apprenticeship,
author = {Abbeel, Pieter and Ng, Andrew Y},
booktitle = {International Conference on Machine Learning (ICML)},
pages = {1},
title = {{Apprenticeship learning via inverse reinforcement learning}},
year = {2004}
}
@article{aksoy2011learning,
author = {Aksoy, Eren Erdal and Abramov, Alexey and D{\"{o}}rr, Johannes and Ning, Kejun and Dellen, Babette and W{\"{o}}rg{\"{o}}tter, Florentin},
journal = {The International Journal of Robotics Research},
pages = {0278364911410459},
publisher = {Sage Publications},
title = {{Learning the semantics of object--action relations by observation}},
year = {2011}
}
@inproceedings{saha2006motion,
author = {Saha, Mitul and Isto, Pekka},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
organization = {IEEE},
pages = {2478--2484},
title = {{Motion planning for robotic manipulation of deformable linear objects}},
year = {2006}
}
@article{ha2018world,
author = {Ha, David and Schmidhuber, J{\"{u}}rgen},
journal = {arXiv preprint arXiv:1803.10122},
title = {{World Models}},
year = {2018}
}
@inproceedings{DBLP:conf/bmvc/KyriazisOA11,
author = {Kyriazis, Nikolaos and Oikonomidis, Iason and Argyros, Antonis A},
booktitle = {British Machine Vision Conference, {\{}BMVC{\}} 2011, Dundee, UK, August 29 - September 2, 2011. Proceedings},
doi = {10.5244/C.25.43},
pages = {1--11},
title = {{Binding Computer Vision to Physics Based Simulation: The Case Study of a Bouncing Ball}},
url = {http://dx.doi.org/10.5244/C.25.43},
year = {2011}
}
@article{lee2013syntactic,
author = {Lee, Kyuhwa and Su, Yanyu and Kim, Tae-Kyun and Demiris, Yiannis},
journal = {Robotics and Autonomous Systems},
number = {12},
pages = {1323--1334},
publisher = {Elsevier},
title = {{A syntactic approach to robot imitation learning using probabilistic activity grammars}},
volume = {61},
year = {2013}
}
@inproceedings{katz2008manipulating,
author = {Katz, Dov and Brock, Oliver},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
organization = {IEEE},
pages = {272--277},
title = {{Manipulating articulated objects with interactive perception}},
year = {2008}
}
@article{florensa2017stochastic,
author = {Florensa, Carlos and Duan, Yan and Abbeel, Pieter},
journal = {arXiv preprint arXiv:1704.03012},
title = {{Stochastic neural networks for hierarchical reinforcement learning}},
year = {2017}
}
@article{mccloskey1983intuitive,
author = {McCloskey, Michael},
journal = {Scientific american},
number = {4},
pages = {122--130},
title = {{Intuitive physics}},
volume = {248},
year = {1983}
}
@article{pinto2015supersizing,
author = {Pinto, Lerrel and Gupta, Abhinav},
journal = {IEEE International Conference on Robotics and Automation (ICRA)},
title = {{Supersizing Self-supervision: Learning to Grasp from 50K Tries and 700 Robot Hours}},
year = {2016}
}
@inproceedings{schulman2013warping,
author = {Schulman, John and Gupta, Ankush and Venkatesan, Sibi and Tayson-Frederick, Mallory and Abbeel, Pieter},
booktitle = {IROS},
title = {{A Case Study of Trajectory Transfer Through Non-Rigid Registration for a Simplified Suturing Scenario}},
year = {2013}
}
@article{mericcli2015push,
author = {Meri{\c{c}}li, Tekin and Veloso, Manuela and Ak$\backslash$in, H Levent},
journal = {Autonomous Robots},
number = {3},
pages = {317--329},
publisher = {Springer},
title = {{Push-manipulation of complex passive mobile objects using experimentally acquired motion models}},
volume = {38},
year = {2015}
}
@inproceedings{kietzmann2009neuro,
author = {Kietzmann, Tim C and Riedmiller, Martin},
booktitle = {Machine Learning and Applications, 2009. ICMLA'09. International Conference on},
organization = {IEEE},
pages = {311--316},
title = {{The neuro slot car racer: Reinforcement learning in a real world setting}},
year = {2009}
}
@inproceedings{chui2000tpsrpm,
author = {Chui, Haili and Rangarajan, Anand},
booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {{A new algorithm for non-rigid point matching}},
year = {2000}
}
@inproceedings{todorov2003unsupervised,
author = {Todorov, Emanuel and Ghahramani, Zoubin},
booktitle = {Engineering in Medicine and Biology Society, 2003. Proceedings of the 25th Annual International Conference of the IEEE},
organization = {IEEE},
pages = {1750--1753},
title = {{Unsupervised learning of sensory-motor primitives}},
volume = {2},
year = {2003}
}
@article{zhang2018unreasonable,
author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A and Shechtman, Eli and Wang, Oliver},
journal = {arXiv preprint arXiv:1801.03924},
title = {{The Unreasonable Effectiveness of Deep Features as a Perceptual Metric}},
year = {2018}
}
@article{pinto2017asymmetric,
author = {Pinto, Lerrel and Andrychowicz, Marcin and Welinder, Peter and Zaremba, Wojciech and Abbeel, Pieter},
journal = {arXiv preprint arXiv:1710.06542},
title = {{Asymmetric Actor Critic for Image-Based Robot Learning}},
year = {2017}
}
@article{desjardins2012disentangling,
author = {Desjardins, Guillaume and Courville, Aaron and Bengio, Yoshua},
journal = {CoRR},
title = {{Disentangling factors of variation via generative entangling}},
volume = {abs/1210.5},
year = {2012}
}
@book{crowell2012introduction,
author = {Crowell, Richard H and Fox, Ralph Hartzler},
publisher = {Springer Science {\&} Business Media},
title = {{Introduction to knot theory}},
volume = {57},
year = {2012}
}
@article{lerer2016learning,
author = {Lerer, Adam and Gross, Sam and Fergus, Rob},
journal = {International Conference on Machine Learning (ICML)},
title = {{Learning Physical Intuition of Block Towers by Example}},
year = {2016}
}
@article{cheung2014discovering,
author = {Cheung, Brian and Livezey, Jesse A and Bansal, Arjun K and Olshausen, Bruno A},
journal = {arXiv preprint arXiv:1412.6583},
title = {{Discovering hidden factors of variation in deep networks}},
year = {2014}
}
@inproceedings{giusti15trails,
author = {Giusti, Alessandro and Guzzi, J{\'{e}}r{\^{o}}me and Cirean, Dan C and He, Fang-Lin and Rodr{\'{i}}guez, Juan P and Fontana, Flavio and Faessler, Matthias and Forster, Christian and Schmidhuber, J{\"{u}}rgen and Caro, Gianni Di and Scaramuzza, Davide and Gambardella, Luca M},
booktitle = {IEEE Robotics and Automation Letters.},
pages = {2377--3766},
title = {{A Machine Learning Approach to Visual Perception of Forest Trails for Mobile Robots}},
year = {2015}
}
@inproceedings{schulman2013generalization,
author = {Schulman, John and Ho, Jonathan and Lee, Cameron and Abbeel, Pieter},
booktitle = {Proceedings of the 16th International Symposium on Robotics Research (ISRR)},
title = {{Generalization in robotic manipulation through the use of non-rigid registration}},
year = {2013}
}
@article{wolpert1995internal,
author = {Wolpert, Daniel M and Ghahramani, Zoubin and Jordan, Michael I},
journal = {Science-AAAS-Weekly Paper Edition},
number = {5232},
pages = {1880--1882},
publisher = {New York, NY:[sn] 1880-},
title = {{An internal model for sensorimotor integration}},
volume = {269},
year = {1995}
}
@inproceedings{krizhevsky2012imagenet,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
pages = {1097--1105},
title = {{Imagenet classification with deep convolutional neural networks}},
year = {2012}
}
@article{huh2016makes,
author = {Huh, Minyoung and Agrawal, Pulkit and Efros, Alexei A},
journal = {arXiv preprint arXiv:1608.08614},
title = {{What makes ImageNet good for transfer learning?}},
year = {2016}
}
@article{dogar2012planning,
author = {Dogar, Mehmet R and Srinivasa, Siddhartha S},
journal = {Autonomous Robots},
number = {3},
pages = {217--236},
publisher = {Springer},
title = {{A planning framework for non-prehensile manipulation under clutter and uncertainty}},
volume = {33},
year = {2012}
}
@article{levine2017grasping,
author = {Levine, Sergey and Pastor, Peter and Krizhevsky, Alex and Quillen, Deirdre},
journal = {International Journal of Robotics Research},
title = {{Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection}},
year = {2017}
}
@article{higgins2017darla,
author = {Higgins, Irina and Pal, Arka and Rusu, Andrei A and Matthey, Loic and Burgess, Christopher P and Pritzel, Alexander and Botvinick, Matthew and Blundell, Charles and Lerchner, Alexander},
journal = {International Conference on Machine Learning (ICML)},
title = {{Darla: Improving zero-shot transfer in reinforcement learning}},
year = {2017}
}
@article{fujimoto2018td3,
author = {Fujimoto, Scott and van Hoof, Herke and Meger, David},
journal = {arXiv preprint arXiv:1802.09477},
title = {{Addressing Function Approximation Error in Actor-Critic Methods}},
year = {2018}
}
@article{mayer2008system,
author = {Mayer, Hermann and Gomez, Faustino and Wierstra, Daan and Nagy, Istvan and Knoll, Alois and Schmidhuber, J{\"{u}}rgen},
journal = {Advanced Robotics},
number = {13-14},
pages = {1521--1537},
publisher = {Taylor {\&} Francis},
title = {{A system for robotic heart surgery that learns to tie knots using recurrent neural networks}},
volume = {22},
year = {2008}
}
@inproceedings{hamrick2011internal,
author = {Hamrick, Jessica and Battaglia, Peter and Tenenbaum, Joshua B},
booktitle = {Proceedings of the 33rd annual conference of the cognitive science society},
organization = {Cognitive Science Society Austin, TX},
pages = {1545--1550},
title = {{Internal physics models guide probabilistic judgments about object dynamics}},
year = {2011}
}
@article{mayne2014model,
abstract = {Abstract This paper recalls a few past achievements in Model Predictive Control, gives an overview of some current developments and suggests a few avenues for future research. },
author = {Mayne, David Q},
doi = {http://dx.doi.org/10.1016/j.automatica.2014.10.128},
issn = {0005-1098},
journal = {Automatica},
keywords = {Model predictive control},
number = {12},
pages = {2967--2986},
title = {{Model predictive control: Recent developments and future promise}},
url = {http://www.sciencedirect.com/science/article/pii/S0005109814005160},
volume = {50},
year = {2014}
}
@article{mnih2015human,
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and Others},
journal = {Nature},
number = {7540},
pages = {529--533},
publisher = {Nature Publishing Group},
title = {{Human-level control through deep reinforcement learning}},
volume = {518},
year = {2015}
}
@article{mottaghi2015newtonian,
author = {Mottaghi, Roozbeh and Bagherinezhad, Hessam and Rastegari, Mohammad and Farhadi, Ali},
journal = {arXiv preprint arXiv:1511.04048},
title = {{Newtonian Image Understanding: Unfolding the Dynamics of Objects in Static Images}},
year = {2015}
}
@inproceedings{stilman2007manipulation,
author = {Stilman, Mike and Schamburek, Jan-Ullrich and Kuffner, James and Asfour, Tamim},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
organization = {IEEE},
pages = {3327--3332},
title = {{Manipulation planning among movable obstacles}},
year = {2007}
}
@article{higgins2016beta,
author = {Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
journal = {International Conference on Learning Representations (ICLR)},
title = {{$\beta$-VAE: Learning basic visual concepts with a constrained variational framework}},
year = {2017}
}
@article{haruno2001mosaic,
author = {Haruno, Masahiko and Wolpert, David H and Kawato, Mitsuo},
journal = {Neural computation},
number = {10},
pages = {2201--2220},
publisher = {MIT Press},
title = {{Mosaic model for sensorimotor learning and control}},
volume = {13},
year = {2001}
}
@inproceedings{lenz2015deepMPC,
author = {Lenz, Ian and Knepper, Ross and Saxena, Ashutosh},
booktitle = {Robotics: Science and Systems (RSS)},
title = {{DeepMPC: Learning Deep Latent Features for Model Predictive Control}},
year = {2015}
}
@article{hopcroft1991case,
author = {Hopcroft, John E and Kearney, Joseph K and Krafft, Dean B},
journal = {The International Journal of Robotics Research},
number = {1},
pages = {41--50},
publisher = {Sage Publications},
title = {{A case study of flexible object manipulation}},
volume = {10},
year = {1991}
}
@article{rusu2016sim,
author = {Rusu, Andrei A and Vecerik, Matej and Roth{\"{o}}rl, Thomas and Heess, Nicolas and Pascanu, Razvan and Hadsell, Raia},
journal = {Conference on Robot Learning (CoRL)},
title = {{Sim-to-real robot learning from pixels with progressive nets}},
year = {2017}
}
@techreport{winograd1971procedures,
author = {Winograd, Terry},
institution = {DTIC Document},
title = {{Procedures as a representation for data in a computer program for understanding natural language}},
year = {1971}
}
@inproceedings{5459407,
author = {Brubaker, Marcus A and Sigal, L and Fleet, D J},
booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/ICCV.2009.5459407},
issn = {1550-5499},
keywords = {Biological system modeling,Geometry,Gravity,Humans},
month = {sep},
pages = {2389--2396},
title = {{Estimating contact dynamics}},
year = {2009}
}
@article{eysenbach2018diayn,
author = {Eysenbach, Benjamin and Gupta, Abhishek and Ibarz, Julian and Levine, Sergey},
journal = {arXiv preprint arXiv:1802.06070},
title = {{Diversity is All You Need: Learning Skills without a Reward Function}},
year = {2018}
}
@inproceedings{agarwal1997nonholonomic,
author = {Agarwal, Pankaj K and Latombe, Jean-Claude and Motwani, Rajeev and Raghavan, Prabhakar},
booktitle = {IEEE International Conference on Robotics and Automation},
organization = {Citeseer},
pages = {3124--3129},
title = {{Nonholonomic path planning for pushing a disk among obstacles}},
year = {1997}
}
@inproceedings{conf/eccv/BhatSP02,
author = {Bhat, Kiran S and Seitz, Steven M and Popovic, Jovan},
booktitle = {European Conference on Computer Vision (ECCV)},
editor = {Heyden, Anders and Sparr, Gunnar and Nielsen, Mads and Johansen, Peter},
isbn = {3-540-43745-2},
keywords = {dblp},
pages = {551--565},
publisher = {Springer},
series = {Lecture Notes in Computer Science},
title = {{Computing the Physical Parameters of Rigid-Body Motion from Video.}},
url = {http://dblp.uni-trier.de/db/conf/eccv/eccv2002-1.html{\#}BhatSP02},
volume = {2350},
year = {2002}
}
@article{wahlstrom2015from,
author = {Wahlstr{\"{o}}m, Niklas and Sch{\"{o}}n, Thomas B and Deisenroth, Marc Peter},
journal = {CoRR},
title = {{From Pixels to Torques: Policy Learning with Deep Dynamical Models}},
volume = {abs/1502.0},
year = {2015}
}
@inproceedings{wu2015galileo,
author = {Wu, Jiajun and Yildirim, Ilker and Lim, Joseph J and Freeman, Bill and Tenenbaum, Josh},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
pages = {127--135},
title = {{Galileo: Perceiving Physical Object Properties by Integrating a Physics Engine with Deep Learning}},
year = {2015}
}

@article{argall2009survey,
author = {Argall, Brenna D and Chernova, Sonia and Veloso, Manuela and Browning, Brett},
journal = {Robotics and autonomous systems},
number = {5},
pages = {469--483},
publisher = {Elsevier},
title = {{A survey of robot learning from demonstration}},
volume = {57},
year = {2009}
}

@article{shelhamer2016loss,
author = {Shelhamer, Evan and Mahmoudieh, Parsa and Argus, Max and Darrell, Trevor},
journal = {arXiv preprint arXiv:1612.07307},
title = {{Loss is its own reward: Self-supervision for reinforcement learning}},
year = {2016}
}
@inproceedings{yamakawa2007one,
author = {Yamakawa, Yuji and Namiki, Akio and Ishikawa, Masatoshi and Shimojo, Makoto},
booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
organization = {IEEE},
pages = {703--708},
title = {{One-handed knotting of a flexible rope with a high-speed multifingered hand having tactile sensors}},
year = {2007}
}
@article{maaten2008visualizing,
author = {van der Maaten, Laurens and Hinton, Geoffrey},
journal = {Journal of Machine Learning Research},
number = {Nov},
pages = {2579--2605},
title = {{Visualizing data using t-SNE}},
volume = {9},
year = {2008}
}
@phdthesis{bell2010flexible,
author = {Bell, Matthew},
school = {Dartmouth College, Hanover, New Hampshire},
title = {{Flexible object manipulation}},
year = {2010}
}
@inproceedings{bellemare2016unifying,
author = {Bellemare, Marc and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, Remi},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
pages = {1471--1479},
title = {{Unifying count-based exploration and intrinsic motivation}},
year = {2016}
}
@inproceedings{maitin2010cloth,
author = {Maitin-Shepard, Jeremy and Cusumano-Towner, Marco and Lei, Jinna and Abbeel, Pieter},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
organization = {IEEE},
pages = {2308--2315},
title = {{Cloth grasp point detection based on multiple-view geometric cues with application to robotic towel folding}},
year = {2010}
}
@inproceedings{miller2011parametrized,
author = {Miller, Stephen and Fritz, Mario and Darrell, Trevor and Abbeel, Pieter},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
organization = {IEEE},
pages = {4861--4868},
title = {{Parametrized shape models for clothing}},
year = {2011}
}
@article{sutton2011horde,
abstract = {Maintaining accurate world knowledge in a complex and changing environment is a perennial problem for robots and other artificial intelligence systems. Our architecture for addressing this problem, called Horde, consists of a large number of independent reinforcement learning sub-agents, or demons. Each demon is responsible for answering a sin-gle predictive or goal-oriented question about the world, thereby contributing in a factored, modular way to the sys-tem's overall knowledge. The questions are in the form of a value function, but each demon has its own policy, re-ward function, termination function, and terminal-reward function unrelated to those of the base problem. Learning proceeds in parallel by all demons simultaneously so as to extract the maximal training information from whatever ac-tions are taken by the system as a whole. Gradient-based temporal-difference learning methods are used to learn ef-ficiently and reliably with function approximation in this off-policy setting. Horde runs in constant time and memory per time step, and is thus suitable for learning online in real-time applications such as robotics. We present results using Horde on a multi-sensored mobile robot to successfully learn goal-oriented behaviors and long-term predictions from off-policy experience. Horde is a significant incremental step towards a real-time architecture for efficient learning of gen-eral knowledge from unsupervised sensorimotor interaction.},
author = {Sutton, Richard S and Modayil, Joseph and Delp, Michael and Degris, Thomas and Pilarski, Patrick M and White, Adam and Precup, Doina},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Sutton et al. - Unknown - Horde A Scalable Real-time Architecture for Learning Knowledge from Unsupervised Sensorimotor Interaction.pdf:pdf},
journal = {International Conference on Autonomous Agents and Multiagent Systems (AAMAS)},
keywords = {Categories and,Descriptors,Subject},
pages = {761--768},
title = {{Horde: A Scalable Real-time Architecture for Learning Knowledge from Unsupervised Sensorimotor Interaction}},
url = {https://www.cs.swarthmore.edu/{~}meeden/DevelopmentalRobotics/horde1.pdf},
volume = {10},
year = {2011}
}
@inproceedings{ziebart2008maxent,
abstract = {Recent research has shown the benefit of framing problems of imitation learning as solutions to Markov Decision Problems. This approach reduces learning to the problem of recovering a utility function that makes the behavior induced by a near-optimal policy closely mimic demonstrated behavior. In this work, we develop a probabilistic approach based on the principle of maximum entropy. Our approach provides a well-defined, globally normalized distribution over decision sequences, while providing the same performance guarantees as existing methods. We develop our technique in the context of modeling real-world navigation and driving behaviors where collected data is inherently noisy and imperfect. Our probabilistic approach enables modeling of route preferences as well as a powerful new approach to inferring destinations and routes based on partial trajectories. Copyright {\textcopyright} 2008.},
archivePrefix = {arXiv},
arxivId = {arXiv:1507.04888v2},
author = {Ziebart, Brian D and Maas, Andrew and Bagnell, J Andrew and Dey, Anind K},
booktitle = {AAAI Conference on Artificial Intelligence},
eprint = {arXiv:1507.04888v2},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Ziebart et al. - Unknown - Maximum Entropy Inverse Reinforcement Learning.pdf:pdf},
isbn = {9781577353683 (ISBN)},
issn = {10450823},
keywords = {Artificial intelligence,Bionics,Driving behaviors,Existing methods,Imitation learnings,Inverse problems,Maximum entropies,New approaches,Optimal policies,Partial trajectories,Performance guarantees,Principle of Maximum entropies,Probabilistic approaches,Probability,Probability distributions,Reinforcement,Reinforcement learning,Route preferences,Utility functions},
pages = {1433--1438},
title = {{Maximum Entropy Inverse Reinforcement Learning.}},
url = {https://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf http://www.scopus.com/inward/record.url?eid=2-s2.0-57749097473{\&}partnerID=40{\%}5Cnhttp://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf},
year = {2008}
}
@article{Gu2016b,
abstract = {Reinforcement learning holds the promise of enabling autonomous robots to learn large repertoires of behavioral skills with minimal human intervention. However, robotic applications of reinforcement learning often compromise the autonomy of the learning process in favor of achieving training times that are practical for real physical systems. This typically involves introducing hand-engineered policy representations and human-supplied demonstrations. Deep reinforcement learning alleviates this limitation by training general-purpose neural network policies, but applications of direct deep reinforcement learning algorithms have so far been restricted to simulated settings and relatively simple tasks, due to their apparent high sample complexity. In this paper, we demonstrate that a recent deep reinforcement learning algorithm based on off-policy training of deep Q-functions can scale to complex 3D manipulation tasks and can learn deep neural network policies efficiently enough to train on real physical robots. We demonstrate that the training times can be further reduced by parallelizing the algorithm across multiple robots which pool their policy updates asynchronously. Our experimental evaluation shows that our method can learn a variety of 3D manipulation skills in simulation and a complex door opening skill on real robots without any prior demonstrations or manually designed representations.},
archivePrefix = {arXiv},
arxivId = {1610.00633},
author = {Gu, Shixiang and Holly, Ethan and Lillicrap, Timothy and Levine, Sergey},
doi = {10.1038/nature20101},
eprint = {1610.00633},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Gu et al. - Unknown - Deep Reinforcement Learning for Robotic Manipulation with Asynchronous Off-Policy Updates.pdf:pdf},
isbn = {0896-6273},
issn = {0028-0836},
journal = {IEEE International Conference on Robotics and Automation (ICRA)},
pmid = {26774160},
title = {{Deep Reinforcement Learning for Robotic Manipulation with Asynchronous Off-Policy Updates}},
url = {https://arxiv.org/pdf/1610.00633.pdf http://arxiv.org/abs/1610.00633},
year = {2017}
}
@article{Deisenroth2011,
abstract = {â€”Over the last years, there has been substantial progress in robust manipulation in unstructured environments. The long-term goal of our work is to get away from precise, but very expensive robotic systems and to develop affordable, potentially imprecise, self-adaptive manipulator systems that can interactively perform tasks such as playing with children. In this paper, we demonstrate how a low-cost off-the-shelf robotic system can learn closed-loop policies for a stacking task in only a handful of trialsâ€”from scratch. Our manipulator is inaccurate and provides no pose feedback. For learning a controller in the work space of a Kinect-style depth camera, we use a model-based reinforcement learning technique. Our learning method is data efficient, reduces model bias, and deals with several noise sources in a principled way during long-term planning. We present a way of incorporating state-space constraints into the learning process and analyze the learning gain by exploiting the sequential structure of the stacking task.},
author = {Deisenroth, Marc Peter and Rasmussen, Carl Edward and Fox, Dieter},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Deisenroth, Rasmussen, Fox - Unknown - Learning to Control a Low-Cost Manipulator using Data-Efficient Reinforcement Learning.pdf:pdf},
journal = {Robotics: Science and Systems (RSS)},
pages = {57--64},
title = {{Learning to Control a Low-Cost Manipulator using Data-Efficient Reinforcement Learning}},
url = {http://www.roboticsproceedings.org/rss07/p08.pdf},
volume = {VII},
year = {2011}
}
@inproceedings{ross2011dagger,
abstract = {Sequential prediction problems such as imitation learning, where future observations depend on previous predictions (actions), violate the com-mon i.i.d. assumptions made in statistical learn-ing. This leads to poor performance in theory and often in practice. Some recent approaches (Daum{\'{e}} III et al., 2009; Ross and Bagnell, 2010) provide stronger guarantees in this setting, but re-main somewhat unsatisfactory as they train either non-stationary or stochastic policies and require a large number of iterations. In this paper, we propose a new iterative algorithm, which trains a stationary deterministic policy, that can be seen as a no regret algorithm in an online learning set-ting. We show that any such no regret algorithm, combined with additional reduction assumptions, must find a policy with good performance under the distribution of observations it induces in such sequential settings. We demonstrate that this new approach outperforms previous approaches on two challenging imitation learning problems and a benchmark sequence labeling problem.},
author = {Ross, St{\'{e}}phane and Gordon, Geoffrey J and Bagnell, J Andrew},
booktitle = {International Conference on Artificial Intelligence and Statistics (AISTATS)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Ross, Gordon, Bagnell - Unknown - A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning(2).pdf:pdf;:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Ross, Gordon, Bagnell - Unknown - A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning.pdf:pdf},
title = {{A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning}},
url = {https://arxiv.org/pdf/1011.0686.pdf},
year = {2011}
}
@article{Kaelbling2011,
abstract = {â€” In this paper we outline an approach to the integration of task planning and motion planning that has the following key properties: It is aggressively hierarchical. It makes choices and commits to them in a top-down fashion in an attempt to limit the length of plans that need to be constructed, and thereby exponentially decrease the amount of search required. Importantly, our approach also limits the need to project the effect of actions into the far future. It operates on detailed, continuous geometric representations and partial symbolic descriptions. It does not require a complete symbolic representation of the input geometry or of the geometric effect of the task-level operations.},
author = {Kaelbling, Leslie Pack and Lozano-Perez, Tomas},
doi = {10.1109/ICRA.2011.5980391},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Kaelbling, Lozano-Perez - 2011 - Hierarchical task and motion planning in the now.pdf:pdf},
isbn = {9781612843865},
issn = {10504729},
journal = {IEEE International Conference on Robotics and Automation},
pages = {1470--1477},
title = {{Hierarchical task and motion planning in the now}},
url = {http://people.csail.mit.edu/lpk/papers/hpn2.pdf http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5980391},
year = {2011}
}
@inproceedings{todorov12mujoco,
abstract = {We describe a new physics engine tailored to model-based control. Multi-joint dynamics are represented in generalized coordinates and computed via recursive algorithms. Contact responses are computed via efficient new algorithms we have developed, based on the modern velocity-stepping approach which avoids the difficulties with spring-dampers. Models are specified using either a high-level C++ API or an intuitive XML file format. A built-in compiler transforms the user model into an optimized data structure used for runtime computation. The engine can compute both forward and inverse dynamics. The latter are well-definedevenin the presence of contacts and equality constraints. Themodel can include tendon wrapping as well as actuator activation states (e.g. pneumatic cylinders or muscles). To facilitate optimal control applications and in particular sampling and finite differencing, the dynamics can be evaluated for different states and controls in parallel. Around 400,000 dynamics evaluations per second are possible on a 12-core machine, for a 3D homanoid with 18 dofs and 6 active contacts. We have already used the engine in a number of control applications. It will soon be made publicly available.},
author = {Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2012.6386109},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Todorov, Erez, Tassa - Unknown - MuJoCo A physics engine for model-based control.pdf:pdf},
isbn = {9781467317375},
issn = {21530858},
pages = {5026--5033},
title = {{MuJoCo: A physics engine for model-based control}},
url = {https://homes.cs.washington.edu/{~}todorov/papers/TodorovIROS12.pdf},
year = {2012}
}
@inproceedings{kim2013apid,
abstract = {We propose a Learning from Demonstration (LfD) algorithm which leverages ex-pert data, even if they are very few or inaccurate. We achieve this by using both expert data, as well as reinforcement signals gathered through trial-and-error inter-actions with the environment. The key idea of our approach, Approximate Policy Iteration with Demonstration (APID), is that expert's suggestions are used to de-fine linear constraints which guide the optimization performed by Approximate Policy Iteration. We prove an upper bound on the Bellman error of the estimate computed by APID at each iteration. Moreover, we show empirically that APID outperforms pure Approximate Policy Iteration, a state-of-the-art LfD algorithm, and supervised learning in a variety of scenarios, including when very few and/or suboptimal demonstrations are available. Our experiments include simulations as well as a real robot path-finding task.},
author = {Kim, Beomjoon and Farahmand, Amir-Massoud and Pineau, Joelle and Precup, Doina},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Kim et al. - Unknown - Learning from Limited Demonstrations.pdf:pdf},
title = {{Learning from Limited Demonstrations}},
url = {https://papers.nips.cc/paper/4918-learning-from-limited-demonstrations.pdf},
year = {2013}
}
@inproceedings{schaul2015uva,
abstract = {Value functions are a core component of rein- forcement learning systems. The main idea is to to construct a single function approximator V (s; $\theta$) that estimates the long-term reward from any state s, using parameters $\theta$. In this paper we introduce universal value function approx- imators (UVFAs) V (s, g; $\theta$) that generalise not just over states s but also over goals g. We de- velop an efficient technique for supervised learn- ing of UVFAs, by factoring observed values into separate embedding vectors for state and goal, and then learning a mapping from s and g to these factored embedding vectors. We show how this technique may be incorporated into a re- inforcement learning algorithm that updates the UVFAsolely from observed rewards. Finally, we demonstrate that a UVFAcan successfully gener- alise to previously unseen goals. 1.},
author = {Schaul, Tom and Horgan, Daniel and Gregor, Karol and Silver, David},
booktitle = {International Conference on Machine Learning (ICML)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Schaul et al. - 2015 - Universal Value Function Approximators.pdf:pdf},
isbn = {9781510810587},
pages = {1312--1320},
title = {{Universal Value Function Approximators}},
url = {http://proceedings.mlr.press/v37/schaul15.pdf http://jmlr.org/proceedings/papers/v37/schaul15.html},
year = {2015}
}
@inproceedings{finn2016deep,
abstract = {Reinforcement learning provides a powerful and flexible framework for automated acquisition of robotic motion skills. However, applying reinforcement learning requires a sufficiently detailed representation of the state, including the configuration of task-relevant objects. We present an approach that automates state-space construction by learning a state representation directly from camera images. Our method uses a deep spatial autoencoder to acquire a set of feature points that describe the environment for the current task, such as the positions of objects, and then learns a motion skill with these feature points using an efficient reinforcement learning method based on local linear models. The resulting controller reacts continuously to the learned feature points, allowing the robot to dynamically manipulate objects in the world with closed-loop control. We demonstrate our method with a PR2 robot on tasks that include pushing a free-standing toy block, picking up a bag of rice using a spatula, and hanging a loop of rope on a hook at various positions. In each task, our method automatically learns to track task-relevant objects and manipulate their configuration with the robot's arm.},
archivePrefix = {arXiv},
arxivId = {1509.06113},
author = {Finn, Chelsea and Tan, Xin Yu and Duan, Yan and Darrell, Trevor and Levine, Sergey and Abbeel, Pieter},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2016.7487173},
eprint = {1509.06113},
file = {:Users/ashvin/code/kindlize/pdfs/deeprl/spatial-autoencoders-Finn15.pdf:pdf},
isbn = {9781467380263},
issn = {10504729},
organization = {IEEE},
pages = {512--519},
title = {{Deep spatial autoencoders for visuomotor learning}},
volume = {2016-June},
year = {2016}
}
@inproceedings{schulman2015trpo,
abstract = {We describe a iterative procedure for optimizing policies, with guaranteed monotonic improve-ment. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effec-tive for optimizing large nonlinear policies such as neural networks. Our experiments demon-strate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. De-spite its approximations that deviate from the theory, TRPO tends to give monotonic improve-ment, with little tuning of hyperparameters.},
archivePrefix = {arXiv},
arxivId = {1502.05477},
author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
booktitle = {International Conference on Machine Learning (ICML)},
doi = {10.1063/1.4927398},
eprint = {1502.05477},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Schulman et al. - Unknown - Trust Region Policy Optimization.pdf:pdf;:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Schulman et al. - 2015 - Trust Region Policy Optimization.pdf:pdf},
isbn = {0375-9687},
issn = {2158-3226},
title = {{Trust Region Policy Optimization}},
url = {https://arxiv.org/pdf/1502.05477.pdf http://arxiv.org/abs/1502.05477},
year = {2015}
}
@inproceedings{kolev2015physically,
abstract = {â€” Successful model based control relies heavily on proper system identification and accurate state estimation. We present a framework for solving these problems in the context of robotic control applications. We are particularly interested in robotic manipulation tasks, which are especially hard due to the non-linear nature of contact phenomena. We developed a solution that solves both the problems of estimation and system identification jointly. We show that these two problems are difficult to solve separately in the presence of discontinuous phenomena such as contacts. The problem is posed as a joint optimization across both trajectory and model parameters and solved via Newton's method. We present several challenges we encountered while modeling contacts and performing state estimation and propose solutions within the MuJoCo physics engine. We present experimental results performed on our manip-ulation system consisting of 3-DOF Phantom Haptic Devices, turned into finger manipulators. Cross-validation between dif-ferent datasets, as well as leave-one-out cross-validation show that our method is robust and is able to accurately explain sensory data.},
author = {Kolev, Svetoslav and Todorov, Emanuel},
booktitle = {IEEE-RAS 15th International Conference on Humanoid Robots (Humanoids)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Kolev, Todorov - Unknown - Physically consistent state estimation and system identification for contacts.pdf:pdf},
organization = {IEEE},
pages = {1036--1043},
title = {{Physically consistent state estimation and system identification for contacts}},
url = {http://homes.cs.washington.edu/{~}todorov/papers/KolevHumanoids15.pdf},
year = {2015}
}
@inproceedings{peters2010reps,
abstract = {Policy search is a successful approach to reinforcement learning. However, policy improvements often result in the loss of information. Hence, it has been marred by premature convergence and implausible solutions. As first suggested in the context of covariant policy gradients (Bagnell and Schneider 2003), many of these problems may be addressed by constraining the infor- mation loss. In this paper, we continue this path of rea- soning and suggest the Relative Entropy Policy Search (REPS) method. The resulting method differs signif- icantly from previous policy gradient approaches and yields an exact update step. It works well on typical reinforcement learning benchmark problems. Introduction},
author = {Peters, Jan and M{\"{u}}lling, Katharina and Alt{\"{u}}n, Yasemin},
booktitle = {AAAI Conference on Artificial Intelligence},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Peters, M{\"{u}}lling, Alt{\"{u}}n - 2010 - Relative Entropy Policy Search.pdf:pdf},
pages = {1607--1612},
title = {{Relative Entropy Policy Search}},
url = {https://pdfs.semanticscholar.org/ff47/526838ce85d77a50197a0c5f6ee5095156aa.pdf http://www-clmc.usc.edu/publications/P/Peters{\_}POTTNCOAIPGAT{\_}2010.pdf},
year = {2010}
}
@inproceedings{ho2016gail,
abstract = {Consider learning a policy from example expert behavior, without interaction with the expert or access to reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data, as if it were obtained by reinforcement learning following inverse reinforcement learning. We show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains signif-icant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environments.},
author = {Ho, Jonathan and Ermon, Stefano},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Ho, Ermon - Unknown - Generative Adversarial Imitation Learning(4).pdf:pdf},
title = {{Generative Adversarial Imitation Learning}},
url = {https://arxiv.org/pdf/1606.03476.pdf},
year = {2016}
}
@inproceedings{chen2016infogan,
abstract = {This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods.},
archivePrefix = {arXiv},
arxivId = {1606.03657},
author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
eprint = {1606.03657},
file = {:Users/ashvin/code/kindlize/pdfs/deeprl/infogan-Chen16.pdf:pdf},
pages = {2172--2180},
title = {{Infogan: Interpretable representation learning by information maximizing generative adversarial nets}},
url = {http://arxiv.org/abs/1606.03657},
year = {2016}
}
@inproceedings{thomas2017independentlycontrollable,
abstract = {It has been postulated that a good representation is one that disentangles the underlying explanatory factors of variation. However, it remains an open question what kind of training framework could potentially achieve that. Whereas most previous work focuses on the static setting (e.g., with images), we postulate that some of the causal factors could be discovered if the learner is allowed to interact with its environment. The agent can experiment with different actions and observe their effects. More specifically, we hypothesize that some of these factors correspond to aspects of the environment which are independently controllable, i.e., that there exists a policy and a learnable feature for each such aspect of the environment, such that this policy can yield changes in that feature with minimal changes to other features that explain the statistical variations in the observed data. We propose a specific objective function to find such factors and verify experimentally that it can indeed disentangle independently controllable aspects of the environment without any extrinsic reward signal.},
author = {Thomas, Valentin and Pondard, Jules and Bengio, Emmanuel and Sarfati, Marc and Beaudoin, Philippe and Meurs, Marie-Jean and Pineau, Joelle and Precup, Doina and Bengio, Yoshua},
booktitle = {NIPS Workshop},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Thomas et al. - 2017 - Independently Controllable Factors.pdf:pdf},
title = {{Independently Controllable Factors}},
url = {https://arxiv.org/pdf/1708.01289.pdf},
year = {2017}
}
@article{Silver2016,
abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses â€˜value networks' to evaluate board positions and â€˜policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8{\%} winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
archivePrefix = {arXiv},
arxivId = {1610.00633},
author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature16961},
eprint = {1610.00633},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Silver et al. - Unknown - Mastering the Game of Go with Deep Neural Networks and Tree Search.pdf:pdf},
isbn = {1476-4687 (Electronic)$\backslash$r0028-0836 (Linking)},
issn = {0028-0836},
journal = {Nature},
month = {jan},
number = {7587},
pages = {484--489},
pmid = {26819042},
publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
title = {{Mastering the game of Go with deep neural networks and tree search}},
url = {http://dx.doi.org/10.1038/nature16961 http://10.0.4.14/nature16961 http://www.nature.com/nature/journal/v529/n7587/abs/nature16961.html{\#}supplementary-information http://airesearch.com/wp-content/uploads/2016/01/deepmind-mastering-go.pdf},
volume = {529},
year = {2016}
}
@inproceedings{nakanishi2004bipedlfd,
abstract = {In this paper, we introduce a framework for learning biped locomotion using dynamical movement primitives based on non-linear oscillators. Our ultimate goal is to establish a design principle of a controller in order to achieve natural human-like locomotion. We suggest dynamical movement primitives as a central pattern generator (CPG) of a biped robot, an approach we have previously proposed for learning and encoding complex human movements. Demonstrated trajectories are learned through movement primitives by locally weighted regression, and the frequency of the learned trajectories is adjusted automatically by a novel frequency adaptation algorithm based on phase resetting and entrainment of coupled oscillators. Numerical simulations and experimental implementation on a physical robot demonstrate the effectiveness of the proposed locomotion controller. {\textcopyright} 2004 Elsevier B.V. All rights reserved.},
author = {Nakanishi, Jun and Morimoto, Jun and Endo, Gen and Cheng, Gordon and Schaal, Stefan and Kawato, Mitsuo},
booktitle = {Robotics and Autonomous Systems},
doi = {10.1016/j.robot.2004.03.003},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Nakanishi et al. - 2004 - Learning from demonstration and adaptation of biped locomotion.pdf:pdf},
isbn = {0921-8890},
issn = {09218890},
keywords = {Biped locomotion,Dynamical movement primitives,Frequency adaptation,Learning from demonstration,Phase resetting},
number = {2-3},
pages = {79--91},
title = {{Learning from demonstration and adaptation of biped locomotion}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.77.534{\&}rep=rep1{\&}type=pdf},
volume = {47},
year = {2004}
}
@inproceedings{schaal2001augmentation,
abstract = {Real-time modeling of complex nonlinear dynamic processes has become increasingly impor-tant in various areas of robotics and human aug-mentation. To address such problems, we have been developing special statistical learning methods that meet the demands of on-line learning, in particular the need for low computational complexity, rapid learning, and scalability to high-dimensional spaces. In this paper, we introduce a novel algorithm that possesses all the necessary properties by combining methods from probabilistic and nonparametric learning. We demonstrate the applicability of our methods for three different applications in humanoid robotics, i.e., the on-line learning of a full-body in-verse dynamics model, an inverse kinematics model, and imitation learning. The latter application will also introduce a novel method to shape attractor landscapes of dynamical system by means of statis-tical learning.},
author = {Schaal, Stefan and Vijayakumar, Sethu and Souza, Aaron D ' and Ijspeert, Auke and Nakanishi, Jun},
booktitle = {International Symposium on Robotics Research},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Schaal et al. - Unknown - Real-Time Statistical Learning For Robotics and Human Augmentation.pdf:pdf},
title = {{Real-Time Statistical Learning For Robotics and Human Augmentation}},
url = {http://www-clmc.usc.edu}
}
@article{Kavraki1996,
author = {Kavraki, Lydia and Svestka, Petr and Latombe, J-C and Overmars, Mark},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Kavraki et al. - 1996 - Probabilistic roadmaps for path planning in high-dimensional configuration spaces.pdf:pdf},
journal = {IEEE Transactions on Robotics and Automation},
number = {4},
pages = {566--580},
title = {{Probabilistic roadmaps for path planning in high-dimensional configuration spaces}},
volume = {12},
year = {1996}
}
@inproceedings{pathak2017curiosity,
abstract = {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch. Demo video and code available at https://pathak22.github.io/noreward-rl/},
archivePrefix = {arXiv},
arxivId = {1705.05363},
author = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A. and Darrell, Trevor},
booktitle = {International Conference on Machine Learning (ICML)},
eprint = {1705.05363},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Pathak et al. - 2017 - Curiosity-driven Exploration by Self-supervised Prediction.pdf:pdf},
organization = {IEEE},
pages = {488--489},
title = {{Curiosity-Driven Exploration by Self-Supervised Prediction}},
year = {2017}
}
@article{smith2005development,
abstract = {The embodiment hypothesis is the idea that intelligence emerges in the interaction of an agent with an environment and as a result of sensorimotor activity. In this paper we offer six lessons for developing embodied intelligent agents suggested by research in developmental psychology. We argue that starting as a baby grounded in a physical, social and linguistic world is crucial to the development of the flexible and inventive intelligence that characterizes humankind.},
author = {Smith, Linda and Gasser, Michael},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Smith, Gasser - Unknown - The Development of Embodied Cognition Six Lessons from Babies.pdf:pdf},
journal = {Artificial life},
keywords = {cognition,development,embodiment,language,motor control},
number = {1-2},
pages = {13--29},
publisher = {MIT Press},
title = {{The development of embodied cognition: Six lessons from babies}},
url = {https://www.cogsci.msu.edu/DSS/2010-2011/Smith/6lessons.pdf},
volume = {11},
year = {2005}
}
@article{ng2000irl,
author = {Ng, Andrew and Russell, Stuart},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Ng, Russell - 2000 - Algorithms for Inverse Reinforcement Learning.pdf:pdf},
journal = {International Conference on Machine Learning (ICML)},
title = {{Algorithms for Inverse Reinforcement Learning}},
url = {http://ai.stanford.edu/{~}ang/papers/icml00-irl.pdf},
year = {2000}
}
@inproceedings{srivastava14tamp,
abstract = {â€” The need for combined task and motion planning in robotics is well understood. Solutions to this problem have typically relied on special purpose, integrated implementations of task planning and motion planning algorithms. We propose a new approach that uses off-the-shelf task planners and motion planners and makes no assumptions about their implementa-tion. Doing so enables our approach to directly build on, and benefit from, the vast literature and latest advances in task planning and motion planning. It uses a novel representational abstraction and requires only that failures in computing a mo-tion plan for a high-level action be identifiable and expressible in the form of logical predicates at the task level. We evaluate the approach and illustrate its robustness through a number of experiments using a state-of-the-art robotics simulator and a PR2 robot. These experiments show the system accomplishing a diverse set of challenging tasks such as taking advantage of a tray when laying out a table for dinner and picking objects from cluttered environments where other objects need to be re-arranged before the target object can be reached.},
author = {Srivastava, Siddharth and Fang, Eugene and Riano, Lorenzo and Chitnis, Rohan and Russell, Stuart and Abbeel, Pieter},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Srivastava et al. - Unknown - Combined Task and Motion Planning Through an Extensible Planner-Independent Interface Layer.pdf:pdf},
title = {{Combined Task and Motion Planning Through an Extensible Planner-Independent Interface Layer}},
url = {https://people.eecs.berkeley.edu/{~}russell/papers/icra14-planrob.pdf},
year = {2014}
}
@article{peters2003naturalac,
abstract = {Reinforcement learning offers a promising framework to take planning for real-world systems towards true autonomy and versatility. However, apply-ing reinforcement learning to high dimensional movement systems (such as real-world robots) in the presence of uncertainty and continuous state-action spaces remains an unsolved problem. In order to make progress towards solving this issue, we focus on a particular type of reinforcement learning methods, i.e., policy gradient methods. These methods are particularly in-teresting to the robotics community as they seem to scale better to continuous state-action problems and have been successfully applied on a variety of high-dimensional robots. However, the main disadvantages of these methods have been the high variance in the gradient estimate, the very slow convergence, and the dependence on baseline functions. In this poster, we show how these policy gradients can be improved in respect to each of these problems. Our approach to policy gradients focuses on the natural policy gradient instead of the regular policy gradient. Natural policy gradients for reinforce-ment learning have first been suggested by Kakade [2] as 'average natural policy gradients', and subsequently been shown to be the true natural policy gradient by Bagnell {\&} Schneider [1], and Peters et al. [3]. As shown by Kakade, natural policy gradients are particularly interesting due to the fact that they equal the parameters of the compatible function approximation. We present a general algorithm for estimating the natural gradient, the Nat-ural Actor-Critic algorithm. This algorithm uses the fact that the compatible function approximation represents an advantage function which can be em-1 bedded cleanly into the Bellman equation. It can be used in two different ways, i.e., in form of general temporal difference learning where reasonable basis functions for the value function are required in order to obtain an un-biased gradient, and in form of start-state reinforcement learning where the gradient is estimated using the property that the sum of all advantages along a roll-out has to equal the sum of rewards plus a single value function offset parameter. The later method is guaranteed to yield an unbiased estimate of the gradient and is well suited for learning and refining parameterized policies, even in the light of incomplete state information. We show two examples of the application of the Natural Actor-Critic algorithm, one where it by far outperforms non-natural policy gradients in the classical cart-pole balancing system, and one for learning nonlinear dy-namic motor primitives for humanoid robot control. From our experience, the start-state Natural Actor-Critic algorithm seems to be one of the most efficient model-free reinforcement learning techniques and offers a promising route for the development of reinforcement learning techniques for truly high dimensionally continuous state-action systems.},
author = {Peters, Jan and Vijayakumar, Sethu and Schaal, Stefan},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Peters, Vijayakumar, Schaal - 2003 - Natural Actor Critic.pdf:pdf},
journal = {Neurocomputing},
pages = {1180--1190},
title = {{Natural Actor Critic}},
url = {http://kyb.tuebingen.mpg.de/fileadmin/user{\_}upload/files/publications/attachments/NIPS-Workshop-2003-Peters{\_}{\%}5B0{\%}5D.pdf},
volume = {71},
year = {2008}
}
@inproceedings{boots2014traces,
author = {Boots, Byron and Byravan, Arunkumar and Fox, Dieter},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2014.6907443},
pages = {4021--4028},
title = {{Learning predictive models of a depth camera {\&} manipulator from raw execution traces}},
url = {http://dx.doi.org/10.1109/ICRA.2014.6907443},
year = {2014}
}
@inproceedings{florensa2017resets,
archivePrefix = {arXiv},
arxivId = {1707.05300},
author = {Florensa, Carlos and Held, David and Wulfmeier, Markus and Abbeel, Pieter},
booktitle = {International Conference on Learning Representations (ICLR)},
eprint = {1707.05300},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Florensa et al. - 2017 - Reverse Curriculum Generation for Reinforcement Learning.pdf:pdf},
keywords = {Automatic Curricu-lum Generation,Reinforcement Learning,Robotic Manipulation},
title = {{Reverse Curriculum Generation for Reinforcement Learning}},
url = {https://arxiv.org/pdf/1707.05300.pdf http://arxiv.org/abs/1707.05300},
year = {2018}
}
@article{peters2008baseball,
abstract = {Autonomous learning is one of the hallmarks of human and animal behavior, and understanding the principles of learning will be crucial in order to achieve true autonomy in advanced machines like humanoid robots. In this paper, we examine learning of complex motor skills with human-like limbs. While supervised learning can offer useful tools for bootstrapping behavior, e.g.,??by learning from demonstration, it is only reinforcement learning that offers a general approach to the final trial-and-error improvement that is needed by each individual acquiring a skill. Neither neurobiological nor machine learning studies have, so far, offered compelling results on how reinforcement learning can be scaled to the high-dimensional continuous state and action spaces of humans or humanoids. Here, we combine two recent research developments on learning motor control in order to achieve this scaling. First, we interpret the idea of modular motor control by means of motor primitives as a suitable way to generate parameterized control policies for reinforcement learning. Second, we combine motor primitives with the theory of stochastic policy gradient learning, which currently seems to be the only feasible framework for reinforcement learning for humanoids. We evaluate different policy gradient methods with a focus on their applicability to parameterized motor primitives. We compare these algorithms in the context of motor primitive learning, and show that our most modern algorithm, the Episodic Natural Actor-Critic outperforms previous algorithms by at least an order of magnitude. We demonstrate the efficiency of this reinforcement learning method in the application of learning to hit a baseball with an anthropomorphic robot arm. ?? 2008 Elsevier Ltd. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {arXiv:1411.3159v1},
author = {Peters, Jan and Schaal, Stefan},
doi = {10.1016/j.neunet.2008.02.003},
eprint = {arXiv:1411.3159v1},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Peters, Schaal - 2008 - Reinforcement learning of motor skills with policy gradients.pdf:pdf},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Motor primitives,Motor skills,Natural Actor-Critic,Natural gradients,Policy gradient methods,Reinforcement learning},
number = {4},
pages = {682--697},
pmid = {18482830},
title = {{Reinforcement learning of motor skills with policy gradients}},
url = {https://pdfs.semanticscholar.org/eb5b/459c8a3e56064158fb3514eeab763486e437.pdf},
volume = {21},
year = {2008}
}
@inproceedings{kalakrishnan09terraintemplates,
abstract = {â€” We address the problem of foothold selection in robotic legged locomotion over very rough terrain. The difficulty of the problem we address here is comparable to that of human rock-climbing, where foot/hand-hold selection is one of the most critical aspects. Previous work in this domain typically involves defining a reward function over footholds as a weighted linear combination of terrain features. However, a significant amount of effort needs to be spent in designing these features in order to model more complex decision functions, and hand-tuning their weights is not a trivial task. We propose the use of terrain templates, which are discretized height maps of the terrain under a foothold on different length scales, as an alternative to manually designed features. We describe an algo-rithm that can simultaneously learn a small set of templates and a foothold ranking function using these templates, from expert-demonstrated footholds. Using the LittleDog quadruped robot, we experimentally show that the use of terrain templates can produce complex ranking functions with higher performance than standard terrain features, and improved generalization to unseen terrain.},
author = {Kalakrishnan, Mrinal and Buchli, Jonas and Pastor, Peter and Schaal, Stefan},
booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Kalakrishnan et al. - Unknown - Learning Locomotion over Rough Terrain using Terrain Templates.pdf:pdf},
title = {{Learning Locomotion over Rough Terrain using Terrain Templates}},
url = {https://pdfs.semanticscholar.org/02db/2c0100ffb02592e8738d0ffcf454224f4b1b.pdf},
year = {2009}
}
@article{schaal97lfd,
abstract = {By now it is widely accepted that learning a task from scratch, i.e., without any prior knowledge, is a daunting undertaking. Humans, however, rarely at- tempt to learn from scratch. They extract initial biases as well as strategies how to approach a learning problem from instructions and/or demonstrations of other humans. For learning control, this paper investigates how learning from demonstration can be applied in the context of reinforcement learning. We consider priming the Q-function, the value function, the policy, and the model of the task dynamics as possible areas where demonstrations can speed up learning. In general nonlinear learning problems, only model-based rein- forcement learning shows significant speed-up after a demonstration, while in the special case of linear quadratic regulator (LQR) problems, all methods profit from the demonstration. In an implementation of pole balancing on a complex anthropomorphic robot arm, we demonstrate that, when facing the complexities of real signal processing, model-based reinforcement learning offers the most robustness for LQR problems. Using the suggested methods, the robot learns pole balancing in just a single trial after a 30 second long demonstration of the human instructor.},
author = {Schaal, Stefan},
doi = {10.1016/j.robot.2004.03.001},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Schaal - Unknown - Learning From Demonstration.pdf:pdf},
isbn = {1558604863},
issn = {1049-5258},
journal = {Advances in Neural Information Processing Systems (NIPS)},
number = {9},
pages = {1040--1046},
pmid = {11540378},
title = {{Robot learning from demonstration}},
url = {http://www.cc.gatech.edulfac http://wwwiaim.ira.uka.de/users/rogalla/WebOrdnerMaterial/ml-robotlearning.pdf},
year = {1997}
}
@inproceedings{finn16guidedcostlearning,
abstract = {Reinforcement learning can acquire complex be-haviors from high-level specifications. However, defining a cost function that can be optimized effectively and encodes the correct task is chal-lenging in practice. We explore how inverse op-timal control (IOC) can be used to learn behav-iors from demonstrations, with applications to torque control of high-dimensional robotic sys-tems. Our method addresses two key challenges in inverse optimal control: first, the need for in-formative features and effective regularization to impose structure on the cost, and second, the dif-ficulty of learning the cost function under un-known dynamics for high-dimensional continu-ous systems. To address the former challenge, we present an algorithm capable of learning ar-bitrary nonlinear cost functions, such as neural networks, without meticulous feature engineer-ing. To address the latter challenge, we formu-late an efficient sample-based approximation for MaxEnt IOC. We evaluate our method on a series of simulated tasks and real-world robotic manip-ulation problems, demonstrating substantial im-provement over prior methods both in terms of task complexity and sample efficiency.},
author = {Finn, Chelsea and Levine, Sergey and Abbeel, Pieter},
booktitle = {International Conference on Machine Learning (ICML)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Finn, Levine, Abbeel - Unknown - Guided Cost Learning Deep Inverse Optimal Control via Policy Optimization.pdf:pdf},
title = {{Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization}},
url = {https://arxiv.org/pdf/1603.00448.pdf},
year = {2016}
}
@inproceedings{bahdanau14attention,
abstract = {Neural machine translation is a recently proposed approach to machine transla-tion. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neu-ral machine translation often belong to a family of encoderâ€“decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoderâ€“decoder architec-ture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
booktitle = {International Conference on Learning Representations (ICLR)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Bahdanau, Cho, Bengio - Unknown - NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE(2).pdf:pdf},
title = {{Neural Machine Translation by Jointly Learning to Align and Translate}},
url = {https://arxiv.org/pdf/1409.0473.pdf},
year = {2015}
}
@inproceedings{duan2017oneshotimitation,
abstract = {Imitation learning has been commonly applied to solve different tasks in isolation. This usually requires either careful feature engineering, or a significant number of samples. This is far from what we desire: ideally, robots should be able to learn from very few demonstrations of any given task, and instantly generalize to new situations of the same task, without requiring task-specific engineering. In this paper, we propose a meta-learning framework for achieving such capability, which we call one-shot imitation learning. Specifically, we consider the setting where there is a very large set of tasks, and each task has many instantiations. For example, a task could be to stack all blocks on a table into a single tower, another task could be to place all blocks on a table into two-block towers, etc. In each case, different instances of the task would consist of different sets of blocks with different initial states. At training time, our algorithm is presented with pairs of demonstrations for a subset of all tasks. A neural net is trained that takes as input one demonstration and the current state (which initially is the initial state of the other demonstration of the pair), and outputs an action with the goal that the resulting sequence of states and actions matches as closely as possible with the second demonstration. At test time, a demonstration of a single instance of a new task is presented, and the neural net is expected to perform well on new instances of this new task. The use of soft attention allows the model to generalize to conditions and tasks unseen in the training data. We anticipate that by training this model on a much greater variety of tasks and settings, we will obtain a general system that can turn any demonstrations into robust policies that can accomplish an overwhelming variety of tasks. Videos available at https://bit.ly/one-shot-imitation .},
archivePrefix = {arXiv},
arxivId = {1703.07326},
author = {Duan, Yan and Andrychowicz, Marcin and Stadie, Bradly C. and Ho, Jonathan and Schneider, Jonas and Sutskever, Ilya and Abbeel, Pieter and Zaremba, Wojciech},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
eprint = {1703.07326},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Duan et al. - Unknown - One-Shot Imitation Learning.pdf:pdf},
title = {{One-Shot Imitation Learning}},
year = {2017}
}
@inproceedings{pong2018tdm,
abstract = {Model-free reinforcement learning (RL) is a powerful, general tool for learning complex behaviors. However, its sample efficiency is often impractically large for solving challenging real-world problems, even with off-policy algorithms such as Q-learning. A limiting factor in classic model-free RL is that the learning signal consists only of scalar rewards, ignoring much of the rich information contained in state transition tuples. Model-based RL uses this information, by training a predictive model, but often does not achieve the same asymptotic performance as model-free RL due to model bias. We introduce temporal difference models (TDMs), a family of goal-conditioned value functions that can be trained with model-free learning and used for model-based control. TDMs combine the bene-fits of model-free and model-based RL: they leverage the rich information in state transitions to learn very efficiently, while still attaining asymptotic performance that exceeds that of direct model-based RL methods. Our experimental results show that, on a range of continuous control tasks, TDMs provide a substantial im-provement in efficiency compared to state-of-the-art model-based and model-free methods.},
author = {Pong, Vitchyr and Gu, Shixiang and Dalal, Murtaza and Levine, Sergey},
booktitle = {International Conference on Learning Representations (ICLR)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Pong et al. - Unknown - TEMPORAL DIFFERENCE MODELS MODEL-FREE DEEP RL FOR MODEL-BASED CONTROL.pdf:pdf},
title = {{Temporal Difference Models: Model-Free Deep RL For Model-Based Control}},
url = {https://arxiv.org/pdf/1802.09081.pdf},
year = {2018}
}
@inproceedings{finn2016visualforesight,
abstract = {A key challenge in scaling up robot learning to many skills and environments is removing the need for human supervision, so that robots can collect their own data and improve their own performance without being limited by the cost of requesting human feedback. Model-based reinforcement learning holds the promise of enabling an agent to learn to predict the effects of its actions, which could provide flexible predictive models for a wide range of tasks and environments, without detailed human supervision. We develop a method for combining deep action-conditioned video prediction models with model-predictive control that uses entirely unlabeled training data. Our approach does not require a calibrated camera, an instrumented training set-up, nor precise sensing and actuation. Our results show that our method enables a real robot to perform nonprehensile manipulation -- pushing objects -- and can handle novel objects not seen during training.},
archivePrefix = {arXiv},
arxivId = {1610.00696},
author = {Finn, Chelsea and Levine, Sergey},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
eprint = {1610.00696},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Finn, Levine - Unknown - Deep Visual Foresight for Planning Robot Motion.pdf:pdf;:Users/ashvin/code/kindlize/pdfs/deeprl/deep-visual-foresight-Finn16.pdf:pdf},
title = {{Deep Visual Foresight for Planning Robot Motion}},
url = {https://arxiv.org/pdf/1610.00696.pdf http://arxiv.org/abs/1610.00696},
year = {2016}
}
@inproceedings{ijspeert2002attractor,
abstract = {Many control problems take place in continuous state-action spaces,$\backslash$n$\backslash$ne.g., as in manipulator robotics, where the control objective is of-$\backslash$n$\backslash$nten deï¿œned as ï¿œnding a desired trajectory that reaches a particular$\backslash$n$\backslash$ngoal state. While reinforcement learning oï¿œers a theoretical frame-$\backslash$n$\backslash$nwork to learn such control policies from scratch, its applicability$\backslash$nto$\backslash$n$\backslash$nhigher dimensional continuous state-action spaces remains rather$\backslash$n$\backslash$nlimited to date. Instead of learning from scratch, in this paper we$\backslash$n$\backslash$nsuggest to learn a desired complex control policy by transforming$\backslash$n$\backslash$nan existing simple canonical control policy. For this purpose, we$\backslash$n$\backslash$nrepresent canonical policies in terms of diï¿œerential equations with$\backslash$n$\backslash$nwell-deï¿œned attractor properties. By nonlinearly transforming the$\backslash$n$\backslash$ncanonical attractor dynamics using techniques from nonparametric$\backslash$n$\backslash$nregression, almost arbitrary new nonlinear policies can be gener-$\backslash$n$\backslash$nated without losing the stability properties of the canonical sys-$\backslash$n$\backslash$ntem. We demonstrate our techniques in the context of learning a$\backslash$n$\backslash$nset of movement skills for a humanoid robot from demonstrations$\backslash$n$\backslash$nof a human teacher. Policies are acquired rapidly, and, due to the$\backslash$n$\backslash$nproperties of well formulated diï¿œerential equations, can be re-used$\backslash$n$\backslash$nand modiï¿œed on-line under dynamic changes of the environment.$\backslash$n$\backslash$nThe linear parameterization of nonparametric regression moreover$\backslash$n$\backslash$nlends itself to recognize and classify previously learned movement$\backslash$n$\backslash$nskills. Evaluations in simulations and on an actual 30 degree-of-$\backslash$n$\backslash$nfreedom humanoid robot exemplify the feasibility and robustness$\backslash$n$\backslash$nof our approach.},
author = {Ijspeert, Auke Jan and Nakanishi, Jun and Schaal, Stefan},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Ijspeert, Nakanishi, Schaal - Unknown - Learning Attractor Landscapes for Learning Motor Primitives.pdf:pdf},
isbn = {1049-5258},
issn = {10495258},
keywords = {reinforcement},
pages = {1547--1554},
title = {{Learning Attractor Landscapes for Learning Motor Primitives}},
url = {https://papers.nips.cc/paper/2140-learning-attractor-landscapes-for-learning-motor-primitives.pdf},
year = {2002}
}
@inproceedings{pathak2018zeroshot,
abstract = {The current dominant paradigm for imitation learning relies on strong supervision of expert actions to learn both what and how to imitate. We pursue an alternative paradigm wherein an agent first explores the world without any expert supervision and then distills its experience into a goal-conditioned skill policy with a novel forward consistency loss. In our framework, the role of the expert is only to communicate the goals (i.e., what to imitate) during inference. The learned policy is then employed to mimic the expert (i.e., how to imitate) after seeing just a sequence of images demonstrating the desired task. Our method is " zero-shot " in the sense that the agent never has access to expert actions during training or for the task demonstration at inference. We evaluate our zero-shot imitator in two real-world settings: complex rope manipulation with a Baxter robot and navigation in previously unseen office environments with a TurtleBot. Through further experiments in VizDoom simulation, we provide evidence that better mechanisms for exploration lead to learning a more capable policy which in turn improves end task performance. Videos, models, and more details are available at https://pathak22.github.io/zeroshot-imitation/.},
author = {Pathak, Deepak and Mahmoudieh, Parsa and Luo, Guanghao and Agrawal, Pulkit and Chen, Dian and Shentu, Yide and Shelhamer, Evan and Malik, Jitendra and Efros, Alexei A and Darrell, Trevor},
booktitle = {International Conference on Learning Representations (ICLR)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Pathak et al. - Unknown - Zero-Shot Visual Imitation.pdf:pdf},
title = {{Zero-Shot Visual Imitation}},
url = {https://arxiv.org/pdf/1804.08606.pdf},
year = {2018}
}
@inproceedings{pomerleau1989alvinn,
abstract = {ALVINN (Autonomous Land Vehicle In a Neural Network) is a 3-layer back-propagation network designed for the task of road following. Currently ALVINN takes images from a camera and a laser range finder as input and produces as output the direction the vehicle should travel in order to follow the road. Training has been conducted using simulated road images. Successful tests on the Carnegie Mellon autonomous navigation test vehicle indicate that the network can effectively follow real roads under certain field conditions. The representation developed to perform the task differs dramatically when the network is trained under various conditions, suggesting the possibility of a novel adaptive autonomous navigation system capable of tailoring its processing to the conditions at hand.},
author = {Pomerleau, Dean A},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Showcase, Cmu, Pomerleau - Unknown - ALVINN, an autonomous land vehicle in a neural network.pdf:pdf},
isbn = {1-558-60015-9},
pages = {305--313},
title = {{Alvinn: An autonomous land vehicle in a neural network}},
url = {http://repository.cmu.edu/compsci},
year = {1989}
}
@inproceedings{sun17deeplyaggrevated,
abstract = {Researchers have demonstrated state-of-the-art performance in sequential decision making prob-lems (e.g., robotics control, sequential predic-tion) with deep neural network models. One often has access to near-optimal oracles that achieve good performance on the task during training. We demonstrate that AggreVaTeD â€” a policy gradient extension of the Imitation Learn-ing (IL) approach of (Ross {\&} Bagnell, 2014) â€” can leverage such an oracle to achieve faster and better solutions with less training data than a less-informed Reinforcement Learning (RL) technique. Using both feedforward and recur-rent neural predictors, we present stochastic gra-dient procedures on a sequential prediction task, dependency-parsing from raw image data, as well as on various high dimensional robotics con-trol problems. We also provide a comprehen-sive theoretical study of IL that demonstrates we can expect up to exponentially lower sample complexity for learning with AggreVaTeD than with RL algorithms, which backs our empirical findings. Our results and theory indicate that the proposed approach can achieve superior per-formance with respect to the oracle when the demonstrator is sub-optimal.},
author = {Sun, Wen and Venkatraman, Arun and Gordon, Geoffrey J and Boots, Byron and Edu, Bboots@cc Gatech and Bagnell, J Andrew},
booktitle = {International Conference on Machine Learning (ICML)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Sun et al. - Unknown - Deeply AggreVaTeD Differentiable Imitation Learning for Sequential Prediction(2).pdf:pdf},
title = {{Deeply AggreVaTeD: Differentiable Imitation Learning for Sequential Prediction}},
url = {https://arxiv.org/pdf/1703.01030.pdf},
year = {2017}
}
@article{wakamatsu2006knotting,
author = {Wakamatsu, Hidefumi and Arai, Eiji and Hirai, Shinichi},
journal = {The International Journal of Robotics Research},
number = {4},
pages = {371--395},
publisher = {SAGE Publications},
title = {{Knotting/unknotting manipulation of deformable linear objects}},
volume = {25},
year = {2006}
}
@article{Ross2014,
abstract = {Recent work has demonstrated that problems-- particularly imitation learning and structured prediction-- where a learner's predictions influence the input-distribution it is tested on can be naturally addressed by an interactive approach and analyzed using no-regret online learning. These approaches to imitation learning, however, neither require nor benefit from information about the cost of actions. We extend existing results in two directions: first, we develop an interactive imitation learning approach that leverages cost information; second, we extend the technique to address reinforcement learning. The results provide theoretical support to the commonly observed successes of online approximate policy iteration. Our approach suggests a broad new family of algorithms and provides a unifying view of existing techniques for imitation and reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1406.5979},
author = {Ross, St{\'{e}}phane Stephane and Bagnell, J Andrew},
eprint = {1406.5979},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Ross, Bagnell - 2014 - Reinforcement and Imitation Learning via Interactive No-Regret Learning(2).pdf:pdf},
journal = {CoRR},
keywords = {()},
title = {{Reinforcement and Imitation Learning via Interactive No-Regret Learning}},
url = {https://arxiv.org/pdf/1406.5979.pdf http://arxiv.org/abs/1406.5979},
volume = {abs/1406.5},
year = {2014}
}
@article{vecerik17ddpgfd,
abstract = {We propose a general and model-free approach for Reinforcement Learning (RL) on real robotics with sparse rewards. We build upon the Deep Deterministic Policy Gradient (DDPG) algorithm to use demonstrations. Both demonstrations and actual interactions are used to fill a replay buffer and the sam-pling ratio between demonstrations and transitions is automatically tuned via a prioritized replay mechanism. Typically, carefully engineered shaping rewards are required to enable the agents to efficiently explore on high dimensional control problems such as robotics. They are also required for model-based acceleration methods relying on local solvers such as iLQG (e.g. Guided Policy Search and Nor-malized Advantage Function). The demonstrations replace the need for carefully engineered rewards, and reduce the exploration problem encountered by classical RL approaches in these domains. Demonstrations are collected by a robot kines-thetically force-controlled by a human demonstrator. Results on four simulated insertion tasks show that DDPG from demonstrations out-performs DDPG, and does not require engineered rewards. Finally, we demonstrate the method on a real robotics task consisting of inserting a clip (flexible object) into a rigid object.},
author = {Ve{\v{c}}er{\'{i}}k, Matej and Hester, Todd and Scholz, Jonathan and Wang, Fumin and Pietquin, Olivier and Piot, Bilal and Heess, Nicolas and Roth{\"{o}}rl, Thomas and Lampe, Thomas and Riedmiller, Martin},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Ve{\v{c}}er{\'{i}}k et al. - Unknown - Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards.pdf:pdf},
journal = {CoRR},
keywords = {Apprenticeship,Demonstrations,Learning,Robot},
title = {{Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards}},
url = {https://arxiv.org/pdf/1707.08817.pdf},
volume = {abs/1707.0},
year = {2017}
}
@inproceedings{reed2014learning,
author = {Reed, Scott and Sohn, Kihyuk and Zhang, Yuting and Lee, Honglak},
booktitle = {International Conference on Machine Learning},
pages = {1431--1439},
title = {{Learning to disentangle factors of variation with manifold interaction}},
year = {2014}
}
@inproceedings{yang2015robot,
author = {Yang, Yezhou and Li, Yi and Ferm{\"{u}}ller, Cornelia and Aloimonos, Yiannis},
booktitle = {AAAI Conference on Artificial Intelligence},
pages = {3686--3693},
title = {{Robot Learning Manipulation Action Plans by" Watching" Unconstrained Videos from the World Wide Web.}},
year = {2015}
}
@article{michotte1963perception,
author = {Michotte, Albert},
publisher = {Basic Books},
title = {{The perception of causality.}},
year = {1963}
}
@article{levy2017hierarchical,
author = {Levy, Andrew and Platt, Robert and Saenko, Kate},
journal = {arXiv preprint arXiv:1712.00948},
title = {{Hierarchical Actor-Critic}},
year = {2017}
}
@inproceedings{morita2003knot,
author = {Morita, Takuma and Takamatsu, Jun and Ogawara, Koichi and Kimura, Hiroshi and Ikeuchi, Katsushi},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
organization = {IEEE},
pages = {3887--3892},
title = {{Knot planning from observation}},
volume = {3},
year = {2003}
}
@book{lavalle2006planning,
author = {LaValle, Steven M},
publisher = {Cambridge university press},
title = {{Planning algorithms}},
year = {2006}
}
@inproceedings{kakade2002approximatelyoptimal,
abstract = {Abstract In order to solve realistic reinforcement learning problems, it is critical that approximate algorithms be used. In this paper, we present the conservative policy iteration algorithm which finds an" approximately" optimal policy, given access to a restart ... $\backslash$n},
author = {Kakade, Sham and Langford, John},
booktitle = {International Conference on Machine Learning (ICML)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Kakade, Langford - 2002 - Approximately Optimal Approximate Reinforcement Learning.pdf:pdf},
isbn = {1-55860-873-7},
pages = {267--274},
title = {{Approximately Optimal Approximate Reinforcement Learning}},
url = {http://www.cs.cmu.edu/afs/cs/Web/People/jcl/papers/aoarl/Final.pdf},
year = {2002}
}
@inproceedings{hosu2016humancheckpoint,
abstract = {This paper introduces a novel method for learning how to play the most difficult Atari 2600 games from the Arcade Learn-ing Environment using deep reinforcement learning. The proposed method, called human checkpoint replay, consists in using check-points sampled from human gameplay as starting points for the learn-ing process. This is meant to compensate for the difficulties of current exploration strategies, such as $\epsilon$-greedy, to find successful control policies in games with sparse rewards. Like other deep reinforce-ment learning architectures, our model uses a convolutional neural network that receives only raw pixel inputs to estimate the state value function. We tested our method on Montezuma's Revenge and Pri-vate Eye, two of the most challenging games from the Atari platform. The results we obtained show a substantial improvement compared to previous learning approaches, as well as over a random player. We also propose a method for training deep reinforcement learning agents using human gameplay experience, which we call human ex-perience replay.},
author = {Hosu, Ionel-Alexandru and Rebedea, Traian},
booktitle = {Workshop on Evaluating General Purpose AI},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Hosu, Rebedea - Unknown - Playing Atari Games with Deep Reinforcement Learning and Human Checkpoint Replay.pdf:pdf},
title = {{Playing Atari Games with Deep Reinforcement Learning and Human Checkpoint Replay}},
url = {https://arxiv.org/pdf/1607.05077.pdf},
year = {2016}
}
@article{Dragan2017,
abstract = {Robots interacting with the physical world plan with models of physics. We advocate that robots interacting with people need to plan with models of cognition. This writeup summarizes the insights we have gained in integrating computational cognitive models of people into robotics planning and control. It starts from a general game-theoretic formulation of interaction, and analyzes how different approximations result in different useful coordination behaviors for the robot during its interaction with people.},
archivePrefix = {arXiv},
arxivId = {1705.04226},
author = {Dragan, Anca D.},
eprint = {1705.04226},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Dragan - 2017 - Robot Planning with Mathematical Models of Human State and Action.pdf:pdf},
month = {may},
title = {{Robot Planning with Mathematical Models of Human State and Action}},
url = {https://arxiv.org/pdf/1705.04226.pdf http://arxiv.org/abs/1705.04226},
year = {2017}
}
@inproceedings{mnih2013atari,
abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
archivePrefix = {arXiv},
arxivId = {1312.5602},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
booktitle = {NIPS Workshop on Deep Learning},
doi = {10.1038/nature14236},
eprint = {1312.5602},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Mnih et al. - Unknown - Playing Atari with Deep Reinforcement Learning.pdf:pdf;:Users/ashvin/code/kindlize/pdfs/deeprl/playing-atari-with-deeprl-Mnih13.pdf:pdf;:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Mnih et al. - Unknown - Playing Atari with Deep Reinforcement Learning(2).pdf:pdf},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {0028-0836},
pages = {1--9},
pmid = {25719670},
title = {{Playing Atari with Deep Reinforcement Learning}},
url = {https://arxiv.org/pdf/1312.5602.pdf http://arxiv.org/abs/1312.5602 https://www.cs.toronto.edu/{~}vmnih/docs/dqn.pdf},
year = {2013}
}
@inproceedings{rajeswaran2017simplicity,
abstract = {This work shows that policies with simple lin-ear and RBF parameterizations can be trained to solve a variety of continuous control tasks, in-cluding the OpenAI gym benchmarks. The per-formance of these trained policies are competi-tive with state of the art results, obtained with more elaborate parameterizations such as fully connected neural networks. Furthermore, exist-ing training and testing scenarios are shown to be very limited and prone to over-fitting, thus giving rise to only trajectory centric policies. Training with a diverse initial state distribution is shown to produce more global policies with better gen-eralization. This allows for interactive control scenarios where the system recovers from large on-line perturbations; as shown in the video.},
author = {Rajeswaran, Aravind and Lowrey, Kendall and Todorov, Emanuel and Kakade, Sham},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Rajeswaran et al. - Unknown - Towards Generalization and Simplicity in Continuous Control.pdf:pdf},
title = {{Towards Generalization and Simplicity in Continuous Control}},
url = {https://arxiv.org/pdf/1703.02660.pdf https://homes.cs.washington.edu/{~}todorov/papers/RajeswaranICML17.pdf},
year = {2017}
}
@inproceedings{nair2018demonstrations,
abstract = {â€” Exploration in environments with sparse rewards has been a persistent problem in reinforcement learning (RL). Many tasks are natural to specify with a sparse reward, and manually shaping a reward function can result in suboptimal performance. However, finding a non-zero reward is exponen-tially more difficult with increasing task horizon or action dimensionality. This puts many real-world tasks out of practical reach of RL methods. In this work, we use demonstrations to overcome the exploration problem and successfully learn to perform long-horizon, multi-step robotics tasks with continuous control such as stacking blocks with a robot arm. Our method, which builds on top of Deep Deterministic Policy Gradients and Hindsight Experience Replay, provides an order of magnitude of speedup over RL on simulated robotics tasks. It is simple to implement and makes only the additional assumption that we can collect a small set of demonstrations. Furthermore, our method is able to solve tasks not solvable by either RL or behavior cloning alone, and often ends up outperforming the demonstrator policy.},
author = {Nair, Ashvin and Mcgrew, Bob and Andrychowicz, Marcin and Zaremba, Wojciech and Abbeel, Pieter},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Nair et al. - Unknown - Overcoming Exploration in Reinforcement Learning with Demonstrations.pdf:pdf},
title = {{Overcoming Exploration in Reinforcement Learning with Demonstrations}},
url = {https://arxiv.org/pdf/1709.10089.pdf},
year = {2018}
}
@inproceedings{vezhnevets2017fun,
abstract = {We introduce FeUdal Networks (FuNs): a novel architecture for hierarchical reinforcement learning. Our approach is inspired by the feudal reinforcement learning proposal of Dayan and Hinton, and gains power and efficacy by decoupling end-to-end learning across multiple levels -- allowing it to utilise different resolutions of time. Our framework employs a Manager module and a Worker module. The Manager operates at a lower temporal resolution and sets abstract goals which are conveyed to and enacted by the Worker. The Worker generates primitive actions at every tick of the environment. The decoupled structure of FuN conveys several benefits -- in addition to facilitating very long timescale credit assignment it also encourages the emergence of sub-policies associated with different goals set by the Manager. These properties allow FuN to dramatically outperform a strong baseline agent on tasks that involve long-term credit assignment or memorisation. We demonstrate the performance of our proposed system on a range of tasks from the ATARI suite and also from a 3D DeepMind Lab environment.},
archivePrefix = {arXiv},
arxivId = {1703.01161},
author = {Vezhnevets, Alexander Sasha and Osindero, Simon and Schaul, Tom and Heess, Nicolas and Jaderberg, Max and Silver, David and Kavukcuoglu, Koray},
booktitle = {International Conference on Machine Learning (ICML)},
eprint = {1703.01161},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Vezhnevets et al. - Unknown - FeUdal Networks for Hierarchical Reinforcement Learning.pdf:pdf;:Users/ashvin/code/kindlize/pdfs/deeprl/feudal-RL-Vezhnevets17.pdf:pdf},
title = {{FeUdal Networks for Hierarchical Reinforcement Learning}},
url = {https://arxiv.org/pdf/1703.01161.pdf http://arxiv.org/abs/1703.01161},
year = {2017}
}
@inproceedings{wang2016duelingdqn,
abstract = {In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this pa-per, we present a new neural network architec-ture for model-free reinforcement learning. Our dueling network represents two separate estima-tors: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to general-ize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architec-ture leads to better policy evaluation in the pres-ence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.},
author = {Wang, Ziyu and Schaul, Tom and Hessel, Matteo and Com, Mtthss@google and {Van Hasselt}, Hado and Lanctot, Marc},
booktitle = {International Conference on Machine Learning (ICML)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - Unknown - Dueling Network Architectures for Deep Reinforcement Learning.pdf:pdf},
title = {{Dueling Network Architectures for Deep Reinforcement Learning}},
url = {https://arxiv.org/pdf/1511.06581.pdf},
year = {2016}
}
@inproceedings{silver2014dpg,
abstract = {In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic pol-icy gradient has a particularly appealing form: it is the expected gradient of the action-value func-tion. This simple form means that the deter-ministic policy gradient can be estimated much more efficiently than the usual stochastic pol-icy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. We demonstrate that deterministic policy gradient algorithms can significantly outperform their stochastic counter-parts in high-dimensional action spaces.},
author = {Silver, David and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
booktitle = {International Conference on Machine Learning (ICML)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Silver et al. - Unknown - Deterministic Policy Gradient Algorithms.pdf:pdf},
title = {{Deterministic Policy Gradient Algorithms}},
url = {http://proceedings.mlr.press/v32/silver14.pdf},
year = {2014}
}
@inproceedings{liu2018imitation,
abstract = {Imitation learning is an effective approach for autonomous systems to acquire control policies when an explicit reward function is unavailable, using supervision provided as demonstrations from an expert, typically a human operator. However, standard imitation learning methods assume that the agent receives examples of observation-action tuples that could be provided, for instance, to a supervised learning algorithm. This stands in contrast to how humans and animals imitate: we observe another person performing some behavior and then figure out which actions will realize that behavior, compensating for changes in viewpoint, surroundings, and embodiment. We term this kind of imitation learning as imitation-from-observation and propose an imitation learning method based on video prediction with context translation and deep reinforcement learning. This lifts the assumption in imitation learning that the demonstration should consist of observations and actions in the same environment, and enables a variety of interesting applications, including learning robotic skills that involve tool use simply by observing videos of human tool use. Our experimental results show that our approach can perform imitation-from-observation for a variety of real-world robotic tasks modeled on common household chores, acquiring skills such as sweeping from videos of a human demonstrator. Videos can be found at https://sites.google.com/ site/imitationfromobservation/},
author = {Liu, Yuxuan and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Liu et al. - Unknown - Imitation from Observation Learning to Imitate Behaviors from Raw Video via Context Translation.pdf:pdf},
title = {{Imitation from Observation: Learning to Imitate Behaviors from Raw Video via Context Translation}},
url = {https://arxiv.org/pdf/1707.03374.pdf},
year = {2018}
}
@inproceedings{Groshev,
abstract = {We consider the problem of learning for planning, where knowledge acquired while planning is reused to plan faster in new problem instances. For robotic tasks, among others, plan execution can be captured as a sequence of visual images. For such domains, we propose to use deep neural networks in learning for planning, based on learning a reactive policy that imitates execution traces produced by a planner. We investigate architectural properties of deep networks that are suitable for learning long-horizon planning behavior, and explore how to learn, in addition to the policy, a heuristic function that can be used with classical planners or search algorithms such as A * . Our results on the challenging Sokoban domain show that, with a suitable network design, complex decision making policies and powerful heuristic functions can be learned through imitation. Videos available at https://sites.google.com/site/learn2plannips/.},
author = {Groshev, Edward and Tamar, Aviv and Srivastava, Siddharth and Abbeel, Pieter},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Groshev et al. - Unknown - Learning Generalized Reactive Policies using Deep Neural Networks(2).pdf:pdf},
title = {{Learning Generalized Reactive Policies using Deep Neural Networks}},
url = {https://arxiv.org/pdf/1708.07280.pdf}
}
@inproceedings{fu2017ex2,
abstract = {Deep reinforcement learning algorithms have been shown to learn complex tasks using highly general policy classes. However, sparse reward problems remain a significant challenge. Exploration methods based on novelty detection have been particularly successful in such settings but typically require generative or predictive models of the observations, which can be difficult to train when the observations are very high-dimensional and complex, as in the case of raw images. We propose a novelty detection algorithm for exploration that is based entirely on discriminatively trained exemplar models, where classifiers are trained to discriminate each visited state against all others. Intuitively, novel states are easier to distinguish against other states seen during training. We show that this kind of discriminative modeling corresponds to implicit density estimation, and that it can be combined with count-based exploration to produce competitive results on a range of popular benchmark tasks, including state-of-the-art results on challenging egocentric observations in the vizDoom benchmark.},
author = {Fu, Justin and Co-Reyes, John D and Levine, Sergey},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Fu, Co-Reyes, Levine - Unknown - EX 2 Exploration with Exemplar Models for Deep Reinforcement Learning.pdf:pdf},
title = {{EX 2 : Exploration with Exemplar Models for Deep Reinforcement Learning}},
url = {https://arxiv.org/pdf/1703.01260.pdf},
year = {2017}
}
@book{sutton1998rl,
author = {Sutton, Richard S and Barto, Andrew G},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Sutton, Barto - 2014 - Reinforcement Learning An Introduction(3).pdf:pdf},
title = {{Reinforcement Learning: An Introduction}},
url = {http://incompleteideas.net/sutton/book/bookdraft2016sep.pdf https://webdocs.cs.ualberta.ca/{~}sutton/book/bookdraft2016sep.pdf},
year = {1998}
}
@inproceedings{heess2015svg,
abstract = {We present a unified framework for learning continuous control policies using backpropagation. It supports stochastic control by treating stochasticity in the Bellman equation as a deterministic function of exogenous noise. The product is a spectrum of general policy gradient algorithms that range from model-free methods with value functions to model-based methods without value functions. We use learned models but only require observations from the environment in-stead of observations from model-predicted trajectories, minimizing the impact of compounded model errors. We apply these algorithms first to a toy stochastic control problem and then to several physics-based control problems in simulation. One of these variants, SVG(1), shows the effectiveness of learning models, value functions, and policies simultaneously in continuous domains.},
author = {Heess, Nicolas and Wayne, Greg and Silver, David and Lillicrap, Timothy and Tassa, Yuval and Erez, Tom and Deepmind, Google},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Heess et al. - Unknown - Learning Continuous Control Policies by Stochastic Value Gradients.pdf:pdf},
title = {{Learning Continuous Control Policies by Stochastic Value Gradients}},
url = {https://arxiv.org/pdf/1510.09142.pdf},
year = {2015}
}
@inproceedings{jang2017grasping,
abstract = {We consider the task of semantic robotic grasping, in which a robot picks up an object of a user-specified class using only monocular images. Inspired by the two-stream hypothesis of visual reasoning, we present a semantic grasping framework that learns object detection, classification, and grasp planning in an end-to-end fashion. A " ventral stream " recognizes object class while a " dorsal stream " simultaneously interprets the geometric relationships necessary to exe-cute successful grasps. We leverage the autonomous data collection capabilities of robots to obtain a large self-supervised dataset for training the dorsal stream, and use semi-supervised label propagation to train the ventral stream with only a modest amount of human supervision. We experimentally show that our approach improves upon grasping systems whose components are not learned end-to-end, including a baseline method that uses bounding box detection. Furthermore, we show that jointly training our model with auxiliary data consisting of non-semantic grasping data, as well as semantically labeled images without grasp actions, has the potential to substantially improve semantic grasping performance 1 .},
author = {Jang, Eric and Brain, Google and Vijayanarasimhan, Sudheendra and Pastor, Peter and Ibarz, Julian and Levine, Sergey},
booktitle = {Conference on Robot Learning (CoRL)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Jang et al. - Unknown - End-to-End Learning of Semantic Grasping.pdf:pdf;:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Jang et al. - Unknown - End-to-End Learning of Semantic Grasping(2).pdf:pdf},
keywords = {deep learning,semantic grasping},
title = {{End-to-End Learning of Semantic Grasping}},
url = {https://arxiv.org/pdf/1707.01932.pdf},
year = {2017}
}
@inproceedings{wu2017acktr,
abstract = {In this work, we propose to apply trust region optimization to deep reinforce-ment learning using a recently proposed Kronecker-factored approximation to the curvature. We extend the framework of natural policy gradient and propose to optimize both the actor and the critic using Kronecker-factored approximate curvature (K-FAC) with trust region; hence we call our method Actor Critic using Kronecker-Factored Trust Region (ACKTR). To the best of our knowledge, this is the first scalable trust region natural gradient method for actor-critic methods. It is also a method that learns non-trivial tasks in continuous control as well as discrete control policies directly from raw pixel inputs. We tested our approach across discrete domains in Atari games as well as continuous domains in the Mu-JoCo environment. With the proposed methods, we are able to achieve higher rewards and a 2-to 3-fold improvement in sample efficiency on average, compared to previous state-of-the-art on-policy actor-critic methods. Code is available at https://github.com/openai/baselines.},
author = {Wu, Yuhuai and Mansimov, Elman and Liao, Shun and Grosse, Roger and Ba, Jimmy},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Wu et al. - Unknown - Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation(2).pdf:pdf;:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Wu et al. - Unknown - Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation.pdf:pdf},
title = {{Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation}},
url = {https://arxiv.org/pdf/1708.05144.pdf},
year = {2017}
}
@inproceedings{mishra2017icml,
abstract = {We introduce a method for learning the dynamics of complex nonlinear systems based on deep gen-erative models over temporal segments of states and actions. Unlike dynamics models that oper-ate over individual discrete timesteps, we learn the distribution over future state trajectories con-ditioned on past state, past action, and planned future action trajectories, as well as a latent prior over action trajectories. Our approach is based on convolutional autoregressive models and vari-ational autoencoders. It makes stable and accu-rate predictions over long horizons for complex, stochastic systems, effectively expressing uncer-tainty and modeling the effects of collisions, sen-sory noise, and action delays. The learned dy-namics model and action prior can be used for end-to-end, fully differentiable trajectory opti-mization and model-based policy optimization, which we use to evaluate the performance and sample-efficiency of our method.},
author = {Mishra, Nikhil and Abbeel, Pieter and Mordatch, Igor},
booktitle = {International Conference on Machine Learning (ICML)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Mishra, Abbeel, Mordatch - Unknown - Prediction and Control with Temporal Segment Models(2).pdf:pdf},
title = {{Prediction and Control with Temporal Segment Models}},
url = {https://arxiv.org/pdf/1703.04070.pdf},
year = {2017}
}
@inproceedings{Haarnoja2017,
abstract = {We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learn-ing maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that ex-presses the optimal policy via a Boltzmann dis-tribution. We use the recently proposed amor-tized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of the proposed algorithm include improved explo-ration and compositionality that allows transfer-ring skills between tasks, which we confirm in simulated experiments with swimming and walk-ing robots. We also draw a connection to actor-critic methods, which can be viewed perform-ing approximate inference on the corresponding energy-based model.},
address = {haarnoja2017sql},
author = {Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
booktitle = {International Conference on Machine Learning (ICML)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Haarnoja et al. - Unknown - Reinforcement Learning with Deep Energy-Based Policies(2).pdf:pdf;:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Haarnoja et al. - Unknown - Reinforcement Learning with Deep Energy-Based Policies(3).pdf:pdf},
title = {{Reinforcement Learning with Deep Energy-Based Policies}},
url = {https://arxiv.org/pdf/1702.08165.pdf},
year = {2017}
}
@inproceedings{christiano2017humanpreferences,
abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.},
archivePrefix = {arXiv},
arxivId = {1706.03741},
author = {Christiano, Paul F and Leike, Jan and Brown, Tom B and Martic, Miljan and Legg, Shane and Amodei, Dario},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
eprint = {1706.03741},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Christiano et al. - 2017 - Deep reinforcement learning from human preferences.pdf:pdf;:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Christiano et al. - Unknown - Deep Reinforcement Learning from Human Preferences.pdf:pdf},
title = {{Deep reinforcement learning from human preferences}},
url = {https://arxiv.org/pdf/1706.03741.pdf http://arxiv.org/abs/1706.03741},
year = {2017}
}
@book{szepesvari2010,
abstract = {Reinforcement learning is a learning paradigm concerned with learning to control a system so as to maximize a numerical performance measure that expresses a long-term objective.What distin- guishes reinforcement learning from supervised learning is that only partial feedback is given to the learner about the learner's predictions. Further, the predictions may have long term effects through influencing the future state of the controlled system. Thus, time plays a special role. The goal in reinforcement learning is to develop efficient learning algorithms, as well as to understand the al- gorithms' merits and limitations. Reinforcement learning is of great interest because of the large number of practical applications that it can be used to address, ranging from problems in artificial intelligence to operations research or control engineering. In this book,we focus on those algorithms of reinforcement learning that build on the powerful theory of dynamic programming.We give a fairly comprehensive catalog of learning problems, describe the core ideas, note a large number of state of the art algorithms, followed by the discussion of their theoretical properties and limitations.},
author = {Szepesv{\'{a}}ri, Csaba},
booktitle = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
doi = {10.2200/S00268ED1V01Y201005AIM009},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Szepesv{\'{a}}ri - 2009 - Algorithms for Reinforcement Learning.pdf:pdf},
isbn = {9781608454921},
issn = {1939-4608},
number = {1},
pages = {1--103},
title = {{Algorithms for Reinforcement Learning}},
url = {https://sites.ualberta.ca/{~}szepesva/papers/RLAlgsInMDPs.pdf},
volume = {4},
year = {2010}
}
@inproceedings{giusti2016trails,
author = {Giusti, Alessandro and Guzzi, J{\'{e}}r{\^{o}}me Jerome and Cirean, Dan C and He, Fang-Lin and Rodr{\'{i}}guez, Juan P and Fontana, Flavio and Faessler, Matthias and Forster, Christian and Schmidhuber, J{\"{u}}rgen Jurgen and Caro, Gianni Di and Scaramuzza, Davide and Gambardella, Luca M and Ciresan, Dan C. and He, Fang-Lin and Rodriguez, Juan P. and Fontana, Flavio and Faessler, Matthias and Forster, Christian and Schmidhuber, J{\"{u}}rgen Jurgen and Caro, Gianni Di and Scaramuzza, Davide and Gambardella, Luca M},
booktitle = {IEEE Robotics and Automation Letters},
doi = {10.1109/LRA.2015.2509024},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Giusti et al. - 2016 - A Machine Learning Approach to Visual Perception of Forest Trails for Mobile Robots.pdf:pdf},
isbn = {9781467380256},
issn = {2377-3766},
keywords = {Aerial Robotics,Deep Learning,Index Termsâ€”Visual-Based Navigation,Ma-chine Learning},
number = {2},
pmid = {12546789},
title = {{A Machine Learning Approach to Visual Perception of Forest Trails for Mobile Robots}},
url = {http://bit.ly/perceivingtrails. http://ieeexplore.ieee.org/document/7358076/},
volume = {1},
year = {2016}
}
@inproceedings{goodfellow2014gan,
abstract = {We propose a new framework for estimating generative models via an adversar-ial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The train-ing procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1 2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference net-works during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
author = {Goodfellow, Ian J and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Goodfellow et al. - Unknown - Generative Adversarial Nets.pdf:pdf;:Users/ashvin/code/kindlize/pdfs/deeprl/gans-Goodfellow14.pdf:pdf},
title = {{Generative Adversarial Nets}},
url = {https://arxiv.org/pdf/1406.2661.pdf},
year = {2014}
}
@article{Sunderhauf2018,
abstract = {The application of deep learning in robotics leads to very specific problems and research questions that are typically not addressed by the computer vision and machine learning communities. In this paper we discuss a number of robotics-specific learning, reasoning, and embodiment challenges for deep learning. We explain the need for better evaluation metrics, highlight the importance and unique challenges for deep robotic learning in simulation, and explore the spectrum between purely data-driven and model-driven approaches. We hope this paper provides a motivating overview of important research directions to overcome the current limitations, and help fulfill the promising potentials of deep learning in robotics.},
archivePrefix = {arXiv},
arxivId = {1804.06557},
author = {S{\"{u}}nderhauf, Niko and Brock, Oliver and Scheirer, Walter and Hadsell, Raia and Fox, Dieter and Leitner, J{\"{u}}rgen and Upcroft, Ben and Abbeel, Pieter and Burgard, Wolfram and Milford, Michael and Corke, Peter},
doi = {10.1177/ToBeAssigned},
eprint = {1804.06557},
file = {:Users/ashvin/Documents/papers/1804.06557.pdf:pdf},
isbn = {0037549716666},
issn = {1756-8277},
title = {{The Limits and Potentials of Deep Learning for Robotics}},
url = {http://arxiv.org/abs/1804.06557},
year = {2018}
}
@inproceedings{Todorov2018,
abstract = {â€” We develop a general control framework where a low-level optimizer is built into the robot dynamics. This optimizer together with the robot constitute a goal directed dynamical system, controlled on a higher level. The high level command is a cost function. It can encode desired accelerations, end-effector poses, center of pressure, and other intuitive features that have been studied before. Unlike the currently popular quadratic programming framework, which comes with performance guarantees at the expense of modeling flexibility, the optimization problem we solve at each time step is non-convex and non-smooth. Nevertheless, by exploiting the unique properties of the soft-constraint physics model we have recently developed, we are able to design an efficient solver for goal directed dynamics. It is only two times slower than the forward dynamics solver, and is much faster than real time. The simulation results reveal that complex movements can be generated via greedy optimization of simple costs. This new computational infrastructure can facilitate teleoperation, feature-based control, deep learning of control policies, and trajectory optimization. It will become a standard feature in future releases of the MuJoCo simulator.},
author = {Todorov, Emanuel},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Todorov - 2018 - Goal Directed Dynamics.pdf:pdf;:Users/ashvin/Documents/papers/TodorovICRA18.pdf:pdf},
isbn = {9781538630808},
title = {{Goal Directed Dynamics}},
url = {https://homes.cs.washington.edu/{~}todorov/papers/TodorovICRA18.pdf},
year = {2018}
}
@article{Keramati,
abstract = {Humans learn to play video games significantly faster than state-of-the-art rein-forcement learning (RL) algorithms. Inspired by this, we introduce strategic object oriented reinforcement learning (SOORL) to learn simple dynamics model through automatic model selection and perform efficient planning with strategic exploration. We compare different exploration strategies in a model-based setting in which exact planning is impossible. Additionally, we test our approach on perhaps the hardest Atari game Pitfall! and achieve significantly improved exploration and performance over prior methods.},
archivePrefix = {arXiv},
arxivId = {1806.00175},
author = {Keramati, Ramtin and Whang, Jay and Cho, Patrick and Brunskill, Emma},
eprint = {1806.00175},
file = {:Users/ashvin/Documents/papers/soorl.pdf:pdf},
pages = {1--10},
title = {{Strategic Object Oriented Reinforcement Learning}}
}
@article{Johnson,
author = {Johnson, Matthew James and Duvenaud, David and Wiltschko, Alexander B and Datta, Sandeep R and Adams, Ryan P},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Johnson et al. - Unknown - Composing graphical models with neural networks for structured representations and fast inference(2).pdf:pdf;:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Johnson et al. - Unknown - Composing graphical models with neural networks for structured representations and fast inference.pdf:pdf},
title = {{Composing graphical models with neural networks for structured representations and fast inference}}
}
@inproceedings{Arjovsky2017,
author = {Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'{e}}on},
booktitle = {International Conference on Machine Learning (ICML)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Arjovsky, Chintala, Bottou - Unknown - Wasserstein GAN(2).pdf:pdf},
title = {{Wasserstein GAN}},
url = {https://arxiv.org/pdf/1701.07875.pdf},
year = {2017}
}
@article{Kalakrishnan2011,
abstract = {We present a control architecture for fast quadruped locomotion over rough terrain. We approach the problem by decomposing it into many sub-systems, in which we apply state-of-the-art learning, planning, optimization, and control techniques to achieve robust, fast locomotion. Unique features of our control strategy include: (1) a system that learns optimal foothold choices from expert demonstration using terrain templates, (2) a body trajectory optimizer based on the Zero-Moment Point (ZMP) stability criterion, and (3) a floating-base inverse dynamics controller that, in conjunction with force control, allows for robust, compliant locomotion over unperceived obstacles. We evaluate the performance of our controller by testing it on the LittleDog quadruped robot, over a wide variety of rough terrains of varying difficulty levels. The terrain that the robot was tested on includes rocks, logs, steps, barriers, and gaps, with obstacle sizes up to the leg length of the robot. We demonstrate the generalization ability of this controller by presenting results from testing performed by an independent external test team on terrain that has never been shown to us. {\textcopyright} 2011 The Author(s).},
author = {Kalakrishnan, Mrinal and Buchli, Jonas and Pastor, Peter and Mistry, Michael and Schaal, Stefan},
doi = {10.1177/0278364910388677},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Kalakrishnan et al. - 2011 - Learning, planning, and control for quadruped locomotion over challenging terrain.pdf:pdf;:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Kalakrishnan et al. - 2011 - Learning, planning, and control for quadruped locomotion over challenging terrain(2).pdf:pdf},
isbn = {0278-3649},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {floating base inverse,locomotion planning and control,quadruped locomotion,template learning,zmp optimization},
number = {2},
pages = {236--258},
title = {{Learning, planning, and control for quadruped locomotion over challenging terrain}},
url = {https://www.researchgate.net/profile/Michael{\_}Mistry/publication/220122611{\_}Learning{\_}planning{\_}and{\_}control{\_}for{\_}quadruped{\_}locomotion{\_}over{\_}challenging{\_}terrain/links/02bfe50ca950d456c2000000.pdf},
volume = {30},
year = {2011}
}
@inproceedings{Lambert2018,
abstract = {We consider the problems of learning forward models that map state to high-dimensional images and inverse models that map high-dimensional images to state in robotics. Specifically, we present a perceptual model for generating video frames from state with deep networks, and provide a framework for its use in tracking and prediction tasks. We show that our proposed model greatly outperforms standard deconvolutional methods and GANs for image generation, producing clear, photo-realistic images. We also develop a convolutional neural network model for state estimation and compare the result to an Extended Kalman Filter to estimate robot trajectories. We validate all models on a real robotic system.},
archivePrefix = {arXiv},
arxivId = {1710.11311},
author = {Lambert, Alexander and Shaban, Amirreza and Raj, Amit and Liu, Zhen and Boots, Byron},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
eprint = {1710.11311},
file = {:Users/ashvin/Documents/papers/1710.11311.pdf:pdf},
isbn = {9781538630808},
title = {{Deep Forward and Inverse Perceptual Models for Tracking and Prediction}},
url = {http://arxiv.org/abs/1710.11311},
year = {2018}
}
@article{Wayne2018,
abstract = {Animals execute goal-directed behaviours despite the limited range and scope of their sensors. To cope, they explore environments and store memories maintaining estimates of important information that is not presently available. Recently, progress has been made with artificial intelligence (AI) agents that learn to perform tasks from sensory input, even at a human level, by merging reinforcement learning (RL) algorithms with deep neural networks, and the excitement surrounding these results has led to the pursuit of related ideas as explanations of non-human animal learning. However, we demonstrate that contemporary RL algorithms struggle to solve simple tasks when enough information is concealed from the sensors of the agent, a property called "partial observability". An obvious requirement for handling partially observed tasks is access to extensive memory, but we show memory is not enough; it is critical that the right information be stored in the right format. We develop a model, the Memory, RL, and Inference Network (MERLIN), in which memory formation is guided by a process of predictive modeling. MERLIN facilitates the solution of tasks in 3D virtual reality environments for which partial observability is severe and memories must be maintained over long durations. Our model demonstrates a single learning agent architecture that can solve canonical behavioural tasks in psychology and neurobiology without strong simplifying assumptions about the dimensionality of sensory input or the duration of experiences.},
archivePrefix = {arXiv},
arxivId = {1803.10760},
author = {Wayne, Greg and Hung, Chia-Chun and Amos, David and Mirza, Mehdi and Ahuja, Arun and Grabska-Barwinska, Agnieszka and Rae, Jack and Mirowski, Piotr and Leibo, Joel Z. and Santoro, Adam and Gemici, Mevlana and Reynolds, Malcolm and Harley, Tim and Abramson, Josh and Mohamed, Shakir and Rezende, Danilo and Saxton, David and Cain, Adam and Hillier, Chloe and Silver, David and Kavukcuoglu, Koray and Botvinick, Matt and Hassabis, Demis and Lillicrap, Timothy},
eprint = {1803.10760},
file = {:Users/ashvin/Documents/papers/1803.10760.pdf:pdf},
title = {{Unsupervised Predictive Memory in a Goal-Directed Agent}},
url = {http://arxiv.org/abs/1803.10760},
year = {2018}
}
@article{Mirowski2018,
abstract = {Navigating through unstructured environments is a basic capability of intelligent creatures, and thus is of fundamental interest in the study and development of artificial intelligence. Long-range navigation is a complex cognitive task that relies on developing an internal representation of space, grounded by recognisable landmarks and robust visual processing, that can simultaneously support continuous self-localisation ("I am here") and a representation of the goal ("I am going there"). Building upon recent research that applies deep reinforcement learning to maze navigation problems, we present an end-to-end deep reinforcement learning approach that can be applied on a city scale. Recognising that successful navigation relies on integration of general policies with locale-specific knowledge, we propose a dual pathway architecture that allows locale-specific features to be encapsulated, while still enabling transfer to multiple cities. We present an interactive navigation environment that uses Google StreetView for its photographic content and worldwide coverage, and demonstrate that our learning method allows agents to learn to navigate multiple cities and to traverse to target destinations that may be kilometres away. A video summarizing our research and showing the trained agent in diverse city environments as well as on the transfer task is available at: https://sites.google.com/view/streetlearn.},
archivePrefix = {arXiv},
arxivId = {1804.00168},
author = {Mirowski, Piotr and Grimes, Matthew Koichi and Malinowski, Mateusz and Hermann, Karl Moritz and Anderson, Keith and Teplyashin, Denis and Simonyan, Karen and Kavukcuoglu, Koray and Zisserman, Andrew and Hadsell, Raia},
eprint = {1804.00168},
file = {:Users/ashvin/Documents/papers/1804.00168.pdf:pdf},
title = {{Learning to Navigate in Cities Without a Map}},
url = {http://arxiv.org/abs/1804.00168},
year = {2018}
}
@article{Hunt1992,
abstract = {This paper focuses on the promise of artificial neural networks in the realm of modelling, identification and control of nonlinear systems. The basic ideas and techniques of artificial neural networks are presented in language and notation familiar to control engineers. Applications of a variety of neural network architectures in control are surveyed. We explore the links between the fields of control science and neural networks in a unified presentation and identify key areas for future research. {\textcopyright} 1992.},
archivePrefix = {arXiv},
arxivId = {0005-1098/92 {\$}5.00 + 0.00},
author = {Hunt, K. J. and Sbarbaro, D. and Zbikowski, R. and Gawthrop, P. J.},
doi = {10.1016/0005-1098(92)90053-I},
eprint = {92 {\$}5.00 + 0.00},
file = {:Users/ashvin/Documents/papers/1-s2.0-000510989290053I-main.pdf:pdf},
isbn = {ISSN{\~{}}{\~{}}0005-1098},
issn = {00051098},
journal = {Automatica},
keywords = {Neural networks,nonlinear control systems,nonlinear systems modelling,systems identification},
number = {6},
pages = {1083--1112},
primaryClass = {0005-1098},
title = {{Neural networks for control systems-A survey}},
volume = {28},
year = {1992}
}
@inproceedings{kaelbling1993goals,
abstract = {Temporal diierence methods solve the tem-poral credit assignment problem for reinforce-ment learning. An important subproblem of general reinforcement learning is learning to achieve dynamic goals. Although existing tem-poral diierence methods, such as Q learning, can be applied to this problem, they do not take advantage of its special structure. This pa-per presents the DG-learning algorithm, which learns eeciently to achieve dynamically chang-ing goals and exhibits good knowledge transfer between goals. In addition, this paper shows how traditional relaxation techniques can be applied to the problem. Finally, experimen-tal results are given that demonstrate the su-periority of DG learning over Q learning in a moderately large, synthetic, non-deterministic domain.},
author = {Kaelbling, L P},
booktitle = {IJCAI-93. Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence},
file = {:Users/ashvin/Documents/papers/3f70f383007a946448122b75918e3a9d6682.pdf:pdf},
keywords = {learning (artificial intelligence),temporal reason},
pages = {1094 -- 8},
title = {{Learning to achieve goals}},
volume = {vol.2},
year = {1993}
}
@article{Cheney2013,
abstract = {In 1994 Karl Sims showed that computational evolution can produce interesting morphologies that resemble natural or-ganisms. Despite nearly two decades of work since, evolved morphologies are not obviously more complex or natural, and the field seems to have hit a complexity ceiling. One hypothesis for the lack of increased complexity is that most work, including Sims', evolves morphologies composed of rigid elements, such as solid cubes and cylinders, limiting the design space. A second hypothesis is that the encod-ings of previous work have been overly regular, not allow-ing complex regularities with variation. Here we test both hypotheses by evolving soft robots with multiple materials and a powerful generative encoding called a compositional pattern-producing network (CPPN). Robots are selected for locomotion speed. We find that CPPNs evolve faster robots than a direct encoding and that the CPPN morphologies appear more natural. We also find that locomotion per-formance increases as more materials are added, that di-versity of form and behavior can be increased with diâ†µer-ent cost functions without stifling performance, and that organisms can be evolved at diâ†µerent levels of resolution. These findings suggest the ability of generative soft-voxel systems to scale towards evolving a large diversity of com-plex, natural, multi-material creatures. Our results suggest that future work that combines the evolution of CPPN-encoded soft, multi-material robots with modern diversity-encouraging techniques could finally enable the creation of creatures far more complex and interesting than those pro-duced by Sims nearly twenty years ago.},
author = {Cheney, Nick and Maccurdy, Robert and Clune, Jeff and Lipson, Hod},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Cheney et al. - 2013 - Unshackling Evolution Evolving Soft Robots with Multiple Materials and a Powerful Generative Encoding.pdf:pdf},
keywords = {Algorithms,CPPN-NEAT,Categories and Subject Descriptors,Design,Evolving Morphologies,Experimentation Keywords,Generative Encodings,Genetic Algorithms,HyperNEAT,I211 [Distributed Artificial Intelligence],Intelligent Agents General Terms,Soft-Robotics},
title = {{Unshackling Evolution: Evolving Soft Robots with Multiple Materials and a Powerful Generative Encoding}},
url = {http://jeffclune.com/publications/2013{\_}Softbots{\_}GECCO.pdf},
year = {2013}
}
@article{Levine2018,
abstract = {The framework of reinforcement learning or optimal control provides a mathe-matical formalization of intelligent decision making that is powerful and broadly applicable. While the general form of the reinforcement learning problem enables effective reasoning about uncertainty, the connection between reinforcement learn-ing and inference in probabilistic models is not immediately obvious. However, such a connection has considerable value when it comes to algorithm design: for-malizing a problem as probabilistic inference in principle allows us to bring to bear a wide array of approximate inference tools, extend the model in flexible and powerful ways, and reason about compositionality and partial observability. In this article, we will discuss how a generalization of the reinforcement learning or optimal control problem, which is sometimes termed maximum entropy rein-forcement learning, is equivalent to exact probabilistic inference in the case of deterministic dynamics, and variational inference in the case of stochastic dynam-ics. We will present a detailed derivation of this framework, overview prior work that has drawn on this and related ideas to propose new reinforcement learning and control algorithms, and describe perspectives on future research.},
archivePrefix = {arXiv},
arxivId = {arXiv:1805.00909v2},
author = {Levine, Sergey},
eprint = {arXiv:1805.00909v2},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Levine - 2018 - Reinforcement Learning and Control as Probabilistic Inference Tutorial and Review.pdf:pdf},
title = {{Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review}},
url = {https://arxiv.org/pdf/1805.00909.pdf},
year = {2018}
}
@article{Lesort2018,
abstract = {Representation learning algorithms are designed to learn abstract features that character-ize data. State representation learning (SRL) focuses on a particular kind of representation learning where learned features are in low dimension, evolve through time, and are influenced by actions of an agent. As the representation learned captures the variation in the environment generated by agents, this kind of representation is particularly suitable for robotics and con-trol scenarios. In particular, the low dimension helps to overcome the curse of dimensionality, provides easier interpretation and utilization by humans and can help improve performance and speed in policy learning algorithms such as reinforcement learning. This survey aims at covering the state-of-the-art on state representation learning in the most recent years. It reviews different SRL methods that involve interaction with the envi-ronment, their implementations and their applications in robotics control tasks (simulated or real). In particular, it highlights how generic learning objectives are differently exploited in the reviewed algorithms. Finally, it discusses evaluation methods to assess the representation learned and summarizes current and future lines of research.},
archivePrefix = {arXiv},
arxivId = {arXiv:1802.04181v1},
author = {Lesort, Timoth{\'{e}}e and D{\'{i}}az-Rodr{\'{i}}guez, Natalia and Goudou, Jean-Fran{\c{c}}ois and Filliat, David},
eprint = {arXiv:1802.04181v1},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Lesort et al. - 2018 - State Representation Learning for Control An Overview.pdf:pdf},
keywords = {Disentanglement of control factors,Learning Disentangled Representations,Low Dimensional Embedding Learning,Reinforcement Learn-ing,Robotics,State Representation Learning},
title = {{State Representation Learning for Control: An Overview}},
url = {https://arxiv.org/pdf/1802.04181.pdf},
year = {2018}
}
@article{Konda,
abstract = {We present an approach to predicting velocity and direction changes from visual information (" visual odom-etry ") using an end-to-end, deep learning-based architecture. The architecture uses a single type of compu-tational module and learning rule to extract visual motion, depth, and finally odometry information from the raw data. Representations of depth and motion are extracted by detecting synchrony across time and stereo channels using network layers with multiplicative interactions. The extracted representations are turned into information about changes in velocity and direction using a convolutional neural network. Preliminary results show that the architecture is capable of learning the resulting mapping from video to egomotion.},
author = {Konda, Kishore and Memisevic, Roland},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Konda, Memisevic - Unknown - Learning visual odometry with a convolutional network.pdf:pdf},
keywords = {Convolutional networks,Motion,Stereo,Visual odometry},
title = {{Learning visual odometry with a convolutional network}},
url = {http://www.iro.umontreal.ca/{~}memisevr/pubs/VISAPP2015.pdf}
}
@article{Coates,
abstract = {We consider the problem of learning to follow a desired trajectory when given a small num-ber of demonstrations from a sub-optimal ex-pert. We present an algorithm that (i) ex-tracts theâ€”initially unknownâ€”desired tra-jectory from the sub-optimal expert's demon-strations and (ii) learns a local model suit-able for control along the learned trajectory. We apply our algorithm to the problem of autonomous helicopter flight. In all cases, the autonomous helicopter's performance ex-ceeds that of our expert helicopter pilot's demonstrations. Even stronger, our results significantly extend the state-of-the-art in au-tonomous helicopter aerobatics. In particu-lar, our results include the first autonomous tic-tocs, loops and hurricane, vastly superior performance on previously performed aero-batic maneuvers (such as in-place flips and rolls), and a complete airshow, which requires autonomous transitions between these and various other maneuvers.},
author = {Coates, Adam and Abbeel, Pieter and Ng, Andrew Y},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Coates, Abbeel, Ng - Unknown - Learning for Control from Multiple Demonstrations.pdf:pdf},
title = {{Learning for Control from Multiple Demonstrations}},
url = {https://www-cs.stanford.edu/people/ang/papers/icml08-LearningForControlFromMultipleDemonstrations.pdf}
}
@article{we2018mpo,
author = {We, Bstract and Optimisation, Policy and Model, Ntroduction and Optimisation, Trust-region Policy and Optimisation, Policy and Deterministic, Deep and Gradient, Policy and Gradient, Stochastic Value and Function, Normalized Advantage and Maximisation, Expectation},
file = {:Users/ashvin/Downloads/mapo.pdf:pdf},
journal = {International Conference on Learning Representations (ICLR)},
pages = {1--19},
title = {{Maximum a Posteriori Policy Optimisation}},
year = {2018}
}
@article{Silver,
abstract = {One of the key challenges of artificial intelli-gence is to learn models that are effective in the context of planning. In this document we intro-duce the predictron architecture. The predictron consists of a fully abstract model, represented by a Markov reward process, that can be rolled for-ward multiple " imagined " planning steps. Each forward pass of the predictron accumulates in-ternal rewards and values over multiple plan-ning depths. The predictron is trained end-to-end so as to make these accumulated values ac-curately approximate the true value function. We applied the predictron to procedurally generated random mazes and a simulator for the game of pool. The predictron yielded significantly more accurate predictions than conventional deep neu-ral network architectures.},
author = {Silver, David and Hasselt, Hado Van and Hessel, Matteo and Schaul, Tom and Guez, Arthur and Harley, Tim and Dulac-Arnold, Gabriel and Reichert, David and Rabinowitz, Neil and Barreto, Andre and Degris, Thomas},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Silver et al. - Unknown - The Predictron End-To-End Learning and Planning.pdf:pdf},
title = {{The Predictron: End-To-End Learning and Planning}},
url = {https://arxiv.org/pdf/1612.08810.pdf}
}
@article{Ritchie,
abstract = {Probabilistic programming languages (PPLs) are a powerful modeling tool, able to represent any computable probability distribution. Unfortunately, probabilistic program inference is often intractable, and existing PPLs mostly rely on expensive, approximate sampling-based methods. To alleviate this problem, one could try to learn from past inferences, so that future inferences run faster. This strategy is known as amortized inference; it has recently been applied to Bayesian net-works [28, 22] and deep generative models [20, 15, 24]. This paper proposes a system for amortized inference in PPLs. In our system, amortization comes in the form of a parameterized guide program. Guide programs have similar structure to the original program, but can have richer data flow, including neural network components. These networks can be optimized so that the guide approximately samples from the posterior distribution defined by the original program. We present a flexible interface for defining guide programs and a stochastic gradient-based scheme for optimizing guide parameters, as well as some preliminary results on automatically deriving guide programs. We explore in detail the common machine learning pattern in which a 'local' model is specified by 'global' random values and used to generate independent observed data points; this gives rise to amortized local inference supporting global model learning.},
author = {Ritchie, Daniel and Horsfall, Paul and Goodman, Noah D},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Ritchie, Horsfall, Goodman - Unknown - Deep Amortized Inference for Probabilistic Programs.pdf:pdf},
title = {{Deep Amortized Inference for Probabilistic Programs}},
url = {https://arxiv.org/pdf/1610.05735.pdf}
}
@article{Blei2017,
abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
archivePrefix = {arXiv},
arxivId = {arXiv:1601.00670v7},
author = {Blei, David M and Kucukelbir, Alp and Mcauliffe, Jon D},
eprint = {arXiv:1601.00670v7},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Blei, Kucukelbir, Mcauliffe - 2017 - Variational Inference A Review for Statisticians.pdf:pdf},
keywords = {Algorithms,Computationally Intensive Methods,Statistical Computing},
title = {{Variational Inference: A Review for Statisticians}},
url = {https://arxiv.org/pdf/1601.00670.pdf},
year = {2017}
}
@article{Hashim2017,
abstract = {{\textcopyright} 2017 Australian and New Zealand Society of Cardiac and Thoracic Surgeons (ANZSCTS) and the Cardiac Society of Australia and New Zealand (CSANZ). The revision of an internal mammary artery graft anastomosis because of a technical error can be time consuming and complicated and may lead to complications. Here, we describe the technical details and our early experience of using a standard transit-time flowmeter to exclude technical errors and facilitate rapid decision making for anastomosis revision in an arrested heart during aortic cross-clamping in the absence of ultrasound guidance.},
author = {Hashim, S.A. and Amin, M.A. and Nair, A. and {Raja Mokhtar}, R.A. and Krishnasamy, S. and Cheng, K.},
doi = {10.1016/j.hlc.2017.11.011},
issn = {14442892},
journal = {Heart Lung and Circulation},
keywords = {CABG device,Coronary artery bypass graft},
title = {{A Flowmeter Technique to Exclude Internal Mammary Artery Anastomosis Error in an Arrested Heart}},
year = {2017}
}
@article{Hestness,
abstract = {Deep learning (DL) creates impactful advances following a virtuous recipe: model architecture search, creating large training data sets, and scaling computation. It is widely believed that growing training sets and models should improve accuracy and result in better products. As DL application domains grow, we would like a deeper understanding of the relationships between training set size, computational scale, and model accuracy improvements to advance the state-of-the-art. This paper presents a large scale empirical characterization of generalization error and model size growth as training sets grow. We introduce a methodology for this measurement and test four machine learning domains: machine translation, language modeling, image processing, and speech recognition. Our empirical results show power-law generalization error scaling across a breadth of factors, resulting in power-law exponentsâ€”the "steepness" of the learning curveâ€”yet to be explained by theoretical work. Further, model improvements only shift the error but do not appear to affect the power-law exponent. We also show that model size scales sublinearly with data size. These scaling relationships have significant implications on deep learning research, practice, and systems. They can assist model debugging, setting accuracy targets, and decisions about data set growth. They can also guide computing system design and underscore the importance of continued computational scaling.},
author = {Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and {Mostofa Ali Patwary}, Md and Yang, Yang and Zhou, Yanqi and Research, Baidu},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Hestness et al. - Unknown - DEEP LEARNING SCALING IS PREDICTABLE, EMPIRICALLY.pdf:pdf},
title = {{Deep Learning Scaling Is Predictable, Empirically}},
url = {https://arxiv.org/pdf/1712.00409.pdf}
}
@article{Legg2007,
abstract = {A fundamental problem in artificial intelligence is that nobody really knows what intelligence is. The problem is especially acute when we need to consider artificial systems which are significantly different to humans. In this paper we approach this problem in the following way: We take a number of well known informal definitions of human intelligence that have been given by experts, and extract their essential features. These are then mathematically formalised to produce a general measure of intelligence for arbitrary machines. We believe that this equation formally captures the concept of machine intelligence in the broadest reasonable sense. We then show how this formal definition is related to the theory of universal optimal learning agents. Finally, we survey the many other tests and definitions of intelligence that have been proposed for machines.},
archivePrefix = {arXiv},
arxivId = {arXiv:0712.3329v1},
author = {Legg, Shane and Hutter, Marcus},
eprint = {arXiv:0712.3329v1},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Legg, Hutter - 2007 - Universal Intelligence A Definition of Machine Intelligence.pdf:pdf},
keywords = {AIXI,Complexity theory,Intel-ligence definitions,Intelligence,Intelligence tests,Theoretical foundations,Turing test},
title = {{Universal Intelligence: A Definition of Machine Intelligence}},
url = {www.vetta.org/shane www.hutter1.net},
year = {2007}
}
@article{Behrouzi,
abstract = {Most algorithms for reinforcement learning work by estimat-ing action-value functions. Here we present a method that uses Lagrange multipliers, the costate equation, and multi-layer neural networks to compute policy gradients. We show that this method can find solutions to time-optimal control problems, driving nonlinear mechanical systems quickly to a target configuration. On these tasks its performance is com-parable to that of deep deterministic policy gradient, a recent action-value method. Research in reinforcement learning has shown the effective-ness of algorithms based on action-value functions (also known as Q-functions) or closely related quantities such as value-or advantage functions [1-6]. Here we present a dif-ferent approach based on Lagrange multipliers and the co-state equation [7, 8], which may work better in some tasks. As usual in reinforcement learning, the setting involves an agent embedded in an environment which evolves through time according to a rule or function f, called the environment dynamics. For instance, the agent might be a brain and the environment its body, in which case f might represent the mechanics of that body. If st is the state of the environment at time t, and at is the action taken by the agent at this time (say, the motor commands issued by the brain), then at the next time step, t + âˆ†t, the state takes a new value},
author = {Behrouzi, Bita and Tweed, Douglas},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Behrouzi, Tweed - Unknown - Lagrange policy gradient.pdf:pdf},
title = {{Lagrange policy gradient}},
url = {https://arxiv.org/pdf/1711.05817.pdf}
}
@article{Klein,
author = {Klein, Dan},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Klein - Unknown - Lagrange Multipliers without Permanent Scarring.pdf:pdf},
title = {{Lagrange Multipliers without Permanent Scarring}},
url = {https://people.eecs.berkeley.edu/{~}klein/papers/lagrange-multipliers.pdf}
}
@article{I,
abstract = {Theories of reward learning in neuroscience have focused on two families of algorithms, thought to capture deliberative vs. habitual choice. " Model-based " algorithms compute the value of candidate actions from scratch, whereas " model-free " algorithms make choice more efficient but less flexible by storing pre-computed action values. We examine an intermediate algorithmic family, the successor representation (SR), which balances flexibility and efficiency by storing partially computed action values: predictions about future events. These pre-computation strategies differ in how they update their choices following changes in a task. SR's reliance on stored predictions about future states predicts a unique signature of insensitivity to changes in the task's sequence of events, but flexible adjustment following changes to rewards. We provide evidence for such differential sensitivity in two behavioral studies with humans. These results suggest that the SR is a computational substrate for semi-flexible choice in humans, introducing a subtler, more cognitive notion of habit.},
author = {I, Momennejad and Em, Russek and Jh, Cheong and Mm, Botvinick and Nd, Daw and Sj, Gershman},
doi = {10.1101/083824},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/I et al. - Unknown - The successor representation in human reinforcement learning.pdf:pdf},
keywords = {Decision making,Human behavior,Model-based,Planning,Predictive representation Acknowledgements,Reinforcement learning,Retrospective revaluation,Successor representation},
title = {{The successor representation in human reinforcement learning}},
url = {https://www.biorxiv.org/content/biorxiv/early/2017/07/04/083824.full.pdf}
}
@article{Dayan,
abstract = {Estimation of returns over time, the focus of temporal difference (TD) algorithms, imposes particular constraints on good function approximators or representations. Appropriate generalisation between states is determined by how similar their succes-sors are, and representations should follow suit. This paper shows how TD machinery can be used to learn such representations, and illustrates, using a navigation task, the appropriately distributed nature of the result.},
author = {Dayan, Peter},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Dayan - Unknown - Improving Generalisation for Temporal Difference Learning The Successor Representation.pdf:pdf},
title = {{Improving Generalisation for Temporal Difference Learning: The Successor Representation}},
url = {http://www.gatsby.ucl.ac.uk/{~}dayan/papers/d93b.pdf}
}
@article{Norouzi2017,
abstract = {A key problem in structured output prediction is direct optimization of the task reward function that matters for test evaluation. This paper presents a simple and computationally efficient approach to incorporate task reward into a maximum likelihood framework. By establishing a link between the log-likelihood and ex-pected reward objectives, we show that an optimal regularized expected reward is achieved when the conditional distribution of the outputs given the inputs is pro-portional to their exponentiated scaled rewards. Accordingly, we present a frame-work to smooth the predictive probability of the outputs using their corresponding rewards. We optimize the conditional log-probability of augmented outputs that are sampled proportionally to their exponentiated scaled rewards. Experiments on neural sequence to sequence models for speech recognition and machine transla-tion show notable improvements over a maximum likelihood baseline by using reward augmented maximum likelihood (RAML), where the rewards are defined as the negative edit distance between the outputs and the ground truth labels.},
archivePrefix = {arXiv},
arxivId = {arXiv:1609.00150v3},
author = {Norouzi, Mohammad and Bengio, Samy and Chen, Zhifeng and Jaitly, Navdeep and Schuster, Mike and Wu, Yonghui and Schuurmans, Dale},
eprint = {arXiv:1609.00150v3},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Norouzi et al. - 2017 - Reward Augmented Maximum Likelihood for Neural Structured Prediction.pdf:pdf},
keywords = {()},
title = {{Reward Augmented Maximum Likelihood for Neural Structured Prediction}},
url = {https://arxiv.org/pdf/1609.00150.pdf},
year = {2017}
}
@article{Petersa,
abstract = {Many robot control problems of practical im-portance, including operational space con-trol, can be reformulated as immediate re-ward reinforcement learning problems. How-ever, few of the known optimization or re-inforcement learning algorithms can be used in online learning control for robots, as they are either prohibitively slow, do not scale to interesting domains of complex robots, or re-quire trying out policies generated by random search, which are infeasible for a physical sys-tem. Using a generalization of the EM-base reinforcement learning framework suggested by Dayan {\&} Hinton, we reduce the prob-lem of learning with immediate rewards to a reward-weighted regression problem with an adaptive, integrated reward transforma-tion for faster convergence. The resulting al-gorithm is efficient, learns smoothly without dangerous jumps in solution space, and works well in applications of complex high degree-of-freedom robots.},
author = {Peters, Jan and Schaal, Stefan},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Peters, Schaal - Unknown - Reinforcement Learning by Reward-weighted Regression for Operational Space Control.pdf:pdf},
title = {{Reinforcement Learning by Reward-weighted Regression for Operational Space Control}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.79.6266{\&}rep=rep1{\&}type=pdf}
}
@article{Lecun2006,
abstract = {To appear in " Predicting Structured Data " , G. Bakir, T. Hofman, B. Sch{\"{o}}lkopf, A. Smola, B. Taskar (eds) MIT Press, 2006 Abstract Energy-Based Models (EBMs) capture dependencies between variables by as-sociating a scalar energy to each configuration of the variables. Inference consists in clamping the value of observed variables and finding configurations of the re-maining variables that minimize the energy. Learning consists in finding an energy function in which observed configurations of the variables are given lower energies than unobserved ones. The EBM approach provides a common theoretical frame-work for many learning models, including traditional discriminative and genera-tive approaches, as well as graph-transformer networks, conditional random fields, maximum margin Markov networks, and several manifold learning methods. Probabilistic models must be properly normalized, which sometimes requires evaluating intractable integrals over the space of all possible variable configura-tions. Since EBMs have no requirement for proper normalization, this problem is naturally circumvented. EBMs can be viewed as a form of non-probabilistic factor graphs, and they provide considerably more flexibility in the design of architec-tures and training criteria than probabilistic approaches.},
author = {Lecun, Yann and Chopra, Sumit and Hadsell, Raia and Ranzato, Marc Aurelio and Huang, Fu Jie},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Lecun et al. - 2006 - A Tutorial on Energy-Based Learning.pdf:pdf},
title = {{A Tutorial on Energy-Based Learning}},
url = {http://yann.lecun.com},
year = {2006}
}
@article{Anderson1999,
abstract = {History and definition: The term " Monte Carlo " was apparently first used by Ulam and von Neumann as a Los Alamos code word for the stochastic simulations they applied to building better atomic bombs. Their methods, involving the laws of chance, were aptly named after the inter-national gaming destination; the moniker stuck and soon after the War a wide range of sticky problems yielded to the new techniques. Despite the widespread use of the methods, and numerous descriptions of them in articles and monographs, it is virtually impossible to find a succint defini-tion of " Monte Carlo method " in the literature. Perhaps this is owing to the intuitive nature of the topic which spawns many definitions by way of specific examples. Some authors prefer to use the term " stochastic simulation " for almost everything, reserving " Monte Carlo " only for Monte Carlo Integration and Monte Carlo Tests (cf. Ripley 1987). Others seem less concerned about blurring the distinction between simulation studies and Monte Carlo methods. Be that as it may, a suitable definition can be good to have, if for nothing other than to avoid the awkwardness of trying to define the Monte Carlo method by appealing to a whole bevy of examples of it. Since I am (so Elizabeth claims!) unduly influenced by my advisor's ways of thinking, I like to define Monte Carlo in the spirit of definitions she has used before. In particular, I use: Definition: Monte Carlo is the art of approximating an expectation by the sample mean of a function of simulated random variables. We will find that this definition is broad enough to cover everything that has been called Monte Carlo, and yet makes clear its essence in very familiar terms: Monte Carlo is about invoking laws of large numbers to approximate expectations. 1 While most Monte Carlo simulations are done by computer today, there were many applications of Monte Carlo methods using coin-flipping, card-drawing, or needle-tossing (rather than computer-generated pseudo-random numbers) as early as the turn of the centuryâ€”long before the name Monte Carlo arose. In more mathematical terms: Consider a (possibly multidimensional) random variable X having probability mass function or probability density function f X (x) which is greater than zero on a set of values X . Then the expected value of a function g of X is},
author = {Anderson, Eric C},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Anderson - 1999 - Lecture Notes for Stat 578C Monte Carlo Methods and Importance Sampling.pdf:pdf},
title = {{Lecture Notes for Stat 578C Monte Carlo Methods and Importance Sampling}},
url = {http://ib.berkeley.edu/labs/slatkin/eriq/classes/guest{\_}lect/mc{\_}lecture{\_}notes.pdf},
year = {1999}
}
@article{Bengioa,
abstract = {Neuroscientists have long criticised deep learn-ing algorithms as incompatible with current knowledge of neurobiology. We explore more bi-ologically plausible versions of deep representa-tion learning, focusing here mostly on unsuper-vised learning but developing a learning mecha-nism that could account for supervised, unsuper-vised and reinforcement learning. The starting point is that the basic learning rule believed to govern synaptic weight updates (Spike-Timing-Dependent Plasticity) arises out of a simple up-date rule that makes a lot of sense from a machine learning point of view and can be interpreted as gradient descent on some objective function so long as the neuronal dynamics push firing rates towards better values of the objective func-tion (be it supervised, unsupervised, or reward-driven). The second main idea is that this corre-sponds to a form of the variational EM algorithm, i.e., with approximate rather than exact posteri-ors, implemented by neural dynamics. Another contribution of this paper is that the gradients re-quired for updating the hidden states in the above variational interpretation can be estimated using an approximation that only requires propagating activations forward and backward, with pairs of layers learning to form a denoising auto-encoder. Finally, we extend the theory about the proba-bilistic interpretation of auto-encoders to justify improved sampling schemes based on the gener-ative interpretation of denoising auto-encoders, and we validate all these ideas on generative learning tasks.},
author = {Bengio, Yoshua and Lee, Dong-Hyun and Bornschein, Jorg and Mesnard, Thomas and Lin, Zhouhan},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Bengio et al. - Unknown - Towards Biologically Plausible Deep Learning.pdf:pdf},
title = {{Towards Biologically Plausible Deep Learning}},
url = {https://arxiv.org/pdf/1502.04156.pdf}
}
@article{Rivest,
abstract = {Successful application of reinforcement learning algorithms often involves considerable hand-crafting of the necessary non-linear features to reduce the complexity of the value functions and hence to promote convergence of the algorithm. In contrast, the human brain readily and autonomously finds the complex features when provided with sufficient training. Recent work in machine learning and neurophysiology has demonstrated the role of the basal ganglia and the frontal cortex in mammalian reinforcement learning. This paper develops and explores new reinforcement learning algorithms inspired by neurological evidence that provides potential new approaches to the feature construction problem. The algorithms are compared and evaluated on the Acrobot task.},
author = {Rivest, Fran{\c{c}}ois and Bengio, Yoshua and Kalaska, John},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Rivest, Bengio, Kalaska - Unknown - Brain Inspired Reinforcement Learning.pdf:pdf},
title = {{Brain Inspired Reinforcement Learning}},
url = {https://papers.nips.cc/paper/2749-brain-inspired-reinforcement-learning.pdf}
}
@article{Dosovitskiy,
abstract = {We present an approach to sensorimotor control in immersive environments. Our approach utilizes a high-dimensional sensory stream and a lower-dimensional measurement stream. The cotemporal structure of these streams provides a rich supervisory signal, which enables training a sensorimotor control model by in-teracting with the environment. The model is trained using supervised learning techniques, but without extraneous supervision. It learns to act based on raw sen-sory input from a complex three-dimensional environment. The presented formu-lation enables learning without a fixed goal at training time, and pursuing dynam-ically changing goals at test time. We conduct extensive experiments in three-dimensional simulations based on the classical first-person game Doom. The results demonstrate that the presented approach outperforms sophisticated prior formulations, particularly on challenging tasks. The results also show that trained models successfully generalize across environments and goals. A model trained using the presented approach won the Full Deathmatch track of the Visual Doom AI Competition, which was held in previously unseen environments.},
author = {Dosovitskiy, Alexey and Koltun, Vladlen},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Dosovitskiy, Koltun - Unknown - LEARNING TO ACT BY PREDICTING THE FUTURE.pdf:pdf},
title = {{Learning To Act By Predicting The Future}},
url = {https://arxiv.org/pdf/1611.01779.pdf}
}
@article{Silvera,
author = {Silver, David},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - Lecture 7 Policy Gradient.pdf:pdf},
title = {{Lecture 7: Policy Gradient}},
url = {http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching{\_}files/pg.pdf}
}
@article{Levine,
author = {Levine, Sergey},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Levine - Unknown - Connections Between Inference and Control CS 294-112 Deep Reinforcement Learning.pdf:pdf},
title = {{Connections Between Inference and Control CS 294-112: Deep Reinforcement Learning}},
url = {http://rll.berkeley.edu/deeprlcourse/f17docs/lecture{\_}11{\_}control{\_}and{\_}inference.pdf}
}
@article{Ziebart,
abstract = {The principle of maximum entropy provides a powerful framework for statistical models of joint, conditional, and marginal distribu-tions. However, there are many important distributions with elements of interaction and feedback where its applicability has not been established. This work presents the principle of maximum causal entropyâ€”an approach based on causally conditioned probabilities that can appropriately model the availabil-ity and influence of sequentially revealed side information. Using this principle, we de-rive models for sequential data with revealed information, interaction, and feedback, and demonstrate their applicability for statisti-cally framing inverse optimal control and de-cision prediction tasks.},
author = {Ziebart, Brian D and Bagnell, J Andrew and Dey, Anind K},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Ziebart, Bagnell, Dey - Unknown - Modeling Interaction via the Principle of Maximum Causal Entropy.pdf:pdf},
title = {{Modeling Interaction via the Principle of Maximum Causal Entropy}},
url = {http://www.cs.cmu.edu/{~}bziebart/publications/maximum-causal-entropy.pdf}
}
@article{Bojanowski,
abstract = {Convolutional neural networks provide visual features that perform remarkably well in many computer vision applications. However, train-ing these networks requires significant amounts of supervision. This paper introduces a generic framework to train deep networks, end-to-end, with no supervision. We propose to fix a set of target representations, called Noise As Targets (NAT), and to constrain the deep features to align to them. This domain agnostic approach avoids the standard unsupervised learning issues of triv-ial solutions and collapsing of features. Thanks to a stochastic batch reassignment strategy and a separable square loss function, it scales to mil-lions of images. The proposed approach pro-duces representations that perform on par with state-of-the-art unsupervised methods on Ima-geNet and PASCAL VOC.},
author = {Bojanowski, Piotr and Joulin, Armand},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Bojanowski, Joulin - Unknown - Unsupervised Learning by Predicting Noise.pdf:pdf},
title = {{Unsupervised Learning by Predicting Noise}},
url = {https://arxiv.org/pdf/1704.05310.pdf}
}
@article{Rolf,
abstract = {We discuss the difficulty to learn control skills in high-dimensional domains that can not be exhaustively explored. We show how infant-development can serve as a role-model for highly efficient exploration: infants neither explore exhaustively, nor do they learn very versatile skills right in the beginning. Rather, they attempt goal-directed exploration to achieve feedforward control as a first step without re-quiring full knowledge of the world. This article reviews recent efforts to mimick such pathways by means of " goal babbling " , which have led to a series of algo-rithms that allow for a likewise efficient learning. We show that it permits to learn inverse models from examples even in the presence of non-convex solution sets by utilizing a reward-weighted regression scheme, and that a human-competitive learning speed can be achieved if online learning is applied " in the loop " . Results are verified on the " Bionic Handling Assistant " , a novel bionic robot that instan-tiates a wide spread of problems like high dimensions, non-stationary behavior, highly constrained actuators, sensory noise, and very slow response-behavior.},
author = {Rolf, Matthias and Asada, Minoru},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Rolf, Asada - Unknown - Learning Inverse Models in High Dimensions with Goal Babbling and Reward-Weighted Averaging.pdf:pdf},
title = {{Learning Inverse Models in High Dimensions with Goal Babbling and Reward-Weighted Averaging}},
url = {http://acl.mit.edu/amlsc/files/amlsc13{\_}submission{\_}7.pdf}
}
@article{Kremer,
abstract = {In machine learning, active learning refers to algorithms that autonomously select the data points from which they will learn. There are many data mining appli-cations in which large amounts of unlabeled data are readily available, but labels (e.g., human annotations or results from complex experiments) are costly to ob-tain. In such scenarios, an active learning algorithm aims at identifying data points that, if labeled and used for training, would most improve the learned model. La-bels are then obtained only for the most promising data points. This speeds up learning and reduces labeling costs. Support vector machine (SVM) classifiers are particularly well-suited for active learning due to their convenient mathemat-ical properties. They perform linear classification, typically in a kernel-induced feature space, which makes measuring the distance of a data point from the de-cision boundary straightforward. Furthermore, heuristics can efficiently estimate how strongly learning from a data point influences the current model. This infor-mation can be used to actively select training samples. After a brief introduction to the active learning problem, we discuss different query strategies for select-ing informative data points and review how these strategies give rise to different variants of active learning with SVMs.},
author = {Kremer, Jan and Pedersen, Kim Steenstrup and Igel, Christian},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Kremer, Pedersen, Igel - Unknown - Active Learning with Support Vector Machines.pdf:pdf},
title = {{Active Learning with Support Vector Machines}},
url = {http://image.diku.dk/jank/papers/WIREs2014.pdf}
}
@article{Jonas2017,
author = {Jonas, Eric and Kording, Konrad Paul},
doi = {10.1371/journal.pcbi.1005268},
editor = {Diedrichsen, J{\"{o}}rn},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Jonas, Kording - 2017 - Could a Neuroscientist Understand a Microprocessor.pdf:pdf},
issn = {1553-7358},
journal = {PLOS Computational Biology},
month = {jan},
number = {1},
pages = {e1005268},
publisher = {Public Library of Science},
title = {{Could a Neuroscientist Understand a Microprocessor?}},
url = {http://dx.plos.org/10.1371/journal.pcbi.1005268},
volume = {13},
year = {2017}
}
@article{Czarnecki,
abstract = {At the heart of deep learning we aim to use neural networks as function approxi-mators â€“ training them to produce outputs from inputs in emulation of a ground truth function or data creation process. In many cases we only have access to input-output pairs from the ground truth, however it is becoming more common to have access to derivatives of the target output with respect to the input â€“ for exam-ple when the ground truth function is itself a neural network such as in network compression or distillation. Generally these target derivatives are not computed, or are ignored. This paper introduces Sobolev Training for neural networks, which is a method for incorporating these target derivatives in addition the to target values while training. By optimising neural networks to not only approximate the func-tion's outputs but also the function's derivatives we encode additional information about the target function within the parameters of the neural network. Thereby we can improve the quality of our predictors, as well as the data-efficiency and generalization capabilities of our learned function approximation. We provide theoretical justifications for such an approach as well as examples of empirical evidence on three distinct domains: regression on classical optimisation datasets, distilling policies of an agent playing Atari, and on large-scale applications of synthetic gradients. In all three domains the use of Sobolev Training, employing target derivatives in addition to target values, results in models with higher accuracy and stronger generalisation.},
author = {Czarnecki, Wojciech Marian and Osindero, Simon and Jaderberg, Max and Swirszcz, Grzegorz and Pascanu, Razvan},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Czarnecki et al. - Unknown - Sobolev Training for Neural Networks.pdf:pdf},
title = {{Sobolev Training for Neural Networks}},
url = {https://arxiv.org/pdf/1706.04859.pdf}
}
@article{Odonoghue2017,
abstract = {We consider the exploration/exploitation problem in reinforcement learning. For exploitation, it is well known that the Bellman equation connects the value at any time-step to the expected value at subsequent time-steps. In this paper we consider a similar uncertainty Bellman equation (UBE), which connects the uncertainty at any time-step to the expected uncertainties at subsequent time-steps, thereby extending the potential exploratory benefit of a policy beyond individual time-steps. We prove that the unique fixed point of the UBE yields an upper bound on the variance of the estimated value of any fixed policy. This bound can be much tighter than traditional count-based bonuses that compound standard deviation rather than variance. Importantly, and unlike several existing approaches to optimism, this method scales naturally to large systems with complex generalization. Substituting our UBE-exploration strategy for -greedy improves DQN performance on 51 out of 57 games in the Atari suite.},
author = {{O 'donoghue}, Brendan and Osband, Ian and Munos, Remi and Deepmind, Volodymyr Mnih},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/O 'donoghue et al. - 2017 - The Uncertainty Bellman Equation and Exploration.pdf:pdf},
title = {{The Uncertainty Bellman Equation and Exploration}},
url = {https://arxiv.org/pdf/1709.05380.pdf},
year = {2017}
}
@article{Vasilaki2017,
abstract = {The Epicurean Philosophy is commonly thought as sim-plistic and hedonistic. Here I discuss how this is a misconception and explore its link to Reinforcement Learning. Based on the letters of Epicurus, I construct an objective function for hedonism which turns out to be equivalent of the Reinforcement Learning objective function when omitting the discount factor. I then dis-cuss how Plato and Aristotle 's views that can be also loosely linked to Reinforcement Learning, as well as their weaknesses in relationship to it. Finally, I em-phasise the close affinity of the Epicurean views and the Bellman equation.},
archivePrefix = {arXiv},
arxivId = {arXiv:1710.04582v1},
author = {Vasilaki, Eleni},
eprint = {arXiv:1710.04582v1},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Vasilaki - 2017 - Is Epicurus the father of Reinforcement Learning.pdf:pdf},
title = {{Is Epicurus the father of Reinforcement Learning?}},
url = {https://arxiv.org/pdf/1710.04582.pdf},
year = {2017}
}
@article{Osband,
abstract = {Efficient exploration remains a major challenge for reinforcement learning (RL). Common dithering strategies for exploration, such as -greedy, do not carry out temporally-extended (or deep) exploration; this can lead to exponentially larger data requirements. However, most algorithms for statistically efficient RL are not computationally tractable in complex en-vironments. Randomized value functions offer a promising approach to efficient exploration with generalization, but existing algorithms are not compatible with nonlinearly parameterized value functions. As a first step towards addressing such contexts we develop bootstrapped DQN. We demon-strate that bootstrapped DQN can combine deep exploration with deep neural networks for exponentially faster learning than any dithering strat-egy. In the Arcade Learning Environment bootstrapped DQN substantially improves learning speed and cumulative performance across most games.},
author = {Osband, Ian and Blundell, Charles and Pritzel, Alexander and Roy, Benjamin Van},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Osband et al. - Unknown - Deep Exploration via Bootstrapped DQN.pdf:pdf},
title = {{Deep Exploration via Bootstrapped DQN}},
url = {https://arxiv.org/pdf/1602.04621.pdf}
}
@article{Deepmind,
abstract = {The deep reinforcement learning community has made sev-eral independent improvements to the DQN algorithm. How-ever, it is unclear which of these extensions are complemen-tary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combina-tion provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final perfor-mance. We also provide results from a detailed ablation study that shows the contribution of each component to overall per-formance.},
author = {Deepmind, Matteo Hessel and Deepmind, Joseph Modayil and Van, Hado and Deepmind, Hasselt and Deepmind, Tom Schaul and Ostrovski, Georg and Will, Deepmind and Deepmind, Dabney and Horgan, Dan and Bilal, Deepmind and Deepmind, Piot and Azar, Mohammad and David, Deepmind and Deepmind, Silver},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Deepmind et al. - Unknown - Rainbow Combining Improvements in Deep Reinforcement Learning.pdf:pdf},
title = {{Rainbow: Combining Improvements in Deep Reinforcement Learning}},
url = {https://arxiv.org/pdf/1710.02298.pdf}
}
@article{Byravana,
abstract = {â€” In this work, we present an approach to deep visuomotor control using structured deep dynamics models. Our deep dynamics model, a variant of SE3-Nets, learns a low-dimensional pose embedding for visuomotor control via an encoder-decoder structure. Unlike prior work, our dynamics model is structured: given an input scene, our network ex-plicitly learns to segment salient parts and predict their pose-embedding along with their motion modeled as a change in the pose space due to the applied actions. We train our model using a pair of point clouds separated by an action and show that given supervision only in the form of point-wise data associations between the frames our network is able to learn a meaningful segmentation of the scene along with consistent poses. We further show that our model can be used for closed-loop control directly in the learned low-dimensional pose space, where the actions are computed by minimizing error in the pose space using gradient-based methods, similar to traditional model-based control. We present results on controlling a Baxter robot from raw depth data in simulation and in the real world and compare against two baseline deep networks. Our method runs in real-time, achieves good prediction of scene dynamics and outperforms the baseline methods on multiple control runs. Video results can be found at: https://rse-lab.cs. washington.edu/se3-structured-deep-ctrl/},
author = {Byravan, Arunkumar and Leeb, Felix and Meier, Franziska and Fox, Dieter},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Byravan et al. - Unknown - SE3-Pose-Nets Structured Deep Dynamics Models for Visuomotor Planning and Control.pdf:pdf},
title = {{SE3-Pose-Nets: Structured Deep Dynamics Models for Visuomotor Planning and Control}},
url = {https://arxiv.org/pdf/1710.00489.pdf}
}
@article{Pritzel,
abstract = {Deep reinforcement learning methods attain super-human performance in a wide range of en-vironments. Such methods are grossly inefficient, often taking orders of magnitudes more data than humans to achieve reasonable performance. We propose Neural Episodic Control: a deep rein-forcement learning agent that is able to rapidly assimilate new experiences and act upon them. Our agent uses a semi-tabular representation of the value function: a buffer of past experience con-taining slowly changing state representations and rapidly updated estimates of the value function. We show across a wide range of environments that our agent learns significantly faster than other state-of-the-art, general purpose deep reinforce-ment learning agents.},
author = {Pritzel, Alexander and Uria, Benigno and Srinivasan, Sriram and Com, Adriap@google and Vinyals, Oriol and Hassabis, Demis and Wierstra, Daan and Blundell, Charles},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Pritzel et al. - Unknown - Neural Episodic Control Adr{\`{i}} aPuigdo enech.pdf:pdf},
title = {{Neural Episodic Control Adr{\`{i}} aPuigdo enech}},
url = {https://arxiv.org/pdf/1703.01988.pdf}
}
@article{Venkatraman,
abstract = {Recurrent neural networks (RNNs) are a vital modeling technique that rely on internal states learned indirectly by optimization of a supervised, unsupervised, or reinforcement training loss. RNNs are used to model dynamic processes that are characterized by underlying latent states whose form is often unknown, precluding its analytic representation inside an RNN. In the Predictive-State Representation (PSR) literature, latent state processes are modeled by an internal state representa-tion that directly models the distribution of future observations, and most recent work in this area has relied on explicitly representing and targeting sufficient statis-tics of this probability distribution. We seek to combine the advantages of RNNs and PSRs by augmenting existing state-of-the-art recurrent neural networks with PREDICTIVE-STATE DECODERS (PSDs), which add supervision to the network's internal state representation to target predicting future observations. PSDs are simple to implement and easily incorporated into existing training pipelines via additional loss regularization. We demonstrate the effectiveness of PSDs with experimental results in three different domains: probabilistic filtering, Imitation Learning, and Reinforcement Learning. In each, our method improves statistical performance of state-of-the-art recurrent baselines and does so with fewer iterations and less data.},
author = {Venkatraman, Arun and Rhinehart, Nicholas and Sun, Wen and Pinto, Lerrel and Hebert, Martial and Boots, Byron and Kitani, Kris M and Bagnell, J Andrew},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Venkatraman et al. - Unknown - Predictive-State Decoders Encoding the Future into Recurrent Networks.pdf:pdf},
title = {{Predictive-State Decoders: Encoding the Future into Recurrent Networks}},
url = {https://arxiv.org/pdf/1709.08520.pdf}
}
@inproceedings{kaiser2017rareevents,
author = {Kaiser, Lukasz and Nachum, Ofir and Aurko, Roy and Bengio, Samy},
booktitle = {International Conference on Learning Representations (ICLR)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - full-text(4).pdf:pdf},
title = {full-text},
year = {2017}
}
@article{Bengio,
abstract = {Finding features that disentangle the different causes of variation in real data is a difficult task, that has nonetheless received consider-able attention in static domains like natural images. Interactive environments, in which an agent can deliberately take actions, offer an opportunity to tackle this task better, because the agent can experiment with different actions and observe their effects. We introduce the idea that in interactive environments, latent factors that control the variation in observed data can be identified by figuring out what the agent can control. We propose a naive method to find factors that explain or measure the effect of the actions of a learner, and test it in illustrative experiments.},
author = {Bengio, Emmanuel and Thoma, Valentin and Pineau, Joelle and Precup, Doina and Bengio, Yoshua},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Bengio et al. - Unknown - Independently Controllable Features.pdf:pdf},
keywords = {controllable features Acknowledgements,representation learning},
title = {{Independently Controllable Features}},
url = {https://arxiv.org/pdf/1703.07718.pdf}
}
@article{Ghahramani2015,
abstract = {T he key idea behind the probabilistic framework to machine learn-ing is that learning can be thought of as inferring plausible models to explain observed data. A machine can use such models to make predictions about future data, and take decisions that are rational given these predictions. Uncertainty plays a fundamental part in all of this. Observed data can be consistent with many models, and therefore which model is appropriate, given the data, is uncertain. Similarly, predictions about future data and the future consequences of actions are uncertain. Probability theory provides a framework for modelling uncertainty. This Review starts with an introduction to the probabilistic approach to machine learning and Bayesian inference, and then discusses some of the state-of-the-art advances in the field. Many aspects of learning and intelligence crucially depend on the careful probabilistic representation of uncertainty. Probabilistic approaches have only recently become a main-stream approach to artificial intelligence 1 , robotics 2 and machine learn-ing 3,4 . Even now, there is controversy in these fields about how important it is to fully represent uncertainty. For example, advances using deep neural networks to solve challenging pattern-recognition problems such as speech recognition 5 , image classification 6,7 , and prediction of words in text 8 , do not overtly represent the uncertainty in the structure or parameters of those neural networks. However, my focus will not be on these types of pattern-recognition problems, characterized by the availability of large amounts of data, but on problems for which uncertainty is really a key ingredient, for example where a decision may depend on the amount of uncertainty. I highlight five areas of current research at the frontier of probabilistic machine learning, emphasizing areas that are of broad relevance to sci-entists across many fields: probabilistic programming, which is a general framework for expressing probabilistic models as computer programs and which could have a major impact on scientific modelling; Bayes-ian optimization, which is an approach to globally optimizing unknown functions; probabilistic data compression; automating the discovery of plausible and interpretable models from data; and hierarchical modelling for learning many related models, for example for personalized medicine or recommendation. Although considerable challenges remain, the com-ing decade promises substantial advances in artificial intelligence and machine learning based on the probabilistic framework.},
author = {Ghahramani, Zoubin},
doi = {10.1038/nature14541},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - Probabilistic modelling and representing uncertainty.pdf:pdf},
journal = {Nature},
title = {{Probabilistic modelling and representing uncertainty}},
url = {https://www.cse.iitk.ac.in/users/piyush/courses/pml{\_}winter16/nature14541.pdf},
volume = {521},
year = {2015}
}
@article{Brooks1991,
abstract = {Artificial intelligence research has foundered on the issue of representation. When intelligence is approached in an incremental manner, with strict reliance on interfacing to the real world through perception and action, reliance on representation disappears. In this paper we outline our approach to incrementally building complete intelligent Creatures. The fundamental decomposition of the intelligent system is not into independent information processing units which must interface with each other via representations. Instead, the intelligent system is decomposed into independent and parallel activity producers which all interface directly to the world through perception and action, rather than interface to each other particularly much. The notions of central and peripheral systems evaporateeverything is both central and peripheral. Based on these principles we have built a very successful series of mobile robots which operate without supervision as Creatures in standard office environments.},
author = {Brooks, Rodney A},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Brooks - 1991 - Intelligence without representation.pdf:pdf},
journal = {Artificial Intelligence},
pages = {139--159},
title = {{Intelligence without representation*}},
url = {http://people.csail.mit.edu/brooks/papers/representation.pdf},
volume = {47},
year = {1991}
}
@article{Held,
abstract = {Reinforcement learning is a powerful technique to train an agent to perform a task. However, an agent that is trained using reinforcement learning is only capable of achieving the single task that is specified via its reward function. Such an approach does not scale well to settings in which an agent needs to perform a diverse set of tasks, such as navigating to varying positions in a room or moving objects to vary-ing locations. Instead, we propose a method that allows an agent to automatically discover the range of tasks that it is capable of performing in its environment. We use a generator network to propose tasks for the agent to try to achieve, each task being specified as reaching a certain parametrized sub-set of the state-space. The generator network is optimized using adversarial training to produce tasks that are always at the appropriate level of difficulty for the agent. Our method thus auto-matically produces a curriculum of tasks for the agent to learn. We show that, by using this framework, an agent can efficiently and automatically learn to perform a wide set of tasks without requiring any prior knowledge of its environment. Our method can also learn to achieve tasks with sparse rewards, which traditionally pose significant challenges.},
author = {Held, David and Geng, Xinyang and Florensa, Carlos and Abbeel, Pieter},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Held et al. - Unknown - Automatic Goal Generation for Reinforcement Learning Agents.pdf:pdf},
title = {{Automatic Goal Generation for Reinforcement Learning Agents}},
url = {https://arxiv.org/pdf/1705.06366.pdf}
}
@article{Schulmanb,
abstract = {Policy gradient methods are an appealing approach in reinforcement learning be-cause they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the diffi-culty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substan-tially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD($\lambda$). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomo-tion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy repre-sentations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experi-ence required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.},
author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael I and Abbeel, Pieter},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Schulman et al. - Unknown - HIGH-DIMENSIONAL CONTINUOUS CONTROL USING GENERALIZED ADVANTAGE ESTIMATION.pdf:pdf},
title = {{HIGH-DIMENSIONAL CONTINUOUS CONTROL USING GENERALIZED ADVANTAGE ESTIMATION}},
url = {https://arxiv.org/pdf/1506.02438.pdf}
}
@article{Pastor2011,
abstract = {on PR2},
author = {Pastor, Peter},
doi = {10.1109/ICRA.2011.5980200},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Pastor - 2011 - Skill Learning and Task Outcome Prediction for Manipulation.pdf:pdf},
isbn = {978-1-61284-386-5},
issn = {1050-4729},
journal = {IEEE International Conference on Robotics and Automation (ICRA)},
pages = {3828--3834},
title = {{Skill Learning and Task Outcome Prediction for Manipulation}},
url = {http://www.cs.utexas.edu/{~}sniekum/classes/RLFD-F16/papers/Pastor11.pdf},
year = {2011}
}
@article{Ghadirzadeh,
abstract = {â€” Skilled robot task learning is best implemented by predictive action policies due to the inherent latency of sensorimotor processes. However, training such predictive poli-cies is challenging as it involves finding a trajectory of motor activations for the full duration of the action. We propose a data-efficient deep predictive policy training (DPPT) framework with a deep neural network policy architecture which maps an image observation to a sequence of motor activations. The architecture consists of three sub-networks referred to as the perception, policy and behavior super-layers. The perception and behavior super-layers force an abstraction of visual and motor data trained with synthetic and simulated training samples, respectively. The policy super-layer is a small sub-network with fewer parameters that maps data in-between the abstracted manifolds. It is trained for each task using methods for policy search reinforcement learning. We demonstrate the suitability of the proposed architecture and learning framework by training predictive policies for skilled object grasping and ball throwing on a PR2 robot. The effectiveness of the method is illustrated by the fact that these tasks are trained using only about 180 real robot attempts with qualitative terminal rewards.},
author = {Ghadirzadeh, Ali and Maki, Atsuto and Kragic, Danica and Bj{\"{o}}rkman, M{\aa}rten},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Ghadirzadeh et al. - Unknown - Deep Predictive Policy Training using Reinforcement Learning.pdf:pdf},
title = {{Deep Predictive Policy Training using Reinforcement Learning}},
url = {https://arxiv.org/pdf/1703.00727.pdf}
}
@article{Plappert,
abstract = {Deep reinforcement learning (RL) methods generally engage in exploratory be-havior through noise injection in the action space. An alternative is to add noise directly to the agent's parameters, which can lead to more consistent exploration and a richer set of behaviors. Methods such as evolutionary strategies use parameter perturbations, but discard all temporal structure in the process and require signif-icantly more samples. Combining parameter noise with traditional RL methods allows to combine the best of both worlds. We demonstrate that both off-and on-policy methods benefit from this approach through experimental comparison of DQN, DDPG, and TRPO on high-dimensional discrete action environments as well as continuous control tasks. Our results show that RL with parameter noise learns more efficiently than traditional RL with action space noise and evolutionary strategies individually.},
author = {Plappert, Matthias and Houthooft, Rein and Dhariwal, Prafulla and Sidor, Szymon and Chen, Richard Y and Chen, Xi and Asfour, Tamim and Abbeel, Pieter and Openai, Marcin Andrychowicz},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Plappert et al. - Unknown - Parameter Space Noise for Exploration.pdf:pdf},
title = {{Parameter Space Noise for Exploration}},
url = {https://arxiv.org/pdf/1706.01905.pdf}
}
@article{Rajeswaran,
abstract = {Sample complexity and safety are major challenges when learning policies with reinforcement learning for real-world tasks, especially when the policies are repre-sented using rich function approximators like deep neural networks. Model-based methods where the real-world target domain is approximated using a simulated source domain provide an avenue to tackle the above challenges by augmenting real data with simulated data. However, discrepancies between the simulated source domain and the target domain pose a challenge for simulated training. We introduce the EPOpt algorithm, which uses an ensemble of simulated source domains and a form of adversarial training to learn policies that are robust and generalize to a broad range of possible target domains, including unmodeled effects. Further, the probability distribution over source domains in the ensemble can be adapted using data from target domain and approximate Bayesian methods, to progressively make it a better approximation. Thus, learning on a model ensemble, along with source domain adaptation, provides the benefit of both robustness and learning/adaptation.},
author = {Rajeswaran, Aravind and Ghotra, Sarvjeet and Ravindran, Balaraman and Levine, Sergey},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Rajeswaran et al. - Unknown - EPOPT LEARNING ROBUST NEURAL NETWORK POLICIES USING MODEL ENSEMBLES.pdf:pdf},
title = {{EPOPT: LEARNING ROBUST NEURAL NETWORK POLICIES USING MODEL ENSEMBLES}},
url = {https://arxiv.org/pdf/1610.01283.pdf}
}
@article{Bansal,
abstract = {â€” Real-world robots are becoming increasingly com-plex and commonly act in poorly understood environments where it is extremely challenging to model or learn their true dynamics. Therefore, it might be desirable to take a task-specific approach, wherein the focus is on explicitly learning the dynamics model which achieves the best control performance for the task at hand, rather than learning the true dynamics. In this work, we use Bayesian optimization in an active learning framework where a locally linear dynamics model is learned with the intent of maximizing the control performance, and used in conjunction with optimal control schemes to efficiently design a controller for a given task. This model is updated directly based on the performance observed in experiments on the physical system in an iterative manner until a desired performance is achieved. We demonstrate the efficacy of the proposed approach through simulations and real experiments on a quadrotor testbed.},
author = {Bansal, Somil and Calandra, Roberto and Xiao, Ted and Levine, Sergey and Tomlin, Claire J},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Bansal et al. - Unknown - Goal-Driven Dynamics Learning via Bayesian Optimization.pdf:pdf},
title = {{Goal-Driven Dynamics Learning via Bayesian Optimization}},
url = {https://arxiv.org/pdf/1703.09260.pdf}
}
@article{Calandra2017,
author = {Calandra, Roberto},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Calandra - 2017 - Goal-Driven Dynamics Learning for Model-Based RL Motivation.pdf:pdf},
title = {{Goal-Driven Dynamics Learning for Model-Based RL Motivation}},
url = {http://www.robertocalandra.com/wp-content/uploads/2017-04-20-dali.pdf},
year = {2017}
}
@inproceedings{Tamar2016,
abstract = {We introduce the value iteration network: a fully differentiable neural network with a `planning module' embedded within. Value iteration networks are suitable for making predictions about outcomes that involve planning-based reasoning, such as predicting a desired trajectory from an observation of a map. Key to our approach is a novel differentiable approximation of the value-iteration algorithm, which can be represented as a convolutional neural network, and trained end-to-end using standard backpropagation. We evaluate our value iteration networks on the task of predicting optimal obstacle-avoiding trajectories from an image of a landscape, both on synthetic data, and on challenging raw images of the Mars terrain.},
archivePrefix = {arXiv},
arxivId = {1602.02867},
author = {Tamar, Aviv and Levine, Sergey and Abbeel, Pieter},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
eprint = {1602.02867},
issn = {10495258},
pmid = {172808},
title = {{Value Iteration Networks}},
year = {2016}
}
@article{Houthooft2016,
abstract = {Scalable and effective exploration remains a key challenge in reinforcement learn-ing (RL). While there are methods with optimality guarantees in the setting of dis-crete state and action spaces, these methods cannot be applied in high-dimensional deep RL scenarios. As such, most contemporary RL relies on simple heuristics such as exploration or adding Gaussian noise to the controls. This paper introduces Variational Information Maximizing Exploration (VIME), an explo-ration strategy based on maximization of information gain about the agent's belief of environment dynamics. We propose a practical implementation, using varia-tional inference in Bayesian neural networks which efficiently handles continuous state and action spaces. VIME modifies the MDP reward function, and can be applied with several different underlying RL algorithms. We demonstrate that VIME achieves significantly better performance compared to heuristic exploration methods across a variety of continuous control tasks and algorithms, including tasks with very sparse rewards.},
archivePrefix = {arXiv},
arxivId = {1605.09674},
author = {Houthooft, Rein and Chen, Xi and Duan, Yan and Schulman, John and {De Turck}, Filip and Abbeel, Pieter},
eprint = {1605.09674},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems (NIPS)},
title = {{Variational Information Maximizing Exploration}},
year = {2016}
}
@article{Reed,
abstract = {We propose the neural programmer-interpreter (NPI): a recurrent and composi-tional neural network that learns to represent and execute programs. NPI has three learnable components: a task-agnostic recurrent core, a persistent key-value pro-gram memory, and domain-specific encoders that enable a single NPI to operate in multiple perceptually diverse environments with distinct affordances. By learning to compose lower-level programs to express higher-level programs, NPI reduces sample complexity and increases generalization ability compared to sequence-to-sequence LSTMs. The program memory allows efficient learning of additional tasks by building on existing programs. NPI can also harness the environment (e.g. a scratch pad with read-write pointers) to cache intermediate results of com-putation, lessening the long-term memory burden on recurrent hidden units. In this work we train the NPI with fully-supervised execution traces; each program has example sequences of calls to the immediate subprograms conditioned on the input. Rather than training on a huge number of relatively weak labels, NPI learns from a small number of rich examples. We demonstrate the capability of our model to learn several types of compositional programs: addition, sorting, and canonicalizing 3D models. Furthermore, a single NPI learns to execute these pro-grams and all 21 associated subprograms.},
author = {Reed, Scott and {De Freitas}, Nando},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Reed, De Freitas - Unknown - NEURAL PROGRAMMER-INTERPRETERS.pdf:pdf},
title = {{NEURAL PROGRAMMER-INTERPRETERS}},
url = {https://arxiv.org/pdf/1511.06279v3.pdf}
}
@article{Nagabandi,
abstract = {Model-free deep reinforcement learning methods have successfully learned com-plex behavioral strategies for a wide range of tasks, but typically require many samples to achieve good performance. Model-based algorithms, in principle, can provide for much more efficient learning, but have proven difficult to extend to expressive, high-capacity models such as deep neural networks. In this work, we demonstrate that medium-sized neural network models can in fact be combined with model predictive control (MPC) to achieve excellent sample complexity in a model-based reinforcement learning algorithm, producing stable and plausible gaits to accomplish various complex locomotion tasks. We also propose using deep neural network dynamics models to initialize a model-free learner, in order to com-bine the sample efficiency of model-based approaches with the high task-specific performance of model-free methods. We perform this pre-initialization by using rollouts from the trained model-based controller as supervision to pre-train a policy, and then fine-tune the policy using a model-free method. We empirically demon-strate that this resulting hybrid algorithm can drastically accelerate model-free learning and outperform purely model-free learners on several MuJoCo locomo-tion benchmark tasks, achieving sample efficiency gains over a purely model-free learner of 330Ã— on swimmer, 26Ã— on hopper, 4Ã— on half-cheetah, and 3Ã— on ant. Videos can be found at https://sites.google.com/view/mbmf.},
author = {Nagabandi, Anusha and Kahn, Gregory and Fearing, Ronald S and Levine, Sergey},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Nagabandi et al. - Unknown - Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning.pdf:pdf},
title = {{Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning}},
url = {https://arxiv.org/pdf/1708.02596.pdf}
}
@article{Devin,
abstract = {Robotic manipulation in complex open-world scenarios requires both reliable physical manipulation skills and effective and generalizable perception. In this paper, we propose a method where general purpose pretrained visual mod-els serve as an object-centric prior for the perception system of a learned policy. We devise an object-level attentional mechanism that can be used to determine rel-evant objects from a few demonstrations, and then immediately incorporate those objects into a learned policy. A task-independent meta-attention locates possible objects in the scene, and a task-specific attention identifies which objects are pre-dictive of the demonstrations. The scope of the task-specific attention is easily adjusted by showing demonstrations with distractor objects or with diverse rele-vant objects. Our results indicate that this approach exhibits good generalization across object instances using very few samples, and can be used to learn a variety of manipulation tasks using reinforcement learning.},
author = {Devin, Coline and Abbeel, Pieter and Darrell, Trevor and Levine, Sergey},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Devin et al. - Unknown - Deep Object-Centric Representations for Generalizable Robot Learning.pdf:pdf},
keywords = {manipulation,reinforcement learning,vision},
title = {{Deep Object-Centric Representations for Generalizable Robot Learning}},
url = {https://arxiv.org/pdf/1708.04225.pdf}
}
@article{Rahmatizadeh,
abstract = {â€” Robots assisting disabled or elderly people in activities of daily living must perform complex manipulation tasks. These tasks are dependent on the user's environment and preferences. Thus, learning from demonstration (LfD) is a promising choice that would allow the non-expert user to teach the robot different tasks. Unfortunately, learning general solu-tions from raw demonstrations requires a significant amount of data. Performing this number of physical demonstrations is unfeasible for a disabled user. In this paper we propose an approach where the user demon-strates the manipulation task in a virtual environment. The collected demonstrations are used to train an LSTM recurrent neural network that can act as the controller for the robot. We show that the controller learned from virtual demonstrations can be used to successfully perform the manipulation tasks on a physical robot.},
author = {Rahmatizadeh, Rouhollah and Abolghasemi, Pooya and Behal, Aman and B{\"{o}}l{\"{o}}ni, Ladislau},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Rahmatizadeh et al. - Unknown - Learning real manipulation tasks from virtual demonstrations using LSTM.pdf:pdf},
title = {{Learning real manipulation tasks from virtual demonstrations using LSTM}},
url = {https://arxiv.org/pdf/1603.03833.pdf}
}
@article{Wulfmeier,
abstract = {Training robots for operation in the real world is a complex, time con-suming and potentially expensive task. Despite significant success of reinforce-ment learning in games and simulations, research in real robot applications has not been able to match similar progress. While sample complexity can be reduced by training policies in simulation, these can perform sub-optimally on the real platform given imperfect calibration of model dynamics. We present an approach -supplemental to fine tuning on the real robot -to further benefit from parallel access to a simulator during training. The developed approach harnesses auxiliary rewards to guide the exploration for the real world agent based on the proficiency of the agent in simulation and vice versa. In this context, we demonstrate em-pirically that the reciprocal alignment for both agents provides further benefit as the agent in simulation can adjust to optimize its behaviour for states commonly visited by the real-world agent.},
author = {Wulfmeier, Markus and Posner, Ingmar and Abbeel, Pieter},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Wulfmeier, Posner, Abbeel - Unknown - Mutual Alignment Transfer Learning.pdf:pdf},
keywords = {Adversarial Learning,Robotics,Simulation,Transfer Learning},
title = {{Mutual Alignment Transfer Learning}},
url = {https://arxiv.org/pdf/1707.07907.pdf}
}
@article{Singh,
abstract = {We tackle the problem of learning robotic sensorimotor control policies that can generalize to visually diverse and unseen environments. Achieving broad generalization typi-cally requires large datasets, which are difficult to obtain for task-specific interactive processes such as reinforce-ment learning or learning from demonstration. However, much of the visual diversity in the world can be captured through passively collected datasets of images or videos. In our method, which we refer to as GPLAC (Generalized Policy Learning with Attentional Classifier), we use both interaction data and weakly labeled image data to augment the generalization capacity of sensorimotor policies. Our method combines multitask learning on action selection and an auxiliary binary classification objective, together with a convolutional neural network architecture that uses an attentional mechanism to avoid distractors. We show that pairing interaction data from just a single environment with a diverse dataset of weakly labeled data results in greatly improved generalization to unseen environments, and show that this generalization depends on both the auxiliary ob-jective and the attentional architecture that we propose. We demonstrate our results in both simulation and on a real robotic manipulator, and demonstrate substantial improve-ment over standard convolutional architectures and domain adaptation methods.},
author = {Singh, Avi and Yang, Larry and Levine, Sergey},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Singh, Yang, Levine - Unknown - GPLAC Generalizing Vision-Based Robotic Skills using Weakly Labeled Images.pdf:pdf},
title = {{GPLAC: Generalizing Vision-Based Robotic Skills using Weakly Labeled Images}},
url = {https://arxiv.org/pdf/1708.02313.pdf}
}
@article{Murali,
abstract = {Recent self-supervised learning approaches focus on using a few thou-sand data points to learn policies for high-level, low-dimensional action spaces. However, scaling this framework for high-dimensional control require either scal-ing up the data collection efforts or using a clever sampling strategy for training. We present a novel approach -Curriculum Accelerated Self-Supervised Learn-ing (CASSL) -to train policies that map visual information to high-level, higher-dimensional action spaces. CASSL orders the sampling of training data based on control dimensions: the learning and sampling are focused on few control pa-rameters before other parameters. The right curriculum for learning is suggested by variance-based global sensitivity analysis of the control space. We apply our CASSL framework to learning how to grasp using an adaptive, underactuated multi-fingered gripper, a challenging system to control. Our experimental results indicate that CASSL provides significant improvement and generalization com-pared to baseline methods such as staged curriculum learning (8{\%} increase) and complete end-to-end learning with random exploration (14{\%} improvement) tested on a set of novel objects. Supplementary video: youtube.com/iCQsM7EE4HI.},
author = {Murali, Adithyavairavan and Pinto, Lerrel and Gandhi, Dhiraj and Gupta, Abhinav},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Murali et al. - Unknown - CASSL Curriculum Accelerated Self-Supervised Learning.pdf:pdf},
title = {{CASSL: Curriculum Accelerated Self-Supervised Learning}},
url = {https://arxiv.org/pdf/1708.01354.pdf}
}
@article{Vaswani,
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Vaswani et al. - Unknown - Attention Is All You Need.pdf:pdf},
title = {{Attention Is All You Need}},
url = {https://arxiv.org/pdf/1706.03762.pdf}
}
@article{Oh,
abstract = {This paper proposes a novel deep reinforcement learning (RL) architecture, called Value Prediction Network (VPN), which integrates model-free and model-based RL methods into a single neural network. In contrast to typical model-based RL methods, VPN learns a dynamics model whose abstract states are trained to make option-conditional predictions of future values (discounted sum of re-wards) rather than of future observations. Our experimental results show that VPN has several advantages over both model-free and model-based baselines in a stochastic environment where careful planning is required but building an accurate observation-prediction model is difficult. Furthermore, VPN outperforms Deep Q-Network (DQN) on several Atari games even with short-lookahead planning, demonstrating its potential as a new way of learning a good state representation.},
author = {Oh, Junhyuk and Singh, Satinder and Lee, Honglak},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Oh, Singh, Lee - Unknown - Value Prediction Network.pdf:pdf},
title = {{Value Prediction Network}},
url = {https://arxiv.org/pdf/1707.03497.pdf}
}
@article{Schulmanc,
abstract = {We propose a new family of policy gradient methods for reinforcement learning, which al-ternate between sampling data through interaction with the environment, and optimizing a " surrogate " objective function using stochastic gradient ascent. Whereas standard policy gra-dient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimiza-tion (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, includ-ing simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Openai, Oleg Klimov},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Schulman et al. - Unknown - Proximal Policy Optimization Algorithms.pdf:pdf},
title = {{Proximal Policy Optimization Algorithms}},
url = {https://arxiv.org/pdf/1707.06347.pdf}
}
@article{Bellemare,
abstract = {In this paper we argue for the fundamental impor-tance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the com-mon approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of liter-ature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy eval-uation and control settings, exposing a signifi-cant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman's equa-tion to the learning of approximate value distri-butions. We evaluate our algorithm using the suite of games from the Arcade Learning En-vironment. We obtain both state-of-the-art re-sults and anecdotal evidence demonstrating the importance of the value distribution in approxi-mate reinforcement learning. Finally, we com-bine theoretical and empirical evidence to high-light the ways in which the value distribution im-pacts learning in the approximate setting.},
author = {Bellemare, Marc G and Dabney, Will and Munos, R{\'{e}}mi},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Bellemare, Dabney, Munos - Unknown - A Distributional Perspective on Reinforcement Learning.pdf:pdf},
title = {{A Distributional Perspective on Reinforcement Learning}},
url = {https://arxiv.org/pdf/1707.06887.pdf}
}
@article{Pearl,
author = {Pearl, Judea},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Pearl - Unknown - REASONING WITH CAUSE AND EFFECT.pdf:pdf},
title = {{REASONING WITH CAUSE AND EFFECT}},
url = {http://bayes.cs.ucla.edu/IJCAI99/ijcai-99.pdf}
}
@article{Rahmatizadeha,
abstract = {In this paper, we propose a multi-task learning from demonstration method that works using raw images as input to autonomously accomplish a wide variety of tasks in the real world using a low-cost robotic arm. The controller is a single recurrent neural network that can generate robot arm trajectories to perform different manipulation tasks. In order to learn complex skills from relatively few demonstrations, we share parameters across different tasks. Our network also combines VAE-GAN-based reconstruction with autoregressive multimodal action prediction for improved data efficiency. Our results show that weight sharing and reconstruction substantially improve generalization and robustness, and that training on multiple tasks simultaneously greatly improves the success rate on all of the tasks. Our experiments, performed on a real-world low-cost Lynxmotion arm, illustrate a variety of picking and placing tasks, as well as non-prehensile manipulation.},
author = {Rahmatizadeh, Rouhollah and Abolghasemi, Pooya and B{\"{o}}l{\"{o}}ni, Ladislau and Levine, Sergey},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Rahmatizadeh et al. - Unknown - Vision-Based Multi-Task Manipulation for Inexpensive Robots Using End-To-End Learning from Demonstration.pdf:pdf},
keywords = {Affordable assistive robotics,Multi-task learning,Robot manipulation},
title = {{Vision-Based Multi-Task Manipulation for Inexpensive Robots Using End-To-End Learning from Demonstration}},
url = {https://arxiv.org/pdf/1707.02920.pdf}
}
@article{Darvish,
abstract = {The Industry 4.0 paradigm emphasizes the crucial benefits that collabora-tive robots, i.e., robots able to work alongside and together with humans, could bring to the whole production process. In this context, an enabling technol-ogy yet unreached is the design of flexible robots able to deal at all levels with humans' intrinsic variability, which is not only a necessary element for a com-fortable working experience for the person, but also a precious capability for efficiently dealing with unexpected events. In this paper, a sensing, represen-tation, planning and control architecture for flexible human-robot cooperation, referred to as FlexHRC, is proposed. FlexHRC relies on wearable sensors for human action recognition, AND/OR graphs for the representation of and rea-soning upon cooperation models, and a Task Priority framework to decouple action planning from robot motion planning and control.},
author = {Darvish, Kourosh and Wanderlingh, Francesco and Bruno, Barbara and Simetti, Enrico and Mastrogiovanni, Fulvio and Casalino, Giuseppe},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Darvish et al. - Unknown - Flexible Human-Robot Cooperation Models for Assisted Shop-floor Tasks.pdf:pdf},
keywords = {AND/OR graph,Human-Robot Cooperation,Smart Factory,Task Priority control,Wearable Sensing},
title = {{Flexible Human-Robot Cooperation Models for Assisted Shop-floor Tasks}},
url = {https://arxiv.org/pdf/1707.02591.pdf}
}
@article{Lopez-Paz,
abstract = {One major obstacle towards artificial intelligence is the poor ability of models to quickly solve new problems, without forgetting previously acquired knowledge. To better understand this issue, we study the problem of learning over a continuum of data, where the model observes, once and one by one, examples concerning a sequence of tasks. First, we propose a set of metrics to evaluate models learning over a continuum of data. These metrics characterize models not only by their test accuracy, but also in terms of their ability to transfer knowledge across tasks. Second, we propose a model to learn over continuums of data, called Gradient of Episodic Memory (GEM), which alleviates forgetting while allowing beneficial transfer of knowledge to previous tasks. Our experiments on variants of the MNIST and CIFAR-100 datasets demonstrate the strong performance of GEM when compared to the state-of-the-art.},
author = {Lopez-Paz, David and Ranzato, Marc Aurelio},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Lopez-Paz, Ranzato - Unknown - Gradient Episodic Memory for Continuum Learning.pdf:pdf},
title = {{Gradient Episodic Memory for Continuum Learning}},
url = {https://arxiv.org/pdf/1706.08840.pdf}
}
@article{Justesen,
abstract = {â€”The real-time strategy game StarCraft has proven to be a challenging environment for artificial intelligence techniques, and as a result, current state-of-the-art solutions consist of numerous hand-crafted modules. In this paper, we show how macromanagement decisions in StarCraft can be learned directly from game replays using deep learning. Neural networks are trained on 789,571 state-action pairs extracted from 2,005 replays of highly skilled players, achieving top-1 and top-3 error rates of 54.6{\%} and 22.9{\%} in predicting the next build action. By integrating the trained network into UAlbertaBot, an open source StarCraft bot, the system can significantly outperform the game's built-in Terran bot, and play competitively against UAlbertaBot with a fixed rush strategy. To our knowledge, this is the first time macromanagement tasks are learned directly from replays in StarCraft. While the best hand-crafted strategies are still the state-of-the-art, the deep network approach is able to express a wide range of different strategies and thus improving the network's performance further with deep reinforcement learning is an immediately promising avenue for future research. Ultimately this approach could lead to strong StarCraft bots that are less reliant on hard-coded strategies.},
author = {Justesen, Niels and Risi, Sebastian},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Justesen, Risi - Unknown - Learning Macromanagement in StarCraft from Replays using Deep Learning.pdf:pdf},
title = {{Learning Macromanagement in StarCraft from Replays using Deep Learning}},
url = {https://njustesen.files.wordpress.com/2017/07/njustesen2017learning.pdf}
}
@article{Merel,
abstract = {Rapid progress in deep reinforcement learning has made it increasingly feasible to train controllers for high-dimensional humanoid bodies. However, methods that use pure reinforcement learning with simple reward functions tend to pro-duce non-humanlike and overly stereotyped movement behaviors. In this work, we extend generative adversarial imitation learning to enable training of generic neural network policies to produce humanlike movement patterns from limited demonstrations consisting only of partially observed state features, without access to actions, even when the demonstrations come from a body with different and unknown physical parameters. We leverage this approach to build sub-skill policies from motion capture data and show that they can be reused to solve tasks when controlled by a higher level controller. [video abstract]},
author = {Merel, Josh and Tassa, Yuval and Tb, Dhruva and Srinivasan, Sriram and Lemmon, Jay and Wang, Ziyu and Wayne, Greg and Heess, Nicolas},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Merel et al. - Unknown - Learning human behaviors from motion capture by adversarial imitation.pdf:pdf},
title = {{Learning human behaviors from motion capture by adversarial imitation}},
url = {https://arxiv.org/pdf/1707.02201.pdf}
}
@article{Cabi,
abstract = {This paper introduces the Intentional Unintentional (IU) agent. This agent endows the deep deterministic policy gradients (DDPG) agent for contin-uous control with the ability to solve several tasks simultaneously. Learning to solve many tasks simultaneously has been a long-standing, core goal of artificial intelligence, inspired by infant development and motivated by the desire to build flexible robot manipulators capable of many diverse behaviours. We show that the IU agent not only learns to solve many tasks simultaneously but it also learns faster than agents that target a single task at-a-time. In some cases, where the single task DDPG method completely fails, the IU agent successfully solves the task. To demonstrate this, we build a playroom environment using the MuJoCo physics engine, and introduce a grounded formal language to automatically generate tasks.},
author = {Cabi, Serkan and {G{\'{o}}mez Colmenarejo}, Sergio and Hoffman, Matthew W and Denil, Misha and Wang, Ziyu and {De Freitas}, Nando},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Cabi et al. - Unknown - The Intentional Unintentional Agent Learning to Solve Many Continuous Control Tasks Simultaneously.pdf:pdf},
keywords = {Deep deterministic policy gradients,control,multi-task,physics},
title = {{The Intentional Unintentional Agent: Learning to Solve Many Continuous Control Tasks Simultaneously}},
url = {https://arxiv.org/pdf/1707.03300.pdf}
}
@book{,
abstract = {This book is to be neither an accusation nor a confession, and least of all an adventure, for death is not an adventure to those who stand face to face with it. It will try simply to tell of a generation of men who, even though they may have escaped its shells, were destroyed by the war. ONE We are at rest five miles behind the front. Yesterday we were relieved, and now our bellies are full of beef and haricot beans. We are satisfied and at peace. Each man has another mess-tin full for the evening; and, what is more, there is a double ration of sausage and bread. That puts a man in fine trim. We have not had such luck as this for a long time. The cook with his carroty head is begging us to eat; he beckons with his ladle to every one that passes, and spoons him out a great dollop. He does not see how he can empty his stew-pot in time for coffee. Tjaden and M{\"{u}}ller have produced two washbasins and had them filled up to the brim as a reserve. In Tjaden this is voracity, in M{\"{u}}ller it is foresight. Where Tjaden puts it all is a mystery, for he is and always will be as thin as a rake. What's more important still is the issue of a double ration of smokes. Ten cigars, twenty cigarettes, and two quids of chew per man; now that is decent. I have exchanged my chewing tobacco with Katczinsky for his cigarettes, which means I have forty altogether. That's enough for a day. It is true we have no right to this windfall. The Prussian is not so generous. We have only a miscalculation to thank for it.},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - All Quiet on the Western Front.pdf:pdf},
title = {{All Quiet on the Western Front}},
url = {http://explainallquietonthewesternfront.weebly.com/uploads/2/4/7/2/24722875/all{\_}quiet{\_}on{\_}the{\_}western{\_}front.pdf}
}
@article{Finna,
abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is com-patible with any model trained with gradient de-scent and applicable to a variety of different learning problems, including classification, re-gression, and reinforcement learning. The goal of meta-learning is to train a model on a vari-ety of learning tasks, such that it can solve new learning tasks using only a small number of train-ing samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and acceler-ates fine-tuning for policy gradient reinforcement learning with neural network policies.},
author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Finn, Abbeel, Levine - Unknown - Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.pdf:pdf},
title = {{Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks}},
url = {https://arxiv.org/pdf/1703.03400.pdf}
}
@article{Lake,
abstract = {Recent progress in artificial intelligence (AI) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving perfor-mance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recog-nition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
author = {Lake, Brenden M and Ullman, Tomer D and Tenenbaum, Joshua B and Gershman, Samuel J},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Lake et al. - Unknown - Building Machines That Learn and Think Like People.pdf:pdf},
title = {{Building Machines That Learn and Think Like People}},
url = {https://arxiv.org/pdf/1604.00289.pdf}
}
@article{Selsam,
abstract = {Noisy data, non-convex objectives, model mis-specification, and numerical instability can all cause undesired behaviors in machine learning systems. As a result, detecting actual imple-mentation errors can be extremely difficult. We demonstrate a methodology in which developers use an interactive proof assistant to both imple-ment their system and to state a formal theorem defining what it means for their system to be cor-rect. The process of proving this theorem inter-actively in the proof assistant exposes all imple-mentation errors since any error in the program would cause the proof to fail. As a case study, we implement a new system, Certigrad, for opti-mizing over stochastic computation graphs, and we generate a formal (i.e. machine-checkable) proof that the gradients sampled by the system are unbiased estimates of the true mathematical gradients. We train a variational autoencoder us-ing Certigrad and find the performance compara-ble to training the same model in TensorFlow.},
author = {Selsam, Daniel and Liang, Percy and Dill, David L},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Selsam, Liang, Dill - Unknown - Developing Bug-Free Machine Learning Systems With Formal Mathematics.pdf:pdf},
title = {{Developing Bug-Free Machine Learning Systems With Formal Mathematics}},
url = {https://arxiv.org/pdf/1706.08605.pdf}
}
@article{Schaul,
abstract = {Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.},
author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David and Deepmind, Google},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Schaul et al. - Unknown - PRIORITIZED EXPERIENCE REPLAY.pdf:pdf},
title = {{PRIORITIZED EXPERIENCE REPLAY}},
url = {https://arxiv.org/pdf/1511.05952.pdf}
}
@article{Parisotto,
abstract = {The ability to act in multiple environments and transfer previous knowledge to new situations can be considered a critical aspect of any intelligent agent. To-wards this goal, we define a novel method of multitask and transfer learning that enables an autonomous agent to learn how to behave in multiple tasks simultane-ously, and then generalize its knowledge to new domains. This method, termed " Actor-Mimic " , exploits the use of deep reinforcement learning and model com-pression techniques to train a single policy network that learns how to act in a set of distinct tasks by using the guidance of several expert teachers. We then show that the representations learnt by the deep policy network are capable of general-izing to new tasks with no prior expert guidance, speeding up learning in novel environments. Although our method can in general be applied to a wide range of problems, we use Atari games as a testing environment to demonstrate these methods.},
author = {Parisotto, Emilio and Ba, Jimmy and Salakhutdinov, Ruslan},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Parisotto, Ba, Salakhutdinov - Unknown - ACTOR-MIMIC DEEP MULTITASK AND TRANSFER REINFORCEMENT LEARNING.pdf:pdf},
title = {{ACTOR-MIMIC DEEP MULTITASK AND TRANSFER REINFORCEMENT LEARNING}},
url = {https://arxiv.org/pdf/1511.06342.pdf}
}
@article{Morales,
abstract = {Fig. 1: Cylindrical robot guiding a box to (purple) goal region. Abstractâ€”We address the motion planning problem for a single robot that manipulates passive objects in an otherwise static environment. We handle uncertainties in passive object responses while abstracting their dynamics by creating a roadmap that encodes the possible passive object responses. The robot extracts reference paths from the passive roadmap to plan approaching motions with an active roadmap. When the robot acts on the object, it receives response feedback and reacts accordingly. We apply this approach to pushing, a basic manipulation primitive. We propose four different pushing strategies, one of them using reinforcement learning. Our simulations show the effectiveness of the approach and of each pushing strategy.},
author = {Morales, Marco and Rodriguez, Samuel and Thomas, Shawna and Amato, Nancy M},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Morales et al. - Unknown - Sampling-Based Planning and Local Reactive Strategies for Environment Manipulation.pdf:pdf},
title = {{Sampling-Based Planning and Local Reactive Strategies for Environment Manipulation}},
url = {http://rss2017ws.is.tuebingen.mpg.de/abstracts/push-planning-rss17-ws{\_}contact-camera-ready.pdf}
}
@article{Hosseini,
author = {Hosseini, Khaled},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Hosseini - Unknown - A THOUSAND SPLENDID SUNS.pdf:pdf},
title = {{A THOUSAND SPLENDID SUNS}},
url = {http://english4success.ru/Upload/books/1806.pdf}
}
@book{,
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - The Handmaid's Tale.pdf:pdf},
title = {{The Handmaid's Tale}}
}
@article{VidiadharSurajprasadNaipaul,
abstract = {On the house in Sikkim Street Mr. Biswas owed, and had been owing for four years, three thousand dollars. The interest on this, at eight per cent, came to twenty dollars a month; the ground rent was ten dollars. Two children were at school. The two older children, on whom Mr. Biswas might have depended, were both abroad on scholarships. It gave Mr. Biswas some satisfaction that in the circumstances Shama did not run straight off to her mother to beg for help. Ten years before that would have been her first thought. Now she tried to comfort Mr. Biswas, and devised plans on her own. " Potatoes, " she said. " We can start selling potatoes. The price around here is eight cents a pound. If we buy at five and sell at sevenâ€” " " Trust the Tulsi bad blood, " Mr. Biswas said. " I know that the pack of you Tulsis are financial geniuses. But have a good look around and count the number of people selling potatoes. Better to sell the old car. " " No. Not the car. Don't worry. We'll manage. " " Yes, " Mr. Biswas said irritably. " We'll manage. " No more was heard of the potatoes, and Mr. Biswas never threatened again to sell the car. He didn't now care to do anything against his wife's wishes. He had grown to accept her judgement and to respect her optimism. He trusted her. Since they had moved to the house Shama had learned a new loyalty, to him and to their children; away from her mother and sisters, she was able to express this without shame, and to Mr. Biswas this was a triumph almost as big as the acquiring of his own house. He thought of the house as his own, though for years it had been irretrievably mortgaged. And during these months of illness and despair he was struck again and again by the wonder of being in his own house, the audacity of it: to walk in through his own front gate, to bar entry to whoever he wished, to close his doors and windows every night, to hear no noises except those of his family, to wander freely from room to room and about his yard, instead of being condemned, as before, to retire the moment he got home to the crowded room in one or the other of Mrs. Tulsi's houses, crowded with Shama's sisters, their husbands, their children. As a boy he had moved from one house of strangers to another; and since his marriage he felt he had lived nowhere but in the houses of the Tulsis, at Hanuman House in Arwacas, in the decaying wooden house at Shorthills, in the clumsy concrete house in Port of Spain. And now at the end he found himself in his own house, on his own half-lot of land, his own portion of the earth. That he should have been responsible for this seemed to him, in these last months, stupendous. The house could be seen from two or three streets away and was known all over St. James. It was like a huge and squat sentry-box: tall, square, two-storeyed, with a pyramidal roof of corrugated iron. It had been designed and built by a solicitor's clerk who built houses in his spare time. The solicitor's clerk had many contacts. He bought land which the City Council had announced was not for sale; he persuaded estate owners to split whole lots into half-lots; he bought lots of barely reclaimed swamp land near Mucurapo and got permission to build on them. On whole lots or three-quarter-lots he built one-storey houses, twenty feet by twenty-six, which could pass unnoticed; on half-lots he built two-storey houses, twenty feet by thirteen, which were distinctive. All},
author = {{Vidiadhar Surajprasad Naipaul}, By and {Riaz Islamabad -Pakistan}, Shahid},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Vidiadhar Surajprasad Naipaul, Riaz Islamabad -Pakistan - Unknown - A House for Mr. Biswas.pdf:pdf},
title = {{A House for Mr. Biswas}},
url = {http://rgi.edu.in/rgi{\_}pdf/A{\_}House{\_}for{\_}Mr{\_}Biswas{\_}By{\_}Vidiadhar{\_}Surajprasad{\_}Naipaul.pdf}
}
@article{GabrielGarciaMarquez,
author = {{Gabriel Garcia Marquez}, By and {Riaz Islamabad -Pakistan}, Shahid},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Gabriel Garcia Marquez, Riaz Islamabad -Pakistan - Unknown - ONE HUNDRED YEARS OF SOLITUDE.pdf:pdf},
title = {{ONE HUNDRED YEARS OF SOLITUDE}},
url = {http://202.74.245.22:8080/xmlui/bitstream/handle/123456789/164/Marquez.pdf?sequence=1}
}
@article{Narayan,
author = {Narayan, R K},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Narayan - Unknown - A TIGER FOR MALGUDI PENGUIN BOOKS.pdf:pdf},
title = {{A TIGER FOR MALGUDI PENGUIN BOOKS}},
url = {http://cms.tamindia.com/matrix/images/stories/article-images/may-2014/A{\_}Tiger{\_}for{\_}Malgudi.pdf}
}
@article{Zhang,
abstract = {â€” Tensegrity robots, composed of rigid rods con-nected by elastic cables, have a number of unique properties that make them appealing for use as planetary exploration rovers. However, control of tensegrity robots remains a difficult problem due to their unusual structures and complex dynamics. In this work, we show how locomotion gaits can be learned automatically using a novel extension of mirror descent guided policy search (MDGPS) applied to periodic locomotion move-ments, and we demonstrate the effectiveness of our approach on tensegrity robot locomotion. We evaluate our method with real-world and simulated experiments on the SUPERball tensegrity robot, showing that the learned policies generalize to changes in system parameters, unreliable sensor measurements, and variation in environmental conditions, including varied terrains and a range of different gravities. Our experiments demonstrate that our method not only learns fast, power-efficient feedback policies for rolling gaits, but that these policies can succeed with only the limited onboard sensing provided by SUPERball's accelerometers. We compare the learned feedback policies to learned open-loop policies and hand-engineered controllers, and demonstrate that the learned policy enables the first continuous, reliable locomotion gait for the real SUPERball robot. Our code and other supplementary materials are available from http://rll.berkeley.edu/drl{\_}tensegrity},
author = {Zhang, Marvin and Geng, Xinyang and Bruce, Jonathan and Caluwaerts, Ken and Vespignani, Massimo and Sunspiral, Vytas and Abbeel, Pieter and Levine, Sergey},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - Unknown - Deep Reinforcement Learning for Tensegrity Robot Locomotion.pdf:pdf},
title = {{Deep Reinforcement Learning for Tensegrity Robot Locomotion}},
url = {https://arxiv.org/pdf/1609.09049.pdf}
}
@article{Bacon,
abstract = {Temporal abstraction is key to scaling up learning and plan-ning in reinforcement learning. While planning with tempo-rally extended actions is well understood, creating such ab-stractions autonomously from data has remained challenging. We tackle this problem in the framework of options [Sutton, Precup {\&} Singh, 1999; Precup, 2000]. We derive policy gra-dient theorems for options and propose a new option-critic architecture capable of learning both the internal policies and the termination conditions of options, in tandem with the pol-icy over options, and without the need to provide any addi-tional rewards or subgoals. Experimental results in both dis-crete and continuous environments showcase the flexibility and efficiency of the framework.},
author = {Bacon, Pierre-Luc and Harb, Jean and Precup, Doina},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Bacon, Harb, Precup - Unknown - The Option-Critic Architecture.pdf:pdf},
title = {{The Option-Critic Architecture}},
url = {https://arxiv.org/pdf/1609.05140.pdf}
}
@article{Szita,
abstract = {The exploration-exploitation dilemma has been an intriguing and unsolved problem within the framework of reinforcement learn-ing. " Optimism in the face of uncertainty " and model building play central roles in ad-vanced exploration methods. Here, we inte-grate several concepts and obtain a fast and simple algorithm. We show that the proposed algorithm finds a near-optimal policy in poly-nomial time, and give experimental evidence that it is robust and efficient compared to its ascendants.},
author = {Szita, Istv{\'{a}}n},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Szita - Unknown - The Many Faces of Optimism a Unifying Approach.pdf:pdf},
title = {{The Many Faces of Optimism: a Unifying Approach}},
url = {http://icml2008.cs.helsinki.fi/papers/490.pdf}
}
@article{Singha,
abstract = {Reinforcement learning has achieved broad and successful ap-plication in cognitive science in part because of its general for-mulation of the adaptive control problem as the maximization of a scalar reward function. The computational reinforcement learning framework is motivated by correspondences to ani-mal reward processes, but it leaves the source and nature of the rewards unspecified. This paper advances a general computa-tional framework for reward that places it in an evolutionary context, formulating a notion of an optimal reward function given a fitness function and some distribution of environments. Novel results from computational experiments show how tra-ditional notions of extrinsically and intrinsically motivated be-haviors may emerge from such optimal reward functions. In the experiments these rewards are discovered through auto-mated search rather than crafted by hand. The precise form of the optimal reward functions need not bear a direct relationship to the fitness function, but may nonetheless confer significant advantages over rewards based only on fitness.},
author = {Singh, Satinder and Lewis, Richard L and Barto, Andrew G},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Singh, Lewis, Barto - Unknown - Where Do Rewards Come From.pdf:pdf},
title = {{Where Do Rewards Come From?}},
url = {https://web.eecs.umich.edu/{~}baveja/Papers/singh-lewis-barto-2009-cogsci.pdf}
}
@article{Kendall,
abstract = {Deep learning has shown to be effective for robust and real-time monocular image relocalisation. In particular, PoseNet [22] is a deep convolutional neural network which learns to regress the 6-DOF camera pose from a single im-age. It learns to localize using high level features and is robust to difficult lighting, motion blur and unknown cam-era intrinsics, where point based SIFT registration fails. However, it was trained using a naive loss function, with hyper-parameters which require expensive tuning. In this paper, we give the problem a more fundamental theoreti-cal treatment. We explore a number of novel loss functions for learning camera pose which are based on geometry and scene reprojection error. Additionally we show how to au-tomatically learn an optimal weighting to simultaneously regress position and orientation. By leveraging geometry, we demonstrate that our technique significantly improves PoseNet's performance across datasets ranging from indoor rooms to a small city.},
author = {Kendall, Alex and Cipolla, Roberto},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Kendall, Cipolla - Unknown - Geometric loss functions for camera pose regression with deep learning.pdf:pdf},
title = {{Geometric loss functions for camera pose regression with deep learning}},
url = {https://arxiv.org/pdf/1704.00390.pdf}
}
@article{Zenke,
abstract = {While deep learning has led to remarkable ad-vances across diverse applications, it struggles in domains where the data distribution changes over the course of learning. In stark contrast, biological neural networks continually adapt to changing domains, possibly by leveraging com-plex molecular machinery to solve many tasks simultaneously. In this study, we introduce in-telligent synapses that bring some of this bio-logical complexity into artificial neural networks. Each synapse accumulates task relevant informa-tion over time, and exploits this information to rapidly store new memories without forgetting old ones. We evaluate our approach on continual learning of classification tasks, and show that it dramatically reduces forgetting while maintain-ing computational efficiency.},
author = {Zenke, Friedemann and Poole, Ben and Ganguli, Surya},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Zenke, Poole, Ganguli - Unknown - Continual Learning Through Synaptic Intelligence.pdf:pdf},
title = {{Continual Learning Through Synaptic Intelligence}},
url = {https://arxiv.org/pdf/1703.04200.pdf}
}
@article{Amodei,
abstract = {Rapid progress in machine learning and artificial intelligence (AI) has brought increasing atten-tion to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function (" avoiding side effects " and " avoiding reward hacking "), an objective function that is too expensive to evaluate frequently (" scalable supervision "), or undesirable behavior during the learning process (" safe exploration " and " distributional shift "). We review previous work in these areas as well as suggesting re-search directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.},
author = {Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Openai, John Schulman and Man{\'{e}}, Dan and Brain, Google},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Amodei et al. - Unknown - Concrete Problems in AI Safety.pdf:pdf},
title = {{Concrete Problems in AI Safety}},
url = {https://arxiv.org/pdf/1606.06565.pdf}
}
@article{Oha,
abstract = {As a step towards developing zero-shot task gen-eralization capabilities in reinforcement learning (RL), we introduce a new RL problem where the agent should learn to execute sequences of in-structions after learning useful skills that solve subtasks. In this problem, we consider two types of generalizations: to previously unseen instruc-tions and to longer sequences of instructions. For generalization over unseen instructions, we pro-pose a new objective which encourages learn-ing correspondences between similar subtasks by making analogies. For generalization over se-quential instructions, we present a hierarchical architecture where a meta controller learns to use the acquired skills for executing the instructions. To deal with delayed reward, we propose a new neural architecture in the meta controller that learns when to update the subtask, which makes learning more efficient. Experimental results on a stochastic 3D domain show that the proposed ideas are crucial for generalization to longer in-structions as well as unseen instructions.},
author = {Oh, Junhyuk and Singh, Satinder and Lee, Honglak and Kohli, Pushmeet},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Oh et al. - Unknown - Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning.pdf:pdf},
title = {{Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning}},
url = {https://arxiv.org/pdf/1706.05064.pdf}
}
@article{Kirkpatrick,
abstract = {The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Neural networks are not, in general, capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks which they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on the MNIST hand written digit dataset and by learning several Atari 2600 games sequentially.},
author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Kirkpatrick et al. - Unknown - Overcoming catastrophic forgetting in neural networks.pdf:pdf},
title = {{Overcoming catastrophic forgetting in neural networks}},
url = {https://arxiv.org/pdf/1612.00796.pdf}
}
@article{Goodfellow,
abstract = {Catastrophic forgetting is a problem faced by many machine learning models and al-gorithms. When trained on one task, then trained on a second task, many machine learning models " forget " how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catas-trophic forgetting problem occurs for mod-ern neural networks, comparing both estab-lished and recent gradient-based training al-gorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catas-trophic forgetting. We find that it is always best to train using the dropout algorithmâ€“ the dropout algorithm is consistently best at adapting to the new task, remembering the old task, and has the best tradeoff curve be-tween these two extremes. We find that dif-ferent tasks and relationships between tasks result in very different rankings of activation function performance. This suggests that the choice of activation function should always be cross-validated.},
author = {Goodfellow, Ian J and Mirza, Mehdi and Xiao, Da and Courville, Aaron and Bengio, Yoshua},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Goodfellow et al. - Unknown - An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks.pdf:pdf},
title = {{An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks}},
url = {https://arxiv.org/pdf/1312.6211.pdf}
}
@article{Ruvolo,
abstract = {The problem of learning multiple consecu-tive tasks, known as lifelong learning, is of great importance to the creation of intelli-gent, general-purpose, and flexible machines. In this paper, we develop a method for on-line multi-task learning in the lifelong learn-ing setting. The proposed Efficient Life-long Learning Algorithm (ELLA) maintains a sparsely shared basis for all task models, transfers knowledge from the basis to learn each new task, and refines the basis over time to maximize performance across all tasks. We show that ELLA has strong connections to both online dictionary learning for sparse coding and state-of-the-art batch multi-task learning methods, and provide robust the-oretical performance guarantees. We show empirically that ELLA yields nearly identi-cal performance to batch multi-task learning while learning tasks sequentially in three or-ders of magnitude (over 1,000x) less time.},
author = {Ruvolo, Paul and Eaton, Eric},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Ruvolo, Eaton - Unknown - ELLA An Efficient Lifelong Learning Algorithm.pdf:pdf},
title = {{ELLA: An Efficient Lifelong Learning Algorithm}},
url = {http://proceedings.mlr.press/v28/ruvolo13.pdf}
}
@article{Williams,
abstract = {â€” We introduce an information theoretic model pre-dictive control (MPC) algorithm capable of handling complex cost criteria and general nonlinear dynamics. The generality of the approach makes it possible to use multi-layer neural networks as dynamics models, which we incorporate into our MPC algorithm in order to solve model-based reinforcement learning tasks. We test the algorithm in simulation on a cart-pole swing up and quadrotor navigation task, as well as on actual hardware in an aggressive driving task. Empirical results demonstrate that the algorithm is capable of achieving a high level of performance and does so only utilizing data collected from the system.},
author = {Williams, Grady and Wagener, Nolan and Goldfain, Brian and Drews, Paul and Rehg, James M and Boots, Byron and Theodorou, Evangelos A},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Williams et al. - Unknown - Information Theoretic MPC for Model-Based Reinforcement Learning.pdf:pdf},
title = {{Information Theoretic MPC for Model-Based Reinforcement Learning}},
url = {http://www.cc.gatech.edu/{~}bboots3/files/InformationTheoreticMPC.pdf}
}
@article{Montgomery,
abstract = {Guided policy search algorithms can be used to optimize complex nonlinear poli-cies, such as deep neural networks, without directly computing policy gradients in the high-dimensional parameter space. Instead, these methods use supervised learning to train the policy to mimic a " teacher " algorithm, such as a trajectory optimizer or a trajectory-centric reinforcement learning method. Guided policy search methods provide asymptotic local convergence guarantees by construction, but it is not clear how much the policy improves within a small, finite number of iterations. We show that guided policy search algorithms can be interpreted as an approximate variant of mirror descent, where the projection onto the constraint manifold is not exact. We derive a new guided policy search algorithm that is sim-pler and provides appealing improvement and convergence guarantees in simplified convex and linear settings, and show that in the more general nonlinear setting, the error in the projection step can be bounded. We provide empirical results on several simulated robotic navigation and manipulation tasks that show that our method is stable and achieves similar or better performance when compared to prior guided policy search methods, with a simpler formulation and fewer hyperparameters.},
author = {Montgomery, William and Levine, Sergey},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Montgomery, Levine - Unknown - Guided Policy Search as Approximate Mirror Descent.pdf:pdf},
title = {{Guided Policy Search as Approximate Mirror Descent}},
url = {https://arxiv.org/pdf/1607.04614.pdf}
}
@article{Chebotar,
abstract = {Reinforcement learning (RL) algorithms for real-world robotic applications need a data-efficient learning process and the ability to handle com-plex, unknown dynamical systems. These re-quirements are handled well by model-based and model-free RL approaches, respectively. In this work, we aim to combine the advantages of these two types of methods in a principled manner. By focusing on time-varying linear-Gaussian poli-cies, we enable a model-based algorithm based on the linear quadratic regulator (LQR) that can be integrated into the model-free framework of path integral policy improvement (PI 2). We can further combine our method with guided pol-icy search (GPS) to train arbitrary parameterized policies such as deep neural networks. Our sim-ulation and real-world experiments demonstrate that this method can solve challenging manipula-tion tasks with comparable or better performance than model-free methods while maintaining the sample efficiency of model-based methods.},
author = {Chebotar, Yevgen and Hausman, Karol and Zhang, Marvin and Sukhatme, Gaurav and Schaal, Stefan and Levine, Sergey},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Chebotar et al. - Unknown - Combining Model-Based and Model-Free Updates for Trajectory-Centric Reinforcement Learning.pdf:pdf},
title = {{Combining Model-Based and Model-Free Updates for Trajectory-Centric Reinforcement Learning}},
url = {https://arxiv.org/pdf/1703.03078.pdf}
}
@article{Klambauer,
abstract = {Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are " scaled exponential linear units " (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance â€” even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization schemes, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs, and other machine learning methods such as random forests and support vector machines. For FNNs we considered (i) ReLU networks without normalization, (ii) batch normalization, (iii) layer normalization, (iv) weight normalization, (v) highway networks, and (vi) residual networks. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: github.com/bioinf-jku/SNNs.},
author = {Klambauer, G{\"{u}}nter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Klambauer et al. - Unknown - Self-Normalizing Neural Networks.pdf:pdf},
title = {{Self-Normalizing Neural Networks}},
url = {https://arxiv.org/pdf/1706.02515.pdf}
}
@article{Watters,
abstract = {From just a glance, humans can make rich predictions about the future state of a wide range of physical systems. On the other hand, modern approaches from engineering, robotics, and graphics are often restricted to narrow domains and require direct measurements of the underlying states. We introduce the Visual Interaction Network, a general-purpose model for learning the dynamics of a physical system from raw visual observations. Our model consists of a perceptual front-end based on convolutional neural networks and a dynamics predictor based on interaction networks. Through joint training, the perceptual front-end learns to parse a dynamic visual scene into a set of factored latent object representations. The dynamics predictor learns to roll these states forward in time by computing their interactions and dynamics, producing a predicted physical trajectory of arbitrary length. We found that from just six input video frames the Visual Interaction Network can generate accurate future trajectories of hundreds of time steps on a wide range of physical systems. Our model can also be applied to scenes with invisible objects, inferring their future states from their effects on the visible objects, and can implicitly infer the unknown mass of objects. Our results demonstrate that the perceptual module and the object-based dynamics predictor module can induce factored latent representations that support accurate dynamical predictions. This work opens new opportunities for model-based decision-making and planning from raw sensory observations in complex physical environments.},
author = {Watters, Nicholas and Tacchetti, Andrea and Weber, Th{\'{e}}ophane and Pascanu, Razvan and Battaglia, Peter and Zoran, Daniel},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Watters et al. - Unknown - Visual Interaction Networks.pdf:pdf},
title = {{Visual Interaction Networks}},
url = {https://arxiv.org/pdf/1706.01433.pdf}
}
@article{Lowe,
abstract = {We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.},
author = {Lowe, Ryan and Wu, Yi and Tamar, Aviv and Harb, Jean and Abbeel, Pieter and Openai, Igor Mordatch},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Lowe et al. - Unknown - Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments.pdf:pdf},
title = {{Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments}},
url = {https://arxiv.org/pdf/1706.02275.pdf}
}
@book{Bender2013,
author = {Bender, Emily},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - full-text(2).pdf:pdf},
title = {{Linguistic Fundamentals for Natural Language Processing}},
year = {2013}
}
@article{Ehrhardt,
abstract = {While the basic laws of Newtonian mechanics are well understood, explaining a physical scenario still requires manually modeling the problem with suitable equations and associated parameters. In order to adopt such models for artificial intelligence, researchers have handcrafted the relevant states, and then used neural networks to learn the state transitions using simulation runs as training data. Un-fortunately, such approaches can be unsuitable for modeling complex real-world scenarios, where manually authoring relevant state spaces tend to be challenging. In this work, we investigate if neural networks can implicitly learn physical states of real-world mechanical processes only based on visual data, and thus enable long-term physical extrapolation. We develop a recurrent neural network architecture for this task and also characterize resultant uncertainties in the form of evolving variance estimates. We evaluate our setup to extrapolate motion of a rolling ball on bowl of varying shape and orientation using only images as input, and report competitive results with approaches that assume access to internal physics models and parameters.},
author = {Ehrhardt, S{\'{e}}bastien and Monszpart, Aron and Vedaldi, Andrea and Mitra, Niloy},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Ehrhardt et al. - Unknown - Learning to Represent Mechanics via Long-term Extrapolation and Interpolation.pdf:pdf},
title = {{Learning to Represent Mechanics via Long-term Extrapolation and Interpolation}},
url = {https://arxiv.org/pdf/1706.02179.pdf}
}
@article{Battaglia,
abstract = {Reasoning about objects, relations, and physics is central to human intelligence, and a key goal of artificial intelligence. Here we introduce the interaction network, a model which can reason about how objects in complex systems interact, supporting dynamical predictions, as well as inferences about the abstract properties of the system. Our model takes graphs as input, performs object-and relation-centric reasoning in a way that is analogous to a simulation, and is implemented using deep neural networks. We evaluate its ability to reason about several challenging physical domains: n-body problems, rigid-body collision, and non-rigid dynamics. Our results show it can be trained to accurately simulate the physical trajectories of dozens of objects over thousands of time steps, estimate abstract quantities such as energy, and generalize automatically to systems with different numbers and configurations of objects and relations. Our interaction network implementation is the first general-purpose, learnable physics engine, and a powerful general framework for reasoning about object and relations in a wide variety of complex real-world domains.},
author = {Battaglia, Peter W and Pascanu, Razvan and Lai, Matthew and Rezende, Danilo and Kavukcuoglu, Koray},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Battaglia et al. - Unknown - Interaction Networks for Learning about Objects, Relations and Physics.pdf:pdf},
title = {{Interaction Networks for Learning about Objects, Relations and Physics}},
url = {https://arxiv.org/pdf/1612.00222.pdf}
}
@article{Theodorou2010,
abstract = {With the goal to generate more scalable algorithms with higher efficiency and fewer open parame-ters, reinforcement learning (RL) has recently moved towards combining classical techniques from optimal control and dynamic programming with modern learning techniques from statistical esti-mation theory. In this vein, this paper suggests to use the framework of stochastic optimal control with path integrals to derive a novel approach to RL with parameterized policies. While solidly grounded in value function estimation and optimal control based on the stochastic Hamilton-Jacobi-Bellman (HJB) equations, policy improvements can be transformed into an approximation problem of a path integral which has no open algorithmic parameters other than the exploration noise. The resulting algorithm can be conceived of as model-based, semi-model-based, or even model free, depending on how the learning problem is structured. The update equations have no danger of numerical instabilities as neither matrix inversions nor gradient learning rates are required. Our new algorithm demonstrates interesting similarities with previous RL research in the framework of probability matching and provides intuition why the slightly heuristically motivated probability matching approach can actually perform well. Empirical evaluations demonstrate significant per-formance improvements over gradient-based policy learning and scalability to high-dimensional control problems. Finally, a learning experiment on a simulated 12 degree-of-freedom robot dog illustrates the functionality of our algorithm in a complex robot learning scenario. We believe that Policy Improvement with Path Integrals (PI 2) offers currently one of the most efficient, numeri-cally robust, and easy to implement algorithms for RL based on trajectory roll-outs.},
author = {Theodorou, Evangelos A and Buchli, Jonas and Org, Jonas@buchli and Schaal, Stefan and Edu, Sschaal@usc},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Theodorou et al. - 2010 - A Generalized Path Integral Control Approach to Reinforcement Learning.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {parameterized policies,reinforcement learning,stochastic optimal control},
pages = {3137--3181},
title = {{A Generalized Path Integral Control Approach to Reinforcement Learning}},
url = {http://www.jmlr.org/papers/volume11/theodorou10a/theodorou10a.pdf},
volume = {11},
year = {2010}
}
@article{Santoro,
abstract = {Relational reasoning is a central component of generally intelligent behavior, but has proven difficult for neural networks to learn. In this paper we describe how to use Relation Networks (RNs) as a simple plug-and-play module to solve problems that fundamentally hinge on relational reasoning. We tested RN-augmented networks on three tasks: visual question answering using a challenging dataset called CLEVR, on which we achieve state-of-the-art, super-human performance; text-based question answering using the bAbI suite of tasks; and complex reasoning about dynamic physical systems. Then, using a curated dataset called Sort-of-CLEVR we show that powerful convolutional networks do not have a general capacity to solve relational questions, but can gain this capacity when augmented with RNs. Our work shows how a deep learning architecture equipped with an RN module can implicitly discover and learn to reason about entities and their relations.},
author = {Santoro, Adam and Raposo, David and Barrett, David G T and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy and London, Deepmind},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Santoro et al. - Unknown - A simple neural network module for relational reasoning.pdf:pdf},
title = {{A simple neural network module for relational reasoning}},
url = {https://arxiv.org/pdf/1706.01427.pdf}
}
@article{Egan,
author = {Egan, Greg},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Egan - Unknown - Crystal Nights.pdf:pdf},
title = {{Crystal Nights}},
url = {http://ttapress.com/downloads/CrystalNights.pdf}
}
@article{Schenck,
abstract = {â€”Recent advances in AI and robotics have claimed many incredible results with deep learning, yet no work to date has applied deep learning to the problem of liquid perception and reasoning. In this paper, we apply fully-convolutional deep neural networks to the tasks of detecting and tracking liquids. We evaluate three models: a single-frame network, multi-frame network, and a LSTM recurrent network. Our results show that the best liquid detection results are achieved when aggregating data over multiple frames, in contrast to standard image segmen-tation. They also show that the LSTM network outperforms the other two in both tasks. This suggests that LSTM-based neural networks have the potential to be a key component for enabling robots to handle liquids using robust, closed-loop controllers.},
author = {Schenck, Connor and Fox, Dieter},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Schenck, Fox - Unknown - Detection and Tracking of Liquids with Fully Convolutional Networks.pdf:pdf},
title = {{Detection and Tracking of Liquids with Fully Convolutional Networks}},
url = {https://arxiv.org/pdf/1606.06266.pdf}
}
@article{Vinyals,
abstract = {Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6{\%} to 93.2{\%} and from 88.0{\%} to 93.8{\%} on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.},
author = {Vinyals, Oriol and Deepmind, Google and Blundell, Charles and Lillicrap, Timothy and Kavukcuoglu, Koray and Wierstra, Daan},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Vinyals et al. - Unknown - Matching Networks for One Shot Learning.pdf:pdf},
title = {{Matching Networks for One Shot Learning}},
url = {https://arxiv.org/pdf/1606.04080.pdf}
}
@inproceedings{zaremba2015execute,
author = {Zaremba, Wojciech and Sutskever, Ilya},
booktitle = {International Conference on Learning Representations (ICLR)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - full-text.pdf:pdf},
title = {{Learning to Execute}},
year = {2015}
}
@article{Sutskever2014,
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excel-lent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the pas-sive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
archivePrefix = {arXiv},
arxivId = {arXiv:1409.3215v3},
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
eprint = {arXiv:1409.3215v3},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Sutskever, Vinyals, Le - 2014 - Sequence to Sequence Learning with Neural Networks.pdf:pdf},
title = {{Sequence to Sequence Learning with Neural Networks}},
year = {2014}
}
@article{Vinyalsa,
abstract = {Syntactic constituency parsing is a fundamental problem in natural language pro-cessing and has been the subject of intensive research and engineering for decades. As a result, the most accurate parsers are domain specific, complex, and in-efficient. In this paper we show that the domain agnostic attention-enhanced sequence-to-sequence model achieves state-of-the-art results on the most widely used syntactic constituency parsing dataset, when trained on a large synthetic cor-pus that was annotated using existing parsers. It also matches the performance of standard parsers when trained only on a small human-annotated dataset, which shows that this model is highly data-efficient, in contrast to sequence-to-sequence models without the attention mechanism. Our parser is also fast, processing over a hundred sentences per second with an unoptimized CPU implementation.},
author = {Vinyals, Oriol and Kaiser, Lukasz and Koo, Terry and Petrov, Slav and Sutskever, Ilya and Hinton, Geoffrey},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Vinyals et al. - Unknown - Grammar as a Foreign Language.pdf:pdf},
title = {{Grammar as a Foreign Language}},
url = {https://arxiv.org/pdf/1412.7449.pdf}
}
@article{Riedmiller,
abstract = {This paper introduces NFQ, an algorithm for efficient and ef-fective training of a Q-value function represented by a multi-layer percep-tron. Based on the principle of storing and reusing transition experiences, a model-free, neural network based Reinforcement Learning algorithm is proposed. The method is evaluated on three benchmark problems. It is shown empirically, that reasonably few interactions with the plant are needed to generate control policies of high quality.},
author = {Riedmiller, Martin},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Riedmiller - Unknown - Neural Fitted Q Iteration -First Experiences with a Data Efficient Neural Reinforcement Learning Method.pdf:pdf},
title = {{Neural Fitted Q Iteration -First Experiences with a Data Efficient Neural Reinforcement Learning Method}},
url = {http://ml.informatik.uni-freiburg.de/{\_}media/publications/rieecml05.pdf}
}
@article{Scott2008,
abstract = {We show that ordinary bananas exhibit closed loops of switched charge versus applied voltage that are nearly identical to those misinterpreted as ferroelectric hysteresis loops in crystals. The 'ferroelectric' properties of bananas are contrasted with those of the real ferroelectric material Ba 2 NaNb 5 O 15 , often nicknamed 'bananas'.},
author = {Scott, J F},
doi = {10.1088/0953-8984/20/02/021001},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Scott - 2008 - Ferroelectrics go bananas.pdf:pdf},
journal = {J. Phys.: Condens. Matter},
pages = {21001--2},
title = {{Ferroelectrics go bananas}},
url = {http://physics.rutgers.edu/{~}pchandra/physics681/bananas.pdf},
volume = {20},
year = {2008}
}
@article{Hutchings,
author = {Hutchings, Michael},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - Introduction to mathematical arguments.pdf:pdf},
title = {{Introduction to mathematical arguments}},
url = {https://math.berkeley.edu/{~}hutching/teach/proofs.pdf}
}
@article{Dietterich,
author = {Dietterich, Thomas and Bishop, Christopher and Heckerman, David and Jordan, Michael and Kearns, Michael and Sutton, Richard S and Barto, Andrew G},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Dietterich et al. - Unknown - Probabilistic Graphical Models Adaptive Computation and Machine Learning.pdf:pdf},
title = {{Probabilistic Graphical Models Adaptive Computation and Machine Learning}},
url = {http://miruvor.weebly.com/uploads/2/3/9/3/23933635/probabilistic{\_}graphical{\_}models.pdf}
}
@article{Gregora,
abstract = {In this paper we introduce a new unsupervised reinforcement learning method for discovering the set of intrinsic options available to an agent. This set is learned by maximizing the number of different states an agent can reliably reach, as measured by the mutual information between the set of options and option termination states. To this end, we instantiate two policy gradient based algorithms, one that creates an explicit embedding space of options and one that represents options implicitly. The algorithms also provide an explicit measure of empowerment in a given state that can be used by an empowerment maximizing agent. The algorithm scales well with function approximation and we demonstrate the applicability of the algorithm on a range of tasks.},
author = {Gregor, Karol and Rezende, Danilo and Deepmind, Daan Wierstra},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Gregor, Rezende, Deepmind - Unknown - VARIATIONAL INTRINSIC CONTROL.pdf:pdf},
title = {{Variational Intrinsic Control}},
url = {https://arxiv.org/pdf/1611.07507.pdf}
}
@article{Walker,
abstract = {Current approaches in video forecasting attempt to gen-erate videos directly in pixel space using Generative Ad-versarial Networks (GANs) or Variational Autoencoders (VAEs). However, since these approaches try to model all the structure and scene dynamics at once, in unconstrained settings they often generate uninterpretable results. Our in-sight is to model the forecasting problem at a higher level of abstraction. Specifically, we exploit human pose detec-tors as a free source of supervision and break the video forecasting problem into two discrete steps. First we ex-plicitly model the high level structure of active objects in the sceneâ€”humansâ€”and use a VAE to model the possible future movements of humans in the pose space. We then use the future poses generated as conditional information to a GAN to predict the future frames of the video in pixel space. By using the structured space of pose as an intermediate representation, we sidestep the problems that GANs have in generating video pixels directly. We show through quantita-tive and qualitative evaluation that our method outperforms state-of-the-art methods for video prediction.},
author = {Walker, Jacob and Marino, Kenneth and Gupta, Abhinav and Hebert, Martial},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Walker et al. - Unknown - The Pose Knows Video Forecasting by Generating Pose Futures.pdf:pdf},
title = {{The Pose Knows: Video Forecasting by Generating Pose Futures}},
url = {https://arxiv.org/pdf/1705.00053.pdf}
}
@article{Writer,
author = {Writer, The Anonymous},
file = {:Users/ashvin/code/kindlize/pdfs/books/Kabuliwala-by-Rabindranath-Tagore.pdf:pdf},
title = {{KABULIWALA Rabindranath Tagore}}
}
@article{BinPeng,
abstract = {The use of deep reinforcement learning allows for high-dimensional state descrip-tors, but little is known about how the choice of action representation impacts the learning difficulty and the resulting performance. We compare the impact of four different action parameterizations (torques, muscle-activations, target joint angles, and target joint-angle velocities) in terms of learning time, policy robust-ness, motion quality, and policy query rates. Our results are evaluated on a gait-cycle imitation task for multiple planar articulated figures and multiple gaits. We demonstrate that the local feedback provided by higher-level action parameteriza-tions can significantly impact the learning, robustness, and quality of the resulting policies.},
author = {{Bin Peng}, Xue and van de Panne, Michiel},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Bin Peng, van de Panne - Unknown - LEARNING LOCOMOTION SKILLS USING DEEPRL DOES THE CHOICE OF ACTION SPACE MATTER.pdf:pdf},
title = {{LEARNING LOCOMOTION SKILLS USING DEEPRL: DOES THE CHOICE OF ACTION SPACE MATTER?}},
url = {https://arxiv.org/pdf/1611.01055.pdf}
}
@misc{Gal,
author = {Gal, Yarin},
title = {{What my deep model doesn't know...}},
url = {http://mlg.eng.cam.ac.uk/yarin/blog{\_}3d801aa532c1ce.html},
urldate = {2017-04-28}
}
@article{Gandhi,
abstract = {â€” How do you learn to navigate an Unmanned Aerial Vehicle (UAV) and avoid obstacles? One approach is to use a small dataset collected by human experts: however, high capacity learning algorithms tend to overfit when trained with little data. An alternative is to use simulation. But the gap between simulation and real world remains large especially for perception problems. The reason most research avoids using large-scale real data is the fear of crashes! In this paper, we propose to bite the bullet and collect a dataset of crashes itself! We build a drone whose sole purpose is to crash into objects: it samples naive trajectories and crashes into random objects. We crash our drone 11,500 times to create one of the biggest UAV crash dataset. This dataset captures the different ways in which a UAV can crash. We use all this negative flying data in con-junction with positive data sampled from the same trajectories to learn a simple yet powerful policy for UAV navigation. We show that this simple self-supervised model is quite effective in navigating the UAV even in extremely cluttered environments with dynamic obstacles including humans. For supplementary video see: https://youtu.be/HbHqC8HimoI},
author = {Gandhi, Dhiraj and Pinto, Lerrel and Gupta, Abhinav},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Gandhi, Pinto, Gupta - Unknown - Learning to Fly by Crashing.pdf:pdf},
title = {{Learning to Fly by Crashing}},
url = {https://arxiv.org/pdf/1704.05588.pdf}
}
@article{Schulman,
abstract = {Two of the leading approaches for model-free reinforcement learning are policy gradient methods and Q-learning methods. Q-learning methods can be effective and sample-efficient when they work, however, it is not well-understood why they work, since empirically, the Q-values they estimate are very inaccurate. A partial explanation may be that Q-learning methods are secretly implementing policy gradient updates: we show that there is a precise equivalence between Q-learning and policy gradient methods in the setting of entropy-regularized reinforcement learning, that " soft " (entropy-regularized) Q-learning is exactly equivalent to a policy gradient method. We also point out a connection between Q-learning methods and natural policy gradient methods. Experimentally, we explore the entropy-regularized versions of Q-learning and policy gradients, and we find them to perform as well as (or slightly better than) the standard variants on the Atari benchmark. We also show that the equivalence holds in practical settings by constructing a Q-learning method that closely matches the learning dynamics of A3C without using a target network or exploration schedule.},
author = {Schulman, John and Chen, Xi and Abbeel, Pieter},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Schulman, Chen, Abbeel - Unknown - Equivalence Between Policy Gradients and Soft Q-Learning.pdf:pdf},
title = {{Equivalence Between Policy Gradients and Soft Q-Learning}},
url = {https://arxiv.org/pdf/1704.06440.pdf}
}
@article{Toussaint2012,
author = {Toussaint, Marc},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Toussaint - 2012 - Lecture Notes Some notes on gradient descent.pdf:pdf},
title = {{Lecture Notes: Some notes on gradient descent}},
url = {https://ipvs.informatik.uni-stuttgart.de/mlr/marc/notes/gradientDescent.pdf},
year = {2012}
}
@article{Doersch2016,
abstract = {In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, in-cluding handwritten digits [1, 2], faces [1, 3, 4], house numbers [5, 6], CIFAR images [6], physical models of scenes [4], segmentation [7], and predicting the future from static images [8]. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
author = {Doersch, Carl},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Doersch - 2016 - Tutorial on Variational Autoencoders.pdf:pdf},
keywords = {neural networks,structured prediction,unsupervised learning,variational autoencoders},
title = {{Tutorial on Variational Autoencoders}},
url = {https://arxiv.org/pdf/1606.05908.pdf},
year = {2016}
}
@article{Kang,
abstract = {Video is one of the fastest-growing sources of data and is rich with interesting semantic information. Furthermore, recent advances in computer vision, in the form of deep convolutional neural networks (CNNs), have made it possible to query this semantic information with near-human accuracy (in the form of image tagging). However, performing inference with state-of-the-art CNNs is computationally expensive: analyzing videos in real time (at 30 frames/sec) requires a {\$}1200 GPU per video stream, posing a serious computational barrier to CNN adoption in large-scale video data management systems. In response, we present NOSCOPE, a system that uses cost-based optimization to assemble a specialized video processing pipeline for each input video stream, greatly accelerating subsequent CNN-based queries on the video. As NOSCOPE observes a video, it trains two types of pipeline components (which we call filters) to exploit the locality in the video stream: difference detectors that exploit temporal locality between frames, and specialized models that are tailored to a specific scene and query (i.e., exploit environmental and query-specific locality). We show that the optimal set of filters and their parameters depends significantly on the video stream and query in question, so NOSCOPE introduces an efficient cost-based optimizer for this problem to select them. With this approach, our NOSCOPE prototype achieves up to 120-3,200Ã— speed-ups (318-8,500Ã— real-time) on binary classification tasks over real-world webcam and surveillance video while maintaining accuracy within 1-5{\%} of a state-of-the-art CNN.},
author = {Kang, Daniel and Emmons, John and Abuzaid, Firas and Bailis, Peter and Zaharia, Matei and Infolab, Stanford},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Kang et al. - Unknown - Optimizing Deep CNN-Based Queries over Video Streams at Scale.pdf:pdf},
title = {{Optimizing Deep CNN-Based Queries over Video Streams at Scale}},
url = {https://arxiv.org/pdf/1703.02529.pdf}
}
@article{Chiappa,
abstract = {Models that can simulate how environments change in response to actions can be used by agents to plan and act efficiently. We improve on previous environment simulators from high-dimensional pixel observations by introducing recurrent neural networks that are able to make temporally and spatially coherent predictions for hundreds of time-steps into the future. We present an in-depth analysis of the factors affecting performance, providing the most extensive attempt to advance the understanding of the properties of these models. We address the issue of computationally inefficiency with a model that does not need to generate a high-dimensional image at each time-step. We show that our approach can be used to improve exploration and is adaptable to many diverse environments, namely 10 Atari games, a 3D car racing environment, and complex 3D mazes.},
author = {Chiappa, Silvia and Racaniere, S{\'{e}}bastien and Wierstra, Daan and Mohamed, Shakir},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Chiappa et al. - Unknown - RECURRENT ENVIRONMENT SIMULATORS.pdf:pdf},
title = {{RECURRENT ENVIRONMENT SIMULATORS}},
url = {https://arxiv.org/pdf/1704.02254.pdf}
}
@article{Li,
abstract = {The goal of imitation learning is to match ex-ample expert behavior, without access to a re-inforcement signal. Expert demonstrations pro-vided by humans, however, often show signifi-cant variability due to latent factors that are not explicitly modeled. We introduce an extension to the Generative Adversarial Imitation Learn-ing method that can infer the latent structure of human decision-making in an unsupervised way. Our method can not only imitate complex be-haviors, but also learn interpretable and mean-ingful representations. We demonstrate that the approach is applicable to high-dimensional en-vironments including raw visual inputs. In the highway driving domain, we show that a model learned from demonstrations is able to both pro-duce different styles of human-like driving be-haviors and accurately anticipate human actions. Our method surpasses various baselines in terms of performance and functionality.},
author = {Li, Yunzhu and Song, Jiaming and Ermon, Stefano},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Li, Song, Ermon - Unknown - Inferring The Latent Structure of Human Decision-Making from Raw Visual Inputs.pdf:pdf},
title = {{Inferring The Latent Structure of Human Decision-Making from Raw Visual Inputs}},
url = {https://arxiv.org/pdf/1703.08840.pdf}
}
@article{Krishnan,
abstract = {Kalman Filters are one of the most influential models of time-varying phenomena. They admit an intuitive probabilistic interpretation, have a simple functional form, and enjoy widespread adoption in a variety of disciplines. Motivated by recent variational methods for learning deep generative models, we introduce a unified algorithm to efficiently learn a broad spectrum of Kalman filters. Of particular interest is the use of temporal generative models for counterfactual inference. We investigate the efficacy of such models for counterfactual inference, and to that end we introduce the " Healing MNIST " dataset where long-term structure, noise and actions are applied to sequences of digits. We show the efficacy of our method for modeling this dataset. We further show how our model can be used for coun-terfactual inference for patients, based on electronic health record data of 8,000 patients over 4.5 years.},
author = {Krishnan, Rahul G and Shalit, Uri and Sontag, David},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Krishnan, Shalit, Sontag - Unknown - Deep Kalman Filters.pdf:pdf},
title = {{Deep Kalman Filters}},
url = {https://arxiv.org/pdf/1511.05121.pdf}
}
@article{Gub,
abstract = {Deep neural networks are powerful parametric models that can be trained effi-ciently using the backpropagation algorithm. Stochastic neural networks com-bine the power of large parametric functions with that of graphical models, which makes it possible to learn very complex distributions. However, as backpropaga-tion is not directly applicable to stochastic networks that include discrete sampling operations within their computational graph, training such networks remains diffi-cult. We present MuProp, an unbiased gradient estimator for stochastic networks, designed to make this task easier. MuProp improves on the likelihood-ratio esti-mator by reducing its variance using a control variate based on the first-order Tay-lor expansion of a mean-field network. Crucially, unlike prior attempts at using backpropagation for training stochastic networks, the resulting estimator is unbi-ased and well behaved. Our experiments on structured output prediction and dis-crete latent variable modeling demonstrate that MuProp yields consistently good performance across a range of difficult tasks.},
author = {Gu, Shixiang and Levine, Sergey and Sutskever, Ilya and Mnih, Andriy},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Gu et al. - Unknown - MUPROP UNBIASED BACKPROPAGATION FOR STOCHASTIC NEURAL NETWORKS.pdf:pdf},
title = {{MUPROP: UNBIASED BACKPROPAGATION FOR STOCHASTIC NEURAL NETWORKS}},
url = {https://arxiv.org/pdf/1511.05176.pdf}
}
@article{Gregor,
abstract = {This paper introduces the Deep Recurrent Atten-tive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distin-guished from real data with the naked eye.},
author = {Gregor, Karol and Danihelka, Ivo and Graves, Alex and {Jimenez Rezende}, Danilo and Wierstra, Daan},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Gregor et al. - Unknown - DRAW A Recurrent Neural Network For Image Generation.pdf:pdf},
title = {{DRAW: A Recurrent Neural Network For Image Generation}},
url = {https://arxiv.org/pdf/1502.04623.pdf}
}
@article{Zhu2016,
author = {Zhu, Richard and Kang, Andrew},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Zhu, Kang - 2016 - Imitation Learning.pdf:pdf},
title = {{Imitation Learning}},
url = {http://www.yisongyue.com/courses/cs159/lectures/imitation-learning-3.pdf},
year = {2016}
}
@article{Berthelot,
abstract = {We propose a new equilibrium enforcing method paired with a loss derived from the Wasserstein distance for training auto-encoder based Generative Adversarial Networks. This method balances the generator and discriminator during training. Additionally, it provides a new approximate convergence measure, fast and stable training and high visual quality. We also derive a way of controlling the trade-off between image diversity and visual quality. We focus on the image generation task, setting a new milestone in visual quality, even at higher resolutions. This is achieved while using a relatively simple model architecture and a standard training procedure.},
author = {Berthelot, David and Schumm, Tom and Metz, Luke},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Berthelot, Schumm, Metz - Unknown - BEGAN Boundary Equilibrium Generative Adversarial Networks.pdf:pdf},
title = {{BEGAN: Boundary Equilibrium Generative Adversarial Networks}},
url = {https://arxiv.org/pdf/1703.10717.pdf}
}
@article{Pinto,
abstract = {Deep neural networks coupled with fast simula-tion and improved computation have led to re-cent successes in the field of reinforcement learn-ing (RL). However, most current RL-based ap-proaches fail to generalize since: (a) the gap be-tween simulation and real world is so large that policy-learning approaches fail to transfer; (b) even if policy learning is done in real world, the data scarcity leads to failed generalization from training to test scenarios (e.g., due to different friction or object masses). Inspired from H âˆž control methods, we note that both modeling er-rors and differences in training and test scenar-ios can be viewed as extra forces/disturbances in the system. This paper proposes the idea of ro-bust adversarial reinforcement learning (RARL), where we train an agent to operate in the pres-ence of a destabilizing adversary that applies dis-turbance forces to the system. The jointly trained adversary is reinforced â€“ that is, it learns an op-timal destabilization policy. We formulate the policy learning as a zero-sum, minimax objec-tive function. Extensive experiments in multiple environments (InvertedPendulum, HalfCheetah, Swimmer, Hopper and Walker2d) conclusively demonstrate that our method (a) improves train-ing stability; (b) is robust to differences in train-ing/test conditions; and c) outperform the base-line even in the absence of the adversary.},
author = {Pinto, Lerrel and Davidson, James and Sukthankar, Rahul and Gupta, Abhinav},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Pinto et al. - Unknown - Robust Adversarial Reinforcement Learning.pdf:pdf},
title = {{Robust Adversarial Reinforcement Learning}},
url = {https://arxiv.org/pdf/1703.02702.pdf}
}
@article{Hanna2017,
abstract = {Robot learning in simulation is a promising alternative to the prohibitive sample cost of learning in the physical world. Unfortunately, policies learned in simulation often perform worse than hand-coded policies when applied on the physi-cal robot. Grounded simulation learning (GSL) promises to address this issue by altering the simulator to better match the real world. This paper proposes a new algorithm for GSL â€“ Grounded Action Transformation â€“ and applies it to learn-ing of humanoid bipedal locomotion. Our approach results in a 43.27{\%} improvement in forward walk velocity compared to a state-of-the art hand-coded walk. We further evaluate our methodology in controlled experiments using a second, higher-fidelity simulator in place of the real world. Our results contribute to a deeper understanding of grounded simulation learning and demonstrate its effectiveness for learning robot control policies.},
author = {Hanna, Josiah P and Stone, Peter},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Hanna, Stone - 2017 - Grounded Action Transformation for Robot Learning in Simulation.pdf:pdf},
title = {{Grounded Action Transformation for Robot Learning in Simulation}},
url = {https://www.cs.utexas.edu/{~}AustinVilla/papers/AAAI17-Hanna.pdf},
year = {2017}
}
@article{Luan,
abstract = {Figure 1: Given a reference style image (a) and an input image (b), we seek to create an output image of the same scene as the input, but with the style of the reference image. The Neural Style algorithm [5] (c) successfully transfers colors, but also introduces distortions that make the output look like a painting, which is undesirable in the context of photo style transfer. In comparison, our result (d) transfers the color of the reference style image equally well while preserving the photorealism of the output. On the right (e), we show 3 insets of (b), (c), and (d) (in that order). Zoom in to compare results. Abstract This paper introduces a deep-learning approach to pho-tographic style transfer that handles a large variety of image content while faithfully transferring the reference style. Our approach builds upon recent work on painterly transfer that separates style from the content of an image by considering different layers of a neural network. However, as is, this ap-proach is not suitable for photorealistic style transfer. Even when both the input and reference images are photographs, the output still exhibits distortions reminiscent of a painting. Our contribution is to constrain the transformation from the input to the output to be locally affine in colorspace, and to express this constraint as a custom CNN layer through which we can backpropagate. We show that this approach successfully suppresses distortion and yields satisfying pho-torealistic style transfers in a broad variety of scenarios, including transfer of the time of day, weather, season, and artistic edits.},
author = {Luan, Fujun and Adobe, Sylvain Paris and {Shechtman Adobe}, Eli and Bala, Kavita},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Luan et al. - Unknown - Deep Photo Style Transfer.pdf:pdf},
title = {{Deep Photo Style Transfer}},
url = {https://arxiv.org/pdf/1703.07511.pdf}
}
@article{Kakade,
abstract = {We provide a natural gradient method that represents the steepest descent direction based on the underlying structure of the param-eter space. Although gradient methods cannot make large changes in the values of the parameters, we show that the natural gradi-ent is moving toward choosing a greedy optimal action rather than just a better action. These greedy optimal actions are those that would be chosen under one improvement step of policy iteration with approximate, compatible value functions, as defined by Sut-ton et al. [9]. We then show drastic performance improvements in simple MDPs and in the more challenging MDP of Tetris.},
author = {Kakade, Sham},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Kakade - Unknown - A Natural Policy Gradient.pdf:pdf},
title = {{A Natural Policy Gradient}},
url = {http:}
}
@inproceedings{stadie2016exploration,
author = {Stadie, Bradly C and Levine, Sergey and Abbeel, Pieter},
booktitle = {International Conference on Learning Representations (ICLR)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - INCENTIVIZING EXPLORATION IN REINFORCEMENT LEARNING WITH DEEP PREDICTIVE MODELS.pdf:pdf},
title = {{Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models}},
url = {https://arxiv.org/pdf/1507.00814.pdf},
year = {2016}
}
@article{Griffiths2014,
abstract = {a b s t r a c t The cognitive revolution offered an alternative to merely analyzing human behavior, using the notion of computation to rigorously express hypotheses about the mind. Computation also gives us new tools for testing these hypotheses â€“ large behavioral databases generated by human interactions with computers and with one another. This kind of data is typically analyzed by computer scientists, who focus on predicting people's behavior based on their history. A new cognitive revolution is needed, demonstrating the value of minds as inter-vening variables in these analyses and using the results to evaluate models of human cognition. {\'{O}} 2014 Elsevier B.V. All rights reserved. Over 60 years ago, the cognitive revolution made legit-imate the scientific study of the mind (Gardner, 1987; Miller, 2003). Formal models of cognition made it possible to postulate processes that lie between a person's history and their actions, offering an alternative to the rigid stimulus-response structure of Behaviorism. Using new mathematical ideas â€“ in particular, the notion of computa-tion â€“ a generation of researchers discovered a way to rigorously state hypotheses about how human minds work. I believe that we stand on the brink of a new revolu-tion, with equally far-reaching consequences and an equally important role for computation. A revolution in how we test those hypotheses. While the decades since the cognitive revolution have seen significant innovations in the kinds of computational models researchers have explored, the methods used to evaluate those models have remained fundamentally the same. In fact, those methods have arguably remained the same for over a century, being based on the small-scale laboratory science that characterized the first psychologi-cal research (Mandler, 2007). If you want to answer a question about the human mind (or publish a paper in Cog-nition) you formulate some hypotheses, bring an appropri-ate number of people into the laboratory, and have them carry out a task that distinguishes between those hypotheses. But while we have remained focused on the events in our laboratories, the world outside those laboratories has changed. The internet offers a way to reach thousands of people in seconds. Human lives are lived more and more through our computers and our mobile phones. And the people with the most data about human behavior are no longer psychologists. They are computer scientists. The mouse clicks and keystrokes of our online interac-tions are data, and figuring out how to make the best use of those data has become an important part of computer science. Recommendation systems that tell you which books you might be interested in, services that suggest related news stories, search engines that make use of the tags people apply to images, algorithms that select the advertisements you are most likely to click on. . . all are sig-nificant areas of research in computer science, and all are fundamentally based on the study of human behavior. They are also all missed opportunities for cognitive science. Recommendation systems need to divine human pref-erences â€“ a problem that has been studied by both},
author = {Griffiths, Thomas L},
doi = {10.1016/j.cognition.2014.11.026},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Griffiths - 2014 - Manifesto for a new (computational) cognitive revolution.pdf:pdf},
journal = {COGNITION},
keywords = {Big data,Computational modeling,Crowdsourcing},
title = {{Manifesto for a new (computational) cognitive revolution}},
url = {http://dx.doi.org/10.1016/j.cognition.2014.11.026},
year = {2014}
}
@article{Britz,
abstract = {Neural Machine Translation (NMT) has shown remarkable progress over the past few years with production systems now being deployed to end-users. One major drawback of current architectures is that they are expensive to train, typically re-quiring days to weeks of GPU time to converge. This makes exhaustive hyper-parameter search, as is commonly done with other neural network architectures, prohibitively expensive. In this work, we present the first large-scale analy-sis of NMT architecture hyperparameters. We report empirical results and variance numbers for several hundred experimental runs, corresponding to over 250,000 GPU hours on the standard WMT English to German translation task. Our experiments lead to novel insights and practical advice for building and extending NMT architec-tures. As part of this contribution, we release an open-source NMT framework 1 that enables researchers to easily experi-ment with novel techniques and reproduce state of the art results.},
author = {Britz, Denny and Goldie, Anna and Luong, Minh-Thang and Le, Quoc and Brain, Google},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Britz et al. - Unknown - Massive Exploration of Neural Machine Translation Architectures.pdf:pdf},
title = {{Massive Exploration of Neural Machine Translation Architectures}},
url = {https://arxiv.org/pdf/1703.03906.pdf}
}
@article{He2017,
abstract = {Recent work has shown that deep neural networks are capable of approximating both value functions and policies in reinforcement learning domains featuring con-tinuous state and action spaces. However, to the best of our knowledge no previous work has succeeded at using deep neural networks in structured (parameterized) continuous action spaces. To fill this gap, this paper focuses on learning within the domain of simulated RoboCup soccer, which features a small set of discrete action types, each of which is parameterized with continuous variables. The best learned agents can score goals more reliably than the 2012 RoboCup champion agent. As such, this paper represents a successful extension of deep reinforcement learning to the class of parameterized action space MDPs.},
archivePrefix = {arXiv},
arxivId = {arXiv:1511.06434v1},
author = {He, Ji and Chen, Jianshu and He, Xiaodong and Gao, Jianfeng and Li, Lihong and Deng, Li and Ostendorf, Mari},
doi = {10.1051/0004-6361/201527329},
eprint = {arXiv:1511.06434v1},
file = {:Users/ashvin/code/kindlize/pdfs/deeprl/rl-squared-Duan16.pdf:pdf},
isbn = {2004012439},
issn = {0004-6361},
journal = {International Conference on Learning Representations (ICLR)},
number = {1999},
pages = {1--17},
pmid = {23459267},
title = {{RL{\^{}}2: Fast Reinforcement Learning via Slow Reinforcement Learning}},
year = {2017}
}
@article{Guo2015,
author = {Guo, Philip J},
file = {:Users/ashvin/code/kindlize/pdfs/deeprl/pguo-PhD-grind.pdf:pdf},
title = {{pguo-PhD-grind}},
year = {2015}
}
@article{Graves2014,
abstract = {We extend the capabilities of neural networks by coupling them to external memory re-sources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demon-strate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
archivePrefix = {arXiv},
arxivId = {arXiv:1410.5401v2},
author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
doi = {10.3389/neuro.12.006.2007},
eprint = {arXiv:1410.5401v2},
file = {:Users/ashvin/code/kindlize/pdfs/deeprl/neural-turing-machines-Graves14.pdf:pdf},
isbn = {0028-0836},
issn = {2041-1723},
journal = {CoRR},
pages = {1--26},
pmid = {18958277},
title = {{Neural Turing Machines}},
url = {http://arxiv.org/abs/1410.5401},
volume = {abs/1410.5},
year = {2014}
}
@article{Oord2016,
abstract = {Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.},
archivePrefix = {arXiv},
arxivId = {1601.06759},
author = {van den Oord, Aaron and Kalchbrenner, Nal and Kavukcuoglu, Koray},
eprint = {1601.06759},
file = {:Users/ashvin/code/kindlize/pdfs/deeprl/pixel-rnns-Oord16.pdf:pdf},
isbn = {9781510829008},
journal = {International Conference on Machine Learning (ICML)},
title = {{Pixel Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1601.06759},
volume = {48},
year = {2016}
}
@article{VanHasselt2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1509.06461v1},
author = {van Hasselt, H and Guez, A and Silver, D},
doi = {10.1016/j.artint.2015.09.002},
eprint = {arXiv:1509.06461v1},
file = {:Users/ashvin/code/kindlize/pdfs/deeprl/double-dqn-Hasselt16.pdf:pdf},
isbn = {9781577357605},
issn = {00043702},
journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
pmid = {26150344},
title = {{Deep reinforcement learning with double Q-learning}},
year = {2016}
}
@article{Bousmalis2016,
abstract = {The cost of large scale data collection and annotation often makes the application of machine learning algorithms to new tasks or datasets prohibitively expensive. One approach circumventing this cost is training models on synthetic data where annotations are provided automatically. Despite their appeal, such models often fail to generalize from synthetic to real images, necessitating domain adaptation algorithms to manipulate these models before they can be successfully applied. Ex-isting approaches focus either on mapping representations from one domain to the other, or on learning to extract features that are invariant to the domain from which they were extracted. However, by focusing only on creating a mapping or shared representation between the two domains, they ignore the individual characteristics of each domain. We suggest that explicitly modeling what is unique to each domain can improve a model's ability to extract domainâ€“invariant features. Inspired by work on privateâ€“shared component analysis, we explicitly learn to extract image representations that are partitioned into two subspaces: one component which is private to each domain and one which is shared across domains. Our model is trained not only to perform the task we care about in the source domain, but also to use the partitioned representation to reconstruct the images from both domains. Our novel architecture results in a model that outperforms the stateâ€“ofâ€“theâ€“art on a range of unsupervised domain adaptation scenarios and additionally produces visualizations of the private and shared representations enabling interpretation of the domain adaptation process.},
archivePrefix = {arXiv},
arxivId = {1608.06019},
author = {Bousmalis, Konstantinos and Trigeorgis, George and Silberman, Nathan and Krishnan, Dilip and Erhan, Dumitru},
eprint = {1608.06019},
file = {:Users/ashvin/code/kindlize/pdfs/deeprl/domain-separation-networks-Bousmalis2016.pdf:pdf},
journal = {Advances in Neural Information Processing Systems (NIPS)},
number = {Nips},
title = {{Domain Separation Networks}},
year = {2016}
}
@article{Finn2016,
abstract = {A core challenge for an agent learning to interact with the world is to predict how its actions affect objects in its environment. Many existing methods for learning the dynamics of physical interactions require labeled object information. However, to scale real-world interaction learning to a variety of scenes and objects, acquiring labeled data becomes increasingly impractical. To learn about physical object motion without labels, we develop an action-conditioned video prediction model that explicitly models pixel motion, by predicting a distribution over pixel motion from previous frames. Because our model explicitly predicts motion, it is partially invariant to object appearance, enabling it to generalize to previously unseen objects. To explore video prediction for real-world interactive agents, we also introduce a dataset of 50,000 robot interactions involving pushing motions, including a test set with novel objects. In this dataset, accurate prediction of videos conditioned on the robot's future actions amounts to learning a "visual imagination" of different futures based on different courses of action. Our experiments show that our proposed method not only produces more accurate video predictions, but also more accurately predicts object motion, when compared to prior methods.},
archivePrefix = {arXiv},
arxivId = {1605.07157},
author = {Finn, Chelsea and Goodfellow, Ian and Levine, Sergey},
eprint = {1605.07157},
file = {:Users/ashvin/code/kindlize/pdfs/deeprl/unsupervised-learning-physical-interaction-through-video-prediction-Finn16.pdf:pdf},
journal = {Advances in Neural Information Processing Systems (NIPS)},
title = {{Unsupervised Learning for Physical Interaction through Video Prediction}},
url = {http://arxiv.org/abs/1605.07157},
year = {2016}
}
@article{Fernando2017,
archivePrefix = {arXiv},
arxivId = {1701.08734},
author = {Fernando, Chrisantha and Banarse, Dylan and Blundell, Charles and Zwols, Yori and Ha, David and Rusu, Andrei A. and Pritzel, Alexander and Wierstra, Daan},
eprint = {1701.08734},
file = {:Users/ashvin/code/kindlize/pdfs/deeprl/pathnet-Fernando17.pdf:pdf},
title = {{PathNet: Evolution Channels Gradient Descent in Super Neural Networks}},
year = {2017}
}
@article{Hardt2004,
archivePrefix = {arXiv},
arxivId = {arXiv:1611.03530v2},
author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
eprint = {arXiv:1611.03530v2},
file = {:Users/ashvin/code/kindlize/pdfs/deeprl/understanding-deep-learning-rethinking-generalization-Zhang16.pdf:pdf},
journal = {CoRR},
title = {{Understanding Deep Learning Requires Rethinking Generalization}},
year = {2016}
}
@article{Lia,
author = {Li, Yuxi},
file = {:Users/ashvin/code/kindlize/pdfs/deeprl/deep-rl-overview-Li16.pdf:pdf},
title = {{Deep Reinforcement Learning: An Overview}}
}
@article{Tang2017,
author = {Tang, Shuai and Jolla, La and Jin, Hailin and Fang, Chen and Wang, Zhaowen and Jose, San},
file = {:Users/ashvin/code/kindlize/pdfs/deeprl/dcgan-Radford16.pdf:pdf},
pages = {1--10},
title = {{Unsupervised Sentence Representation Learning}},
year = {2017}
}
@article{Fukui2016,
abstract = {Modeling textual or visual information with vector representations trained from large language or visual datasets has been successfully explored in recent years. However, tasks such as visual question answering require combining these vector representations with each other. Approaches to multimodal pooling include element-wise multiplication or addition, as well as concatenation of the visual and textual representations. We hypothesize that these methods are not as expressive as an outer product of the visual and textual vectors. As the outer product is typically infeasible due to its high dimensionality, we instead propose utilizing Multimodal Compact Bilinear pooling (MCB) to efficiently and expressively combine multimodal features. We extensively evaluate MCB on the visual question answering and grounding tasks. We consistently show the benefit of MCB over ablations without MCB. For visual question answering, we present an architecture which uses MCB twice, once for predicting attention over spatial features and again to combine the attended representation with the question representation. This model outperforms the state-of-the-art on the Visual7W dataset and the VQA challenge.},
archivePrefix = {arXiv},
arxivId = {1606.01847},
author = {Fukui, Akira and Park, Dong Huk and Yang, Daylen and Rohrbach, Anna and Darrell, Trevor and Rohrbach, Marcus},
eprint = {1606.01847},
file = {:Users/ashvin/code/kindlize/pdfs/deeprl/compact-bilinear-pooling-visual-qa-Fukui16.pdf:pdf},
isbn = {978-1-945626-25-8},
journal = {Conference on Empirical Methods in Natural Language Processing (EMNLP)},
pages = {9},
pmid = {121187},
title = {{Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding}},
url = {http://arxiv.org/abs/1606.01847},
year = {2016}
}
@article{Kaufmann2016,
abstract = {In this paper we propose and investigate a novel end-to-end method for automatically generating short email responses, called Smart Reply. It generates semantically diverse sug- gestions that can be used as complete email responses with just one tap on mobile. The system is currently used in In- box by Gmail and is responsible for assisting with 10{\%} of all mobile responses. It is designed to work at very high throughput and process hundreds of millions of messages daily. The system exploits state-of-the-art, large-scale deep learning. We describe the architecture of the system as well as the challenges that we faced while building it, like response di- versity and scalability. We also introduce a new method for semantic clustering of user-generated content that requires only a modest amount of explicitly labeled data.},
archivePrefix = {arXiv},
arxivId = {arXiv:1606.04870v1},
author = {Kaufmann, Tobias and Ravi, Sujith},
doi = {http://dx.doi.org/10.1145/2939672.2939801},
eprint = {arXiv:1606.04870v1},
file = {:Users/ashvin/code/kindlize/pdfs/deeprl/smart-reply-Kannan16.pdf:pdf},
isbn = {9781450342322},
issn = {0146-4833},
journal = {Kdd '16},
keywords = {clustering,deep learning,email,lstm,semantics},
title = {{Smart Reply : Automated Response Suggestion for Email}},
year = {2016}
}
@article{Oord2001,
author = {van den Oord, Aaron},
file = {:Users/ashvin/code/kindlize/pdfs/deeprl/wavenet-Oord16.pdf:pdf},
number = {c},
title = {{Wavenet: A Generative Model for Raw Audio}},
year = {2001}
}
@article{Mordatch,
abstract = {By capturing statistical patterns in large cor-pora, machine learning has enabled significant advances in natural language processing, includ-ing in machine translation, question answering, and sentiment analysis. However, for agents to intelligently interact with humans, simply cap-turing the statistical patterns is insufficient. In this paper we investigate if, and how, grounded compositional language can emerge as a means to achieve goals in multi-agent populations. To-wards this end, we propose a multi-agent learn-ing environment and learning methods that bring about emergence of a basic compositional lan-guage. This language is represented as streams of abstract discrete symbols uttered by agents over time, but nonetheless has a coherent structure that possesses a defined vocabulary and syntax. We also observe emergence of non-verbal com-munication such as pointing and guiding when language communication is unavailable.},
author = {Mordatch, Igor and Abbeel, Pieter},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Mordatch, Abbeel - Unknown - Emergence of Grounded Compositional Language in Multi-Agent Populations.pdf:pdf},
title = {{Emergence of Grounded Compositional Language in Multi-Agent Populations}},
url = {https://arxiv.org/pdf/1703.04908.pdf}
}
@article{Jain,
abstract = {This work characterizes the benefits of averaging techniques widely used in conjunction with stochastic gradient descent (SGD). In particular, this work sharply analyzes: (1) mini-batching, a method of averaging many samples of the gradient to both reduce the variance of a stochastic gradient estimate and for parallelizing SGD and (2) tail-averaging, a method involving averaging the final few iterates of SGD in order to decrease the variance in SGD's final iterate. This work presents the first tight non-asymptotic generalization error bounds for these schemes for the stochastic approximation problem of least squares regression. Furthermore, this work establishes a precise problem-dependent extent to which mini-batching can be used to yield provable near-linear parallelization speedups over SGD with batch size one. These results are utilized in providing a highly parallelizable SGD algorithm that obtains the optimal statistical error rate with nearly the same number of serial updates as batch gradient descent, which improves significantly over existing SGD-style methods. Finally, this work sheds light on some fundamental differences in SGD's behavior when dealing with agnostic noise in the (non-realizable) least squares regression problem. In particular, the work shows that the stepsizes that ensure optimal statistical error rates for the agnostic case must be a function of the noise properties. The central analysis tools used by this paper are obtained through generalizing the operator view of averaged SGD, introduced by D{\'{e}}fossez and Bach [1] followed by developing a novel analysis in bounding these operators to characterize the generalization error. These techniques may be of broader interest in analyzing various computational aspects of stochastic approximation.},
author = {Jain, Prateek and Kakade, Sham M and Kidambi, Rahul and Netrapalli, Praneeth and Sidford, Aaron},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Jain et al. - Unknown - Parallelizing Stochastic Approximation Through Mini-Batching and Tail-Averaging.pdf:pdf},
title = {{Parallelizing Stochastic Approximation Through Mini-Batching and Tail-Averaging}},
url = {https://arxiv.org/pdf/1610.03774.pdf}
}
@article{Byravan,
abstract = {â€”We introduce SE3-Nets which are deep networks designed to model rigid body motion from raw point cloud data. Based only on pairs of depth images along with an action vector and point wise data associations, SE3-Nets learn to segment effected object parts and predict their motion resulting from the applied force. Rather than learning point wise flow vectors, SE3-Nets predict SE3 transformations for different parts of the scene. Using simulated depth data of a table top scene and a robot manipulator, we show that the structure underlying SE3-Nets enables them to generate a far more consistent prediction of object motion than traditional flow based networks. I. INTRODUCTION The ability to predict how an environment changes based on forces applied to it is fundamental for a robot to achieve specific goals. For instance, in order to arrange objects on a table into a desired configuration, a robot has to be able to reason about where and how to push individual objects, which requires some understanding of physical quantities such as object boundaries, mass, surface friction, and their relationship to forces. A standard approach in robot control is to use a physical model of the environment and perform optimal control to find a policy that leads to the goal state. For instance, extensive work utilizing the MuJoCo physics engine [30] has shown how strong physics models can enable solutions to optimal control problems and policy learning in complex and contact-rich environments [11, 22]. A fundamental problem of such models is that they rely on very accurate estimates of the state of the system [37]. Unfortunately, estimating values such as the mass distribution and surface friction of an object using visual information and force feedback is extremely difficult. This is one of the main reasons why humans are still far better than robots at performing even simple tasks such as pushing an object along a desired trajectory. Humans achieve this even though their control policies and decision making are informed only by approximate notions of physics [5, 27]. Research has shown that these mental models are learned from a young age, potentially by observing the effect of actions on the physical world [4]. Learning such a model of physical intuition can help robots reason about the effect of their actions, which is a critical component of operating in complex real-world environments. In this work, we explore the use of deep learning to model this concept of "physical intuition", learning a model that predicts changes to the environment based on specific actions. Our model uses motion cues to learn to segment the scene into "salient" objects (much akin to a saliency map [17])},
author = {Byravan, Arunkumar and Fox, Dieter},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Byravan, Fox - Unknown - SE3-Nets Learning Rigid Body Motion using Deep Neural Networks.pdf:pdf},
title = {{SE3-Nets: Learning Rigid Body Motion using Deep Neural Networks}},
url = {https://arxiv.org/pdf/1606.02378.pdf}
}

@inproceedings{pere2018unsupervisedgoalspaces,
abstract = {Intrinsically motivated goal exploration algorithms enable machines to discover repertoires of policies that produce a diversity of effects in complex environ-ments. These exploration algorithms have been shown to allow real world robots to acquire skills such as tool use in high-dimensional continuous state and action spaces. However, they have so far assumed that self-generated goals are sampled in a specifically engineered feature space, limiting their autonomy. In this work, we propose to use deep representation learning algorithms to learn an adequate goal space. This is a developmental 2-stage approach: first, in a perceptual learn-ing stage, deep learning algorithms use passive raw sensor observations of world changes to learn a corresponding latent space; then goal exploration happens in a second stage by sampling goals in this latent space. We present experiments where a simulated robot arm interacts with an object, and we show that explo-ration algorithms using such learned representations can match the performance obtained using engineered representations.},
author = {P{\'{e}}r{\'{e}}, Alexandre and Forestier, Sebastien and Sigaud, Olivier and Oudeyer, Pierre-Yves},
booktitle = {International Conference on Learning Representations (ICLR)},
file = {::},
mendeley-groups = {Old Research},
title = {{Unsupervised Learning of Goal Spaces for Intrinsically Motivated Goal Exploration}},
url = {https://arxiv.org/pdf/1803.00781.pdf},
year = {2018}
}

@article{baranes2012activeimgep,
abstract = {Highlights: 1) SAGG-RIAC is an architecture for active learning of inverse models in high-dimensional redundant spaces 2) This allows a robot to learn efficiently distributions of parameterized motor policies that solve a corresponding distribution of parameterized tasks 3) Active sampling of parameterized tasks, called active goal exploration, can be significantly faster than direct active sampling of parameterized policies 4) Active developmental exploration, based on competence progress, autonomously drives the system to progressively explore tasks of increasing learning complexity. Abstract We introduce the Self-Adaptive Goal Generation-Robust Intelligent Adaptive Curiosity (SAGG-RIAC) architecture as an intrinsically motivated goal exploration mechanism which allows active learning of inverse models in high-dimensional redundant robots. This allows a robot to efficiently and actively learn distributions of parameterized motor skills/policies that solve a corresponding distribution of parameterized tasks/goals. The architecture makes the robot sample actively novel parameterized tasks in the task space, based on a measure of competence progress, each of which triggers low-level goal-directed learning of the motor policy parameters that allow to solve it. For both learning and generalization, the system leverages regression techniques which allow to infer the motor policy parameters corresponding to a given novel parameterized task, and based on the previously learnt correspondences between policy and task parameters.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.4862v1},
author = {Baranes, A and Oudeyer, P-Y},
doi = {10.1016/j.robot.2012.05.008},
eprint = {arXiv:1301.4862v1},
file = {::},
journal = {Robotics and Autonomous Systems},
mendeley-groups = {Old Research},
number = {1},
pages = {49--73},
title = {{Active Learning of Inverse Models with Intrinsically Motivated Goal Exploration in Robots}},
url = {http://dx.doi.org/10.1016/j.robot.2012.05.008},
volume = {61},
year = {2012}
}

@article{srinivas2018upn,
  title={Universal Planning Networks},
  author={Srinivas, Aravind and Jabri, Allan and Abbeel, Pieter and Levine, Sergey and Finn, Chelsea},
  journal={arXiv preprint arXiv:1804.00645},
  year={2018}
}

@inproceedings{lee2017servo,
author = {Lee, Alex and Levine, Sergey and Abbeel, Pieter},
booktitle = {International Conference on Learning Representations (ICLR)},
file = {::},
mendeley-groups = {Old Research},
title = {{Learning Visual Servoing with Deep Features and Fitted Q-Iteration}},
url = {https://arxiv.org/pdf/1703.11000.pdf},
year = {2017}
}

@article{jonschkowski2017pves,
  title={Pves: Position-velocity encoders for unsupervised learning of structured state representations},
  author={Jonschkowski, Rico and Hafner, Roland and Scholz, Jonathan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1705.09805},
  year={2017}
}

@article{plappert2018techreport,
abstract = {The purpose of this technical report is twofold. First of all, it introduces a suite of challenging continuous control tasks (integrated with OpenAI Gym) based on currently existing robotics hardware. The tasks include pushing, sliding and pick {\&} place with a Fetch robotic arm as well as in-hand object manipulation with a Shadow Dexterous Hand. All tasks have sparse binary rewards and follow a Multi-Goal Reinforcement Learning (RL) framework in which an agent is told what to do using an additional input. The second part of the paper presents a set of concrete research ideas for improving RL algorithms, most of which are related to Multi-Goal RL and Hindsight Experience Replay.},
archivePrefix = {arXiv},
arxivId = {arXiv:1802.09464v2},
author = {Plappert, Matthias and Andrychowicz, Marcin and Ray, Alex and Mcgrew, Bob and Baker, Bowen and Powell, Glenn and Schneider, Jonas and Tobin, Josh and Chociej, Maciek and Welinder, Peter and Kumar, Vikash and Zaremba, Wojciech},
eprint = {arXiv:1802.09464v2},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Plappert et al. - Unknown - Multi-Goal Reinforcement Learning Challenging Robotics Environments and Request for Research.pdf:pdf},
journal = {arXiv preprint arXiv:1802.09464},
mendeley-groups = {Old Research},
title = {{Multi-Goal Reinforcement Learning: Challenging Robotics Environments and Request for Research}},
url = {http://fetchrobotics.com/},
year = {2018}
}

@article{finn2017one,
  title={One-shot visual imitation learning via meta-learning},
  author={Finn, Chelsea and Yu, Tianhe and Zhang, Tianhao and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1709.04905},
  year={2017}
}

@article{DBLP:journals/corr/abs-1810-05017,
  author    = {Tom Le Paine and
               Sergio Gomez Colmenarejo and
               Ziyu Wang and
               Scott E. Reed and
               Yusuf Aytar and
               Tobias Pfaff and
               Matthew W. Hoffman and
               Gabriel Barth{-}Maron and
               Serkan Cabi and
               David Budden and
               Nando de Freitas},
  title     = {One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with
               {RL}},
  journal   = {CoRR},
  volume    = {abs/1810.05017},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.05017},
  archivePrefix = {arXiv},
  eprint    = {1810.05017},
  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1810-05017},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{wang2017robust,
  title={Robust imitation of diverse behaviors},
  author={Wang, Ziyu and Merel, Josh S and Reed, Scott E and de Freitas, Nando and Wayne, Gregory and Heess, Nicolas},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5320--5329},
  year={2017}
}

@article{DBLP:journals/corr/JamesDJ17,
  author    = {Stephen James and
               Andrew J. Davison and
               Edward Johns},
  title     = {Transferring End-to-End Visuomotor Control from Simulation to Real
               World for a Multi-Stage Task},
  journal   = {CoRR},
  volume    = {abs/1707.02267},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.02267},
  archivePrefix = {arXiv},
  eprint    = {1707.02267},
  timestamp = {Mon, 13 Aug 2018 16:47:24 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/JamesDJ17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1806-10166,
  author    = {Ferran Alet and
               Tom{\'{a}}s Lozano{-}P{\'{e}}rez and
               Leslie Pack Kaelbling},
  title     = {Modular meta-learning},
  journal   = {CoRR},
  volume    = {abs/1806.10166},
  year      = {2018},
  url       = {http://arxiv.org/abs/1806.10166},
  archivePrefix = {arXiv},
  eprint    = {1806.10166},
  timestamp = {Mon, 13 Aug 2018 16:46:38 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1806-10166},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{misra2016shuffle,
  title={Shuffle and learn: unsupervised learning using temporal order verification},
  author={Misra, Ishan and Zitnick, C Lawrence and Hebert, Martial},
  booktitle={European Conference on Computer Vision},
  pages={527--544},
  year={2016},
  organization={Springer}
}

@inproceedings{wang2015unsupervised,
  title={Unsupervised learning of visual representations using videos},
  author={Wang, Xiaolong and Gupta, Abhinav},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={2794--2802},
  year={2015}
}

@inproceedings{hausman2018learning,
  title={Learning an Embedding Space for Transferable Robot Skills},
  author={Karol Hausman and Jost Tobias Springenberg and Ziyu Wang and Nicolas Heess and Martin Riedmiller},
  booktitle={International Conference on Learning Representations},
  year={2018},
  url={https://openreview.net/forum?id=rk07ZXZRb},
}

@book{wood2005play,
  title={Play, learning and the early childhood curriculum},
  author={Wood, Elizabeth and Attfield, Jane},
  year={2005},
  publisher={Sage}
}

@article{pellegrini2007play,
  title={Play in evolution and development},
  author={Pellegrini, Anthony D and Dupuis, Danielle and Smith, Peter K},
  journal={Developmental review},
  volume={27},
  number={2},
  pages={261--276},
  year={2007},
  publisher={Elsevier}
}

@article{kalashnikov2018qt,
  title={Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation},
  author={Kalashnikov, Dmitry and Irpan, Alex and Pastor, Peter and Ibarz, Julian and Herzog, Alexander and Jang, Eric and Quillen, Deirdre and Holly, Ethan and Kalakrishnan, Mrinal and Vanhoucke, Vincent and others},
  journal={arXiv preprint arXiv:1806.10293},
  year={2018}
}

@article{haarnoja2018soft,
  title={Soft Actor-Critic Algorithms and Applications},
  author={Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and others},
  journal={arXiv preprint arXiv:1812.05905},
  year={2018}
}

@article{ebert2018visual,
  title={Visual Foresight: Model-Based Deep Reinforcement Learning for Vision-Based Robotic Control},
  author={Ebert, Frederik and Finn, Chelsea and Dasari, Sudeep and Xie, Annie and Lee, Alex and Levine, Sergey},
  journal={arXiv preprint arXiv:1812.00568},
  year={2018}
}

@book{robert1981animal,
  title={Animal play behavior.},
  author={Robert, Fagen},
  year={1981}
}

@article{bateson2005play,
  title={Play and its role in the development of great apes and humans},
  author={Bateson, PPG},
  journal={The nature of play: Great apes and humans},
  pages={13--26},
  year={2005},
  publisher={Guilford New York, NY}
}

@book{sutton2009ambiguity,
  title={The ambiguity of play},
  author={Sutton-Smith, Brian},
  year={2009},
  publisher={Harvard University Press}
}

@book{burghardt2005genesis,
  title={The genesis of animal play: Testing the limits},
  author={Burghardt, Gordon M},
  year={2005},
  publisher={Mit Press}
}

@article{hinde1983ethology,
  title={Ethology and child development},
  author={Hinde, ROBERT A},
  journal={Handbook of child psychology: formerly Carmichael's Manual of child psychology/Paul H. Mussen, editor},
  year={1983},
  publisher={New York: Wiley, c1983.}
}

@article{pellegrini2002children,
  title={Children’s play: A developmental and evolutionary orientation},
  author={Pellegrini, AD and Smith, PK},
  journal={Handbook of developmental psychology},
  pages={276--291},
  year={2002},
  publisher={Sage London}
}

@inproceedings{hutt1966exploration,
  title={Exploration and play in children},
  author={Hutt, Corinne},
  booktitle={Symposia of the Zoological Society of London},
  volume={18},
  number={1},
  pages={61--81},
  year={1966},
  organization={London}
}

@inproceedings{agrawal2016learning,
  title={Learning to poke by poking: Experiential learning of intuitive physics},
  author={Agrawal, Pulkit and Nair, Ashvin V and Abbeel, Pieter and Malik, Jitendra and Levine, Sergey},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5074--5082},
  year={2016}
}

@inproceedings{nair2017combining,
  title={Combining self-supervised learning and imitation for vision-based rope manipulation},
  author={Nair, Ashvin and Chen, Dian and Agrawal, Pulkit and Isola, Phillip and Abbeel, Pieter and Malik, Jitendra and Levine, Sergey},
  booktitle={2017 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={2146--2153},
  year={2017},
  organization={IEEE}
}

@inproceedings{pinto2016supersizing,
  title={Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours},
  author={Pinto, Lerrel and Gupta, Abhinav},
  booktitle={2016 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={3406--3413},
  year={2016},
  organization={IEEE}
}

@article{levine2018learning,
  title={Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection},
  author={Levine, Sergey and Pastor, Peter and Krizhevsky, Alex and Ibarz, Julian and Quillen, Deirdre},
  journal={The International Journal of Robotics Research},
  volume={37},
  number={4-5},
  pages={421--436},
  year={2018},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{belsky1981exploration,
  title={From exploration to play: a cross-sectional study of infant free play behavior.},
  author={Belsky, Jay and Most, Robert K},
  journal={Developmental psychology},
  volume={17},
  number={5},
  pages={630},
  year={1981},
  publisher={American Psychological Association}
}

@article{kingma1606improving,
  title={Improving variational inference with inverse autoregressive flow.(nips), 2016},
  author={Kingma, Diederik P and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max},
  journal={URL http://arxiv. org/abs/1606.04934}
}

@inproceedings{van2017neural,
  title={Neural discrete representation learning},
  author={van den Oord, Aaron and Vinyals, Oriol and others},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6306--6315},
  year={2017}
}

@article{forestier2017intrinsically,
  title={Intrinsically motivated goal exploration processes with automatic curriculum learning},
  author={Forestier, S{\'e}bastien and Mollard, Yoan and Oudeyer, Pierre-Yves},
  journal={arXiv preprint arXiv:1708.02190},
  year={2017}
}

@book{sansone2000intrinsic,
  title={Intrinsic and extrinsic motivation: The search for optimal motivation and performance},
  author={Sansone, Carol and Harackiewicz, Judith M},
  year={2000},
  publisher={Elsevier}
}

@inproceedings{schmidhuber1991possibility,
  title={A possibility for implementing curiosity and boredom in model-building neural controllers},
  author={Schmidhuber, J{\"u}rgen},
  booktitle={Proc. of the international conference on simulation of adaptive behavior: From animals to animats},
  pages={222--227},
  year={1991}
}

@article{spelke2007core,
  title={Core knowledge},
  author={Spelke, Elizabeth S and Kinzler, Katherine D},
  journal={Developmental science},
  volume={10},
  number={1},
  pages={89--96},
  year={2007},
  publisher={Wiley Online Library}
}

</script>
<script src="lib/blazy.js"></script>
<script>
  var bLazy = new Blazy({
    success: function(){
      updateCounter();
    }
  });
  var imageLoaded = 0;
  function updateCounter() {
    imageLoaded++;
    console.log("blazy image loaded: "+imageLoaded);
  }
</script>
